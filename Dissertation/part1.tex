\chapter{Вероятностное тематическое моделирование}

\section{Задача тематического моделирования}

Тематическое моделирование – это метод анализа коллекции текстов, оперирующий понятиями \textit{токена} и \textit{темы}. Предполагается, что токен --– это просто слово, а тема описывается как распределение вероятностей над множеством всех токенов. Каждый документ из коллекции текстов связан с некоторым распределением над множеством тем.

% Например, тема, которую можно связать с понятием ``театр'',  будет представлять собой распределение, сконцентрированное в словах наподобие \texttt{актёр}, \texttt{пьеса}, \texttt{премьера}, \texttt{партер}, \texttt{зритель} (при этом вероятности слов \texttt{займ} и \texttt{млекопитающее} будут незначительными или даже нулевыми).

Тематическая модель описывается двумя распределениями: $p(w \mid t)$ (вероятность того, что тема $t$ породит токен $w$) и $p(t \mid d)$ (пропорция темы $t$ в документе $d$). Часто используется обозначения $\phi_{wt} = p(w \mid t), \theta_{td} = p(t \mid d)$. Два искомых распределения можно тогда представить в виде стохастических матриц $\Phi$ и $\Theta$. 

С точки зрения тематического моделирования корпус документов представляет собой последовательность трёхэлементных кортежей ${\Omega_n = \bigl\{ (w_i,d_i,t_i) \bigm| i=1,\dots,n \bigr\}}$.
Токены~$w_i$ и~документы~$d_i$ являются наблюдаемыми переменными,
а темы~$t_i$ являются скрытыми переменными.

Можно сказать, что тематическое моделирование формализует два наблюдения лингвистической природы: 
\begin{enumerate}
\item{существуют естественные кластера слов, употребляемых вместе (темы)}
\item{в разных документах эти темы встречаются в разной степени}
\end{enumerate}

Главное допущение, лежащее в основе тематического моделирования, проще всего сформулировать через процес генерации документов. Тематическая модель «считает», что каждый документ создаётся так: 
\begin{enumerate}
    \item{Автор выбирает случайную тему $t$, руководствуясь распределением $p(t \mid d)$}
    \item{Из распределения $p(w \mid d, t)$ выбирается случайное слово $w$}
\end{enumerate}

Кажущаясся нереалистичность этого допущения на практике смягчается тем, что порядок слов в документе игнорируется. Модель оперирует только с величинами $n_{dw}$, обозначающими число вхождений слова $w$ в документ $d$. Это предположение, называемое  \textit{гипотезой мешка слов}, сильно упрощает математический аппарат и поэтому часто используется в анализе текстов.

Ещё одно важное допущение, связанное с постановкой задачи, называется \textit{гипотезой условной независимости}. Все вероятностные зависимости токенов между собой объясняются исключительно их тематиками:
\begin{equation}
    p(w| d,t) = p(w| t).
\end{equation}

Тема может описывать множество терминов из какой-то области: например, тема «театр» включает в себя слова «зритель», «опера», «премьера» и не включает в себя слова «космонавтика», «эмпиризм», «кредит» или «гемоглобин». 

Однако, темы могут иметь и более гибкую природу. Например, при анализе художественных текстов можно выделить темы, связанные с определённым персонажем; при анализе обращений в техподдержку можно выделить темы про конкретные проблемы; для sentiment analysis можно выделять темы про отдельные эмоции.

Несмотря на то, что тематическое моделирование предназначалось в первую очередь для поиска скрытых тем в текстовых документах, существуют и более экзотические сферы его применения:

\begin{itemize}
    \item Анализ видеозаписей. Видеозапись является документом, в качестве токенов выступают признаки на его кадрах, а в качестве тем – события; например, проезд автомобиля\cite{sparse}.
    \item Анализ изображений. Документ --- это одно изображение, его токены --- признаковое описание этого изображения, темы --- находящиесся на изображении объекты \cite{mml1}.
    \item Судовые журналы кораблей. Документ --- это одно путешествие, токен --- координаты корабля в определённый день, а темы --- различные ''миссии'', такие как перевозка грузов в определённый порт или китобойный промысел \cite{dh_sea}. 
    \item Банковские транзакции. Документ --- история покупок одного клиента, токен --- код продавца, темы --- виды деятельности (например, ``ремонт квартиры'') \cite{egorov2019topic}.
    \item Анализ электрокардиосигналов. Документ --- одна кардиограмма, токены --- кодограммы (полученные на основе амплитуд и интервалов), темы --- отдельные заболевания \cite{shapulin}
    \item Анализ музыкальных произведений. Документ --- одно произведение, токены --- последовательность тональных высотных классов (tonal pitch-classes), темы --- тональные профили \cite{moss2019transitions}
\end{itemize}

Поэтому для того чтобы подчеркнуть общность изложения далее мы будем использовать понятие \textbf{токена} вместо понятий термина/слова.

\section{Классические тематические модели}

Исторически тематическое моделирование вырастает из метода анализа произвольных многомерных данных, называемого \textit{факторным анализом}. Две основные цели ФА --- это выявление закономерностей в данных и нахождение более компактного способа описания данных. Как правило, в процессе ФА исследователь раскладывает данные на линейно независимые компоненты, называемые ''факторами'', а затем отсекает все незначительные факторы. 

В работе \cite{deerwester1990indexing} было предложено применить факторный анализ к задаче информационного поиска (information retrieval). Этот подход получил название \textit{латентного семантического анализа} или \textit{латентной семантической индексации}.

По любому документу $d$ можно построить набор чисел $v$, в компоненте $i$ которого будет указано $n_{{w_i}, d}$ --- число вхождений слова $w_i$ в документ $d$. Этот набор чисел можно рассмотреть как вектор в пространстве высокой размерности. Также важную роль в латентном семантическом анализе играет \textit{терм-документная матрица}, элемент которой на позиции $(w, d)$ равен $n_{wd}$ --- числу вхождений токена $w$ в документ $d$ и её разложение в произведение трёх матриц определённого вида.

Ключевая идея латентного семантического анализа --- в том, что документы следует отображать в пространство более низкой размерности. Таким образом выявляется скрытая структура коллекции документов, позволяющая, например, сравнивать друг с другом пары документов, не имеющие общих слов.

Латентный семантический анализ был математически проанализирован в работе \cite{papadimitriou1998latent}: авторы рассмотрели корпус документов, сгенерированных определённым вероятностным распределением и доказали ряд результатов, касающихся геометрических свойств LSI. Использованное ими распределение фактически являлось упрощённой тематической моделью и описывалось в схожих терминах.

В 1999 году в работе \cite{hofmann1999probabilistic} Хофманн рассмотрел статистическое обобщение задачи латентного семантического анализа и сформулировал задачу вероятностного латентного семантического анализа (PLSA), которая является основой задачи тематического моделирования. 

Введённые им понятия и метрики качества используются в тематическом моделировании и по сей день. Также в этой статье Хофман описал EM-алгоритм построения модели и экспериментально изучил его свойства.

PLSA можно считать прямым наследником LSA, поскольку его тоже можно проинтерпретировать в терминах матричного разложения. Терм-документная матрица представляется в виде произведения трёх матриц, задающих распределения вероятностей (а именно: $p(d|t)$, $p(t)$ и $p(w|t)$).

Модель PLSA была далее дополнена Дэвидом Блеем во влиятельной работе \cite{blei2003latent}. Здесь Блей описал полноценную вероятностную модель коллекции документов (латентное размещение Дирихле). Модель LDA отличается от PLSA тем, что в ней добавляется уровень генерации распределений $p(t|d)$ (считается, что они порождены распределением Дирихле).

В рамках LDA процесс генерации коллекции документов считается таким:

\begin{itemize}
    \item{Для каждого документа $d$:}
    \begin{enumerate}
    \item{Автор выбирает число слов $N$ в документе $d$ (как правило, из распределения Пуассона)}
    \item{Из распределения Дирихле генерируется вектор $\theta$, который можно трактовать как распределение $p(t \mid d)$}
    \item{Для каждого из $N$ слов $w$ в документе $d$:}
        \begin{enumerate}
        \item{Автор выбирает случайную тему $t$, руководствуясь распределением $p(t \mid d)$}
        \item{Из распределения $p(w \mid t)$ выбирается случайное слово $w$}
        \end{enumerate}
    \end{enumerate}
\end{itemize}

Главным достоинством LDA является способность генерировать новые документы. Также считается что LDA более устойчива к переобучению хотя в этом вопросе нет окончательной ясности.

Большинство тематических моделей сегодня строятся на основе LDA.

\section{Аддитивная регуляризация тематических моделей}

Тематическую модель достаточно естественно описывать в терминах байесовского вывода.  Задаваемые ей распределения вероятностей $p(w|t)$ и $p(t|d)$ – это отражение знаний некого условного агента, изучившего коллекцию текстов. Эти распределения задаются условием максимизации правдоподобия коллекции:

\begin{equation}
\label{eq:logL}
    \sum_{d\in D} \sum_{w\in d} n_{dw} \ln
        \sum_{t\in T}
            \phi_{wt}\theta_{td}
    \;\to\; \max_{\Phi,\Theta},
\end{equation}


В рамках байесова подхода было предложено множество моделей, базирующихся на подходе LDA, которые постепенно усложняли вероятностную модель за счёт учёта связей между документами \cite{cohn2001missing,mccallum2005author,nallapati2008link}, метаданных о документах \cite{steyvers2004probabilistic}, 
времени и мультиязычности \cite{zosa-granroth-wilding-2019-multilingual}, и даже информации о порядке слов в документе \cite{gruber2007hidden,wallach2006topic}. Возможность подобного расширения играет важную роль в популярности подхода LDA \cite{fntir2017applications}.

Традиционный способ построения новых тематических моделей описан в  \cite{fntir2017applications}. Рекомендации включают в себя: введение новой вероятностной модели коллекции документов (которая не должна быть слишком вычислительно сложной, оставаясь при этом реалистичной); нахождение нового алгоритма оценки апостериорного распределения параметров; реализацию этого алгоритма (при этом, новая модель может оказаться несовместимой с известными вычислительные оптимизациями); валидацию результатов. В целом можно заметить, что построение тематических моделей, удовлетворяющих нескольким различным требований одновременно, остаётся трудной задачей.

\subsection{Тематическое моделирование внутри подхода ARTM}

Чтобы обойти обозначенные недостатки описанных алгоритмов, и в итоге получить простую, но гибкую и легко расширяемую модель для вероятностного тематического моделирования, был предложен подход Аддитивной Регуляризации Тематических Моделей (Additive Regularization for Topic Models, ARTM) \cite{vorontsov2014additive,vorontsov2014tutorial,vorontsov2015additive}. 

В отличии от байесовского подхода (где тематическое моделирование описывается в терминах апостериорных распредлений), в подходе ARTM тематическое моделирование является задачей неотрицательного матричного разложения. При заданной матрице совстречаемостей ``слово в документе''  $n_{dw}$ (где $n_{dw}$ обозначает количество раз, которое слово  $w$ встретилось в документе $d$), необходимо найти стохастические матрицы $\Phi,~\Theta$ произведение которых приблизительно равно нормированной матрице $n_{dw}$. Правдоподобие (или перплексия) конкретной модели интерпретируется как целевая метрика, отражающая качество этого приближения.

Обобщением этой интерпретации является многокритериальный подход, в котором к основному критерию правдоподобия $L(\Phi, \Theta)$ 
добавляется взвешенная сумма регуляризаторов $R(\Phi, \Theta) = \sum_i \tau_i R_i(\Phi, \Theta)$. В результате получаем следующую оптимизационную задачу при ограничениях неотрицательности и~нормировки:

\begin{gather}
\label{eq:logL+R}
    \sum_{d\in D} \sum_{w\in d}
        n_{dw}\ln
        \sum_{t\in T}
            \phi_{wt}\theta_{td}
    + R(\Phi,\Theta)
    \;\to\; \max_{\Phi,\Theta};
    \qquad
    R(\Phi,\Theta)
    = \sum_{i=1}^k \tau_i R_i(\Phi,\Theta);
\\
\label{eq:logL.constraints}
    \sum_{w\in W} \phi_{wt} = 1;
    \quad
    \phi_{wt}\geq 0;
    \qquad
    \sum_{t\in T} \theta_{td} = 1;
    \quad
    \theta_{td}\geq 0.
\end{gather}

За счёт аддитивности алгоритм ARTM  выведен и реализован один раз в самом общем виде, и поэтому оптимизация любых моделей  производится при помощи одного и того же итерационного процесса  (обобщённого EM-алгоритма)  \cite{vorontsov2014additive,vorontsov2015}. Важнейшую роль в АРТМ играет следующая теорема:

\begin{Theorem}
\label{th:ARTM}
    Пусть функция $R(\Phi,\Theta)$ непрерывно дифференцируема.
    Тогда точка $(\Phi,\Theta)$ локального экстремума задачи
    \eqref{eq:logL+R} с~ограничениями \eqref{eq:logL.constraints}
    удовлетворяет системе уравнений со вспомогательными переменными $p_{tdw}=p(t\cond d,w)$,
    если из решения исключить нулевые столбцы матриц $\Phi$, $\Theta$:
    \begin{align}
    \label{eq:ARTM.Estep}
        p_{tdw} &=\norm_{t\in T} \bigl( \phi_{wt}\theta_{td} \bigr);
    \\\label{eq:ARTM.Mstep.phi}
        \phi_{wt} &=\norm_{w\in W}
            \biggl(
                n_{wt} + \phi_{wt} \frac{\partial R}{\partial \phi_{wt}}
            \biggr);
        &
        n_{wt} &= \sum_{d\in D} n_{dw}p_{tdw};&
    \\\label{eq:ARTM.Mstep.theta}
        \theta_{td} &=\norm_{t\in T}
            \biggl(
                n_{td} + \theta_{td} \frac{\partial R}{\partial \theta_{td}}
            \biggr);
        &
        n_{td} &= \sum_{w\in d} n_{dw}p_{tdw}.&
    \end{align}
\end{Theorem}

Получаемая в результате система уравнений допускает решение методом простых итераций, который можно проинтерпретировать как ЕМ-алгоритм. На E-шаге обновляются значения $p_{tdw}$ (эту величину можно истрактовать, как вероятность того, что слово $w$ в документе $d$ порожденно скрытой темой $t$). На M-шаге обновляются значения $\Phi$ и $\Theta$:

\begin{equation} \label{eq:Mstep_Theta}
\Phi_{wt}^{new} =\norm_{w\in W} \left( \sum_{d} n_{dw} p_{tdw} + \Phi_{wt}^{old} \frac{\partial{R}}{\partial{\Phi_{wt}}}\right)
\end{equation}
\[
\Theta_{td}^{new}   =\norm_{d\in D} \left( \sum_{w} n_{dw} p_{tdw} + \Theta_{td}^{old} \frac{\partial{R}}{\partial{\Theta_{td}}}\right).
\]
 
Из формул видно, что для того, чтобы добавить новый регуляризатор, нужно знать лишь $\frac{\partial{R}}{\partial{\Phi_{wt}}}, \frac{\partial{R}}{\partial{\Theta_{td}}}$, его частные производные по параметрам модели. За счёт вышесказанного можно утверждать, что ARTM~---~это не одна частная тематическая модель, а общий подход к их построению и комбинированию.

\subsection{Мультимодальное тематическое моделирование}

Аналогичным образом можно и организовать учёт дополнительной информации. 

В классическом тематическом моделировании считается, что документы содержат лишь слова $w \in W$, однако на практике о документе часто известна какая-то дополнительная информация, представимая в виде меток из некого конечного множества другой природы (примерами таких меток могут быть авторы документа, категории, дата и места публикации, реклама на странице). 

Будем считать, что эти метки имеют $m$ различных видов, называемыми  \textit{модальностями}, одной из которых является модальность слов (помимо которой можно рассмотреть, например, модальность авторов, модальность библиографии, модальность времени и прочее). Естественным образом обобщив вероятностную модель на этот случай, можно получить следующее выражения для общего правдополобия коллекции:

\[
L(\Phi^m, \Theta) = \sum_m \tau_m \sum_{d\in D} \sum_{w \in W^m} n_{dw} \ln p(w \mid d) \rightarrow \max
\]

Задачи такого вида можно единообразно решать при помощи модифицированного ЕМ-алгоритма.

\section{Приложения тематического моделирования}

% Early work on topic modeling conceptualized it as an intermediate stage of information retrieval pipeline. The possibility of meaningful interpretation was an afterthought. For measuring the quality of topics when evaluated against human judgments, several metrics were proposed. 


Тематические модели доказали свою актуальность в широком диапазоне контекстов\cite{fntir2017applications}. Их используют для информационного поиска \cite{yi2009, wang2011}, категоризации документов \cite{rubin2012}, анализа социальных сетей \cite{varshney2014, pinto2016}, рекоммендационных систем \cite{wang2011}, \cite{lee2015} и других задач.

Применения тематического моделирования можно разделить на две условных групы. Во-первых, выход тематической модели является способом векторного представления документа или слов. Такие представления можно использовать для прикладных задач классификации, ранжирования, поиска (непосредственно или как дополнительное признаковое описание для какого-либо алгоритма машинного обучения). Во-вторых, тематическая модель позволяет выявить структуру коллекции документов, обнаружить в данных закономерности, помочь эксперту охватить всю коллекцию целиком.

\subsection{Анализ программных продуктов}

Тематическое моделирование также применяется для задач анализа программных артефактов \cite{sun2016exploring,chen2016survey}. Два самых популярных приложения описывают около 50\% всех работ в этой области\cite{chen2016survey}. 

Первое из них --- восстановление прослеживаемости (traceability link recovery), предназначенное для автоматического выявления связей между парами разнородных программных артефактов, таких как документы с исходным кодом и документы с требованиями\cite{asuncion2010software}. Возможность ответить на вопрос ``Какой файл (какие файлы) с исходным кодом реализует требование (функционал) X?'' важно как для разработчиков, так и для заказчика. Близкой является задача локализации ошибок (bug localization), при которой ищется связь между баг-репортами и исходным кодом.

Второе приложение --- локализация функционала (concept location/feature location), предназначенное для идентификации кусков исходного кода, связанных с реализацией данного функционала программной системы\cite{dit2013feature}. Здесь, в отличии от восстановления прослеживаемости, от модели требуется построить связи между однородными программными артефактами (т.е. между исходниками, лежащими в одном и том же репозитории). Эта задача в первую очередь актуальна для разработчиков, желающих отладить или улучшить данную функцию.

Также стоит отметить: кластеризацию и визуализацию кода  (Source Code Comprehension) для помощи в понимании и ориентировании в неструктурированном массиве текстов, основой которого служат идентификаторы переменных и комментарии; автоматическую разметку программных артефактов (software artifact labeling), предназначенную для характеризации большого документа коротким множеством ключевых слов\cite{de2012using}; прогнозирование возможных дефектов (Software Defects Prediction) посредством анализа исходников или логов выполнения; помощь в  устранении дефектов посредством статистической отладки и анализа причин (Statistical Debugging и Root Cause Analysis), включающую в себя интеллектуальную фильтрацию логов исполнения.

К прочим применениям можно отнести анализ истории изменений (Software History Comprehension), маршрутизацию задач к наиболее подходящему разработчику (Developer Recommendation/Bug Triaging), помощь в рефакторинге, приоритезацию исполнения тестовых кейсов, измерение дополнительных мер качества и так далее \cite{sun2016exploring,chen2016survey}.

\subsection{Разведочный поиск}

Разведочный поиск включает в себя большой класс задач, связанных с систематизацией информации, суммаризацией текста, переработкой знаний. Разведочный поиск --- это итерационный процесс, в котором пользователь несколько раз переформулирует свой запрос с желанием разобраться в новой для себя области. 

Тематические модели оказываются наиболее актуальными для обработки длинных или сложных поисковых запросов, которые трудно  или невозможно сформулировать в виде короткого списка ключевых слов (в том числе и для поиска документов, похожих на либо релевантным данному). В недавних работах \cite{ianina2017multi, ianina2019regularized} показано, что аддитивно регуляризованные тематические модели демонстрируют качество, лишь незначительно уступающее качеству, достигнутому асессорами.

\section{Интерпретация тематических моделей людьми}

Тематическое моделирование не обязательно показывает наилучшие результаты с точки зрения численных измерений качества для прикладных задач, но за ним явное преимущество, когда речь заходит о интерпретируемости. Каждую тему можно интерпретировать как вероятностное распределение над токенами в этой теме, и каждая координата в представлении документа несёт смысл (вероятность того, что в этом документе будет содержаться определенная тема). Это свойство тематических моделей делает их более подходящими для областей ИИ, где важна ясность прогнозирования или где может быть необходимо ручное исправление нежелательных искажений.

При этом вторая область применения ~---~ извлечение структуры коллекции ~---~ является уникальной для тематического моделирования. Зачастую исследователь ищет ответы на вопросы о структуре и природе коллекции, а не пытается оптимизировать заданную меру качества. Такой подход (который можно назвать ``дальним чтением'' \cite{milkova2019distant}), например, был принят в области биологии \cite{Liu2016,funnell2019integrated} и гуманитарных наук \cite{fntir2017applications,antons2019content}. Благодаря ему возможно получить важное представление о больших данных, которое в противном случае могло бы быть упущено исследователем.

Примером такого использования математического аппарата тематического моделирования служит проект  Mining the Dispatch \cite{monsters_men,dispatch}, посвящённый анализу более ста тысяч статей из газеты Richmond Daily Dispatch времен американской гражданской войны. Главная мотивация этого проекта состоит в том, что повседневная жизнь в Ричмонде, столице Конфедеративных Штатов Америки, до сих пор во многом остаётся загадкой.

В этой работе выявляются основные темы, проявляющиесся в газете. К таким темам относится, например, "обьявление о сбежавшем рабе" (в результате этого анализа был замечен резкий и необьяснимый всплеск числа таких обьявлений в начале 1862 года, что даёт основу для дальнейших исследований с использованием традиционных исторических методов), "антисеверная пропаганда" или "патриотизм и поэзия" (было замечено, что последние две темы очень часто появляются вместе). Автор отмечает, что подобные закономерности было бы практически невозможно обнаружить более традиционными методами.

Ещё одной интересной областью гуманитарного применения тематического моделирования является литературный анализ. В работе \cite{buurma2015fictionality} анализируются работы Энтони Троллопа, известного романиста Викторианской эпохи. Исследователь выявляет ряд тем, часто встречающихся в творчестве писателя и обосновывает точку зрения о том, что тематическая модель позволяет увидеть работы с непривычной точки зрения (например, наличие посвящённой письмам темы помогает обнаружить частые отсылки к эпистолярному жанру). Исследователь замечает, что полученные темы можно рассматривать как составные части книг, которые писатель мог бы гипотетически написать.

Также при помощи тематического моделирования удобно изучать историю развития темы, примером чего служит работа \cite{goldstone2012can}. Авторы показывают принципиальную возможность анализа статей в журнале Publications of the Modern Language Association of America при помощи латентного размещения Дирихле. Авторы строят две разные тематические модели, сравнивают их и отмечают, что сделанные на основе их выводы согласуются друг с другом. Затрагивается вопрос о том, может ли тематическая модель быть аргументом в пользу какой-то гипотезы.

Также можно упомянуть работу~\cite{wang2016catching}, в которой была построена тематическая модель, описывающая твиты Дональда Трампа. На основании пропорций тем, полученных из этой модели, была создана модель, предсказывающая число ``лайков''. Авторы приходят к выводу, что самой ``любимой'' темой читателей Трампа является тема \texttt{Демократы}.

Актуальным кроссдициплинарным применением тематического моделирования является работа \cite{franz2020using}, в которой анализируется корпус сообщений с TeenHelp.org, форума  поддержки подростков в трудных и кризисных ситуациях. Исследователи фокусируются на категории постов, относящихся к самоповреждениям, суицидам, депрессии и семейных проблемах. Было показано, что темы модели, построенной на корпусе 2535 постов соотносятся с экспертной разметкой, проведённой на случайной подвыборке из 500 постов. 10 из 15 построенных тем оказались хорошо интерпретируемыми. Были замечены стататистические связи между распределением тем в сообщениях и демографических характеристиках (пол, возраст) авторов сообщений. 
Исследователи видят практическое применение своей работы в возможности раннего выявления предикторов риска аутодеструктивных проявлений у подростков, а также в способности более тонко дифференцировать значимые темы (депрессия, суицид), связанных с психическим здоровьем. 



% Много внимания уделяется и анализу переписки разработчиков на сайте Stack Overflow \cite{barua2014developers, linares2013exploratory, allamanis2013and, rosen2016mobile}

\section{Проблемы тематического моделирования}

В связи с этими и многими другими приложениями тематического моделирования возникает ряд очень важных вопросов.

\subsection{Неинтерпретируемые темы}

Не все построенные темы можно однозначно проинтерпретировать. Это ожидаемо: коллекции реальных текстов содержат в себе аномальные документы, неинформативные слова (к числу которых отностся как и глобальные стоп-слова/слова общей лексики, так и слова, широко распространённые в конкретном корпусе), опечатки и другие подобные вещи. При этом тематическая модель должна успешно описывать весь корпус, и поэтому таким артефактам нужно найти место.

Тем не менее, на практике важно минимизировать количество неинтерпретируемых тем и постараться сделать так, чтобы ``хорошие'' темы не перемешивались с ``плохими'' и не засорялись ``мусором''.


В работе \cite{mimno} были отмечены некоторые частые проблемы, связанные с темами низкого качества:

\begin{enumerate}
\item{Смешанные темы (mixed topics): набор слов, которые трудно проинтерпретировать вместе, но который содержит хорошо интерпретируемые подмножества слов. Можно сказать, что смешанные темы --- это несколько ``хороших'' тем, которые слиплись вместе}
\item{Темы-химеры (chained topics): смешанные темы, обьединённые общими словами. В качестве примера такой темы можно привести тему из слов "старение - человек - глобин". Пары "старение человек" и "глобин человек" имеют смысл, но пару "старение глобин" обьединяет лишь то, что оба слова часто встречаются рядом со словом "человек". Темы-химеры часто возникают из-за слов-омонимов (тема ``reagan, roosevelt, clinton, lincoln, honda, chevrolet, bmw'' объединяет в себе имена президентов США и марки автомобилей, слипшихся по неоднозначному слову "Линкольн") или из-за иерархических отношений между словами (общее понятие ``налог'' включает в себя ``налог на автомобиль'' и ``налог на прибыль'', из-за чего может появиться тема, содержащая в себе редкое сочетание ``автомобиль прибыль'').}
\item{Темы с посторонними словами (intruded topics): в целом осмысленные темы, внутри которых есть слова, не имеющие ничего общего с остальными словами}
\item{Случайные темы: набор слов, не имеющих отношения друг к другу}
\item{Несбалансированные по масштабу темы: содержат внутри себя как термины как широкой направленности, так и крайне специфические. Например, "сигнальная трансдукция" (общее название передачи сигнала в молекулярной биологии) и "сигнальный путь Notch" (один из числа сигнальных путей, которые, в свою очередь, являются одним из аспектов сигнальной трансдукции)}
\end{enumerate}

Заметим, что многие перечисленные здесь пункты могут делать потенциально информативные темы некачественными.

Этот список проблем далее был уточнён и переработан в работе \cite{boydcare}, в которой дополнителньо описываются темы с излишней общностью (состоящие из слишком частых и/или неспецифичных токенов), темы с избыточной специфичностью (состоящие из слишком редких и/или специфичных токенов), идентичные темы, и темы, ``испорченные'' неполным списком стоп-слов.

Другой класс явных проблем можно определить через расстояние до известных ``мусорных'' тем: D-, W-Vacuous, W-Uniform, среднее геометрическое, равномерное распределение $p(t)$; или по другим мерам качества (более подробно эти меры качества изложены в следующем разделе). 

Несмотря на то что суммарное качество для модели хорошо коррелирует с общей интерпретируемостью, разбиение мер качества по отдельным темам не всегда коррелирует с экспертной оценкой интерпретируемости индвидуальных тем. В ряде случаев экстремальные по каким-либо мерам качества темы (то есть как ``самые хорошие'', так и ``самые плохие'') являются неинтерпретируемыми или низкокачественными с точки зрения экспертов.

\subsection{Вводящие в заблуждение темы}

В противовес явным проблемам, изложенным выше, работа \cite{dh_sea} предостерегает исследователей от ряда возможных неявных проблем: <<Упрощение тематических моделей для ученых-гуманитариев, которые не будут (и не должны) изучать лежащие в их основе алгоритмы, создает огромный потенциал для необоснованных --- или даже вводящих в заблуждение --- ``инсайтов''>>. 

Примером такого проблематичного применения может служить исследователь, изучающий график "вероятность темы в момент времени", и рассматривающий одну тему отдельно от всех остальных. Взлёты и падения этой темы он интерпретирует в терминах изменения частоты понятия, характеризуемого её множеством из топ 10 слов. Автор работы призывает этого исследователя к осторожности и обосновывает свой тезис рядом демонстраций.

Первая демонстрация изучает роль верхних токенов в описании темы. Для этого тематическое моделирование применяется в экзотической области анализа географических данных: роль корпуса документов играют судовые журналы, токены которых --- координаты корабля в один из дней его путешествия. Главной особенностью этого эксперимента является возможность визуализации всего распределения целиком --- задача, которая, как правило, нереалистична в большинстве практических приложений.

Автор демонстрирует нерепрезентативность верхних токенов при помощи карты Земли с нанесёнными на неё точками (визуализация заданной темы состоит из линий, соответствующих документам/путешествиям и из точек, соответсвующих верхним токенам/самым частым местоположениям). В большинстве случаев видно, что верхние токены дают очень плохое представление о теме; большинство верхних токенов расположено на границах маршрута и не несут никакой информации о его форме. 

Вторая демонстрация проверяет темы на однородность по времени. Коллекция статей журнала Proceedings of the Modern Language Association растянута по времени с 1889 года по 2006 год. Это даёт возможность ``разбить построенную тему на две части'': отметить все отнесённые к ней слова во всех документах, затем поделить документы на две хронологические группы (с 1889 по 1959 год и с 1959 по 2006 год) и вычислить частоты этих слов отдельно внутри каждой группы. Полученные таким образом пары списков верхних слов можно сравнить. Около четверти всех тем имеют существенные различия и могут иметь две несовместимые интерпретации для этих двух временных периодов. 

На основании этих рассуждений автор заключает, что для популяризации тематического моделирования среди учёных-гуманитариев требуются средства визуализации, выходящие за рамки списка верхних слов, а также инструментарий тематического моделирования должен быть обогащён множеством автоматических проверок. Сформированная в работе критика остаётся актуальной и в настоящее время \cite{paakkonen2020humanistic}. 

\subsection{Неустойчивость}

Различные реализации тематических моделей могут использовать стохастические элементы в своей инициализации, обучении. Многопоточные реализации алгоритмов обучения могут быть подвержены состоянию гонки (race condition), что делает результат обучения недетерминированным даже в условиях переиспользования одной и той же фиксированной последовательности случайных чисел.

Также известно, что многие реализации тематического моделирования неустойчивы относительно порядка документов в корпусе \cite{agrawal2018wrong}. Более половины из 57 рассмотренных в работе \cite{agrawal2018wrong} статей в том или ином виде упоминают проблему неустойчивости латентного размещения Дирихле.

Это проблематично по ряду причин. Это затрудняет применение тематических моделей для анализа того, как тематики корпуса эволюционируют с прошествием времени или добавлением новых документов \cite{mehta_clustering_bank}. Если тематическая модель используется для принятия каких-то решений, то неустойчивость может привести к ошибочным выводам, основанным не на объективной структуре корпуса, а на случайном шуме\cite{mantyla2018measuring}. Это может ввести пользователя в замешательство и понизить эффективность других автоматических классификаторов, использующих выход тематической модели как обучающую выборку или дополнительные признаки \cite{agrawal2018wrong}.

\subsection{Доступность}

В монографии \cite{fntir2017applications} описываются рекомендации по разработке новой тематической модели: этот процесс включает в себя множество нетривиальных шагов, а читателю даётся предупреждение о том, что это не самое простое предприятие. В одной из завершающих глав авторы, авторитетные эксперты в области тематического моделирования, называют доступность важнейшей нерешённой проблемой в тематическом моделировании: ``первоочередная исследовательская задача тематических моделей... сделать их более доступными''.

Вышедшая в том же году публикация \cite{lee2017human} показывает, что не-специалисты нуждаются в понятных инструментах для непосредственного исправления ``плохих'' тем, а существующих средств недостаточно. Исследование предлагает несколько мер, улучшающих опыт взаимодействия (user experience) с существующими тематическими моделями, в основном касающихся визуализации темы для пользователей.

Статья от другой группы исследователей \cite{agrawal2018wrong} демонстрирует необходимость подбора гиперпараметров для популярных существующих тематических моделей. Если использовать эти модели ``с настройками по умолчанию'', то это не даст хороших результатов и может привести к большой неустойчивости.

Резюмируя всё это, можно сказать, что у области тематического моделирования имеется высокий порог входа, не позволяющий не-специалистам эффективно использовать этот инструмент для решения своих задач.

\subsection{Подбор гиперпараметров}

Как показывает ряд недавних публикаций \cite{agrawal2018wrong,fan2019assessing}, настройка гиперпараметров даже такой простой модели, как латентное размещение Дирихле, может давать значительный прирост качества. К сожалению, большинство прикладных работ либо не занимаются настройкой гиперпараметров, либо производят её слабо специфицированным образом \cite{agrawal2018wrong}.

Ещё более проблематичным является вопрос определения гиперпараметра $T$, числа тем. Этот ключевой гиперпараметр присутствовал уже в двух самых ранних тематических моделях (PLSA и неотрицательное матричное разложение) и используется в широком числе тематических моделей (в том числе и предложенных недавно, таких как \cite{zosa-granroth-wilding-2019-multilingual}). К сожалению, как показывает большой обзор 167 статей \cite{chen2016survey}, большинство работ либо не указывают использованное значение $T$ вовсе (45\%), либо приводят его без разъяснений о том, как оно было выбрано (33\%). Ряд работ использует крайне непрозрачную методологию определения этой величины.

В подходе АРТМ ситуация ещё более усложняется, поскольку в распоряжении исследователя оказывается внушительный арсенал регуляризаторов и их произвольный комбинаций, каждый из которых имеет настраиваемые параметры.

Можно заключить, что подбор гиперпараметров --- болевая точка области тематического моделирования.


