\chapter{Вероятностное тематическое моделирование}

Тематическое моделирование – это направление исследований на стыке машинного обучения и обработки естественного языка. Вероятностная тематическая модель коллекции текстовых документов описывает каждый документ дискретным вероятностным распределением на множестве тем, а каждую тему~--- дискретным вероятностным распределением на множестве слов. Наряду со словами могут использоваться словосочетания, теги, категории и даже нетекстовые сущности, поэтому для общности будем говорить не о словах, а о \textit{токенах} или термах.

В данной главе рассматриваются постановка задачи вероятностного тематического моделирования и основы теории аддитивной регуляризации тематических моделей согласно \cite{vorontsov2014additive,kochedykov2017fast}.

\section{Задача тематического моделирования}

% Например, тема, которую можно связать с понятием ``театр'',  будет представлять собой распределение, сконцентрированное в словах наподобие \texttt{актёр}, \texttt{пьеса}, \texttt{премьера}, \texttt{партер}, \texttt{зритель} (при этом вероятности слов \texttt{заём} и \texttt{млекопитающее} будут незначительными или даже нулевыми).

Тематическая модель описывается двумя распределениями: $p(w \mid t)$ (вероятность того, что тема $t$ породит токен $w$) и $p(t \mid d)$ (пропорция темы $t$ в документе $d$). Часто используется обозначения $\phi_{wt} = p(w \mid t), \theta_{td} = p(t \mid d)$. Два искомых распределения можно тогда представить в виде стохастических матриц $\Phi$ и $\Theta$.

С точки зрения тематического моделирования, корпус документов представляет собой последовательность трёхэлементных кортежей ${\Omega_n = \bigl\{ (w_i,d_i,t_i) \bigm| i=1,\dots,n \bigr\}}$.
Токены~$w_i$ и~документы~$d_i$ являются наблюдаемыми переменными,
а темы~$t_i$ являются скрытыми переменными.

Тематическое моделирование основано на формализации двух наблюдений лингвистической природы.
\begin{enumerate}
\item{Существуют естественные кластеры слов, употребляемых вместе (темы).}
\item{В разных документах эти темы встречаются с разной частотой.}
\end{enumerate}

Главное допущение, лежащее в основе тематического моделирования, проще всего сформулировать через процесс генерации документов. Тематическая модель «считает», что каждый документ порождается следующим образом:
\begin{enumerate}
    \item{Для каждой словопозиции $i$ автор документа $d_i$ выбирает случайную тему $t$ из распределения $p(t \mid d_i)$}
    \item{Из распределения $p(w \mid d_i,t)$ выбирается случайное слово $w_i$}
\end{enumerate}

Кажущаясся нереалистичность этого допущения на практике смягчается тем, что порядок слов в документе не важен для определения его тематики. Модель оперирует только с величинами $n_{dw}$, обозначающими число вхождений слова $w$ в документ $d$. Это предположение, называемое  \textit{гипотезой мешка слов}, сильно упрощает математический аппарат и поэтому часто используется в анализе текстов.

Ещё одно важное допущение, связанное с постановкой задачи, называется \textit{гипотезой условной независимости}. Частота токена зависит только от его темы, но не зависит от документа:
\begin{equation}
    p(w| d,t) = p(w| t).
\end{equation}

Тема может описывать множество терминов из какой-либо области: например, тема «театр» включает в себя слова «зритель», «опера», «премьера» и не включает в себя слова «космонавтика», «эмпиризм», «кредит» или «гемоглобин».

Однако темы могут иметь и более гибкую природу. Например, при анализе художественных текстов можно выделить темы, связанные с определённым персонажем; при анализе обращений в техподдержку можно выделить темы, связанные с конкретными проблемами; в анализе тональности текста (sentiment analysis) можно выделять темы, связанные с определёнными эмоциями.

Несмотря на то что тематическое моделирование предназначалось в первую очередь для поиска скрытых тем в текстовых документах, существуют и более экзотические сферы его применения:

\begin{itemize}
    \item Анализ видеозаписей. Видеозапись является документом, в качестве токенов выступают признаки на его кадрах, а в качестве тем – события; например, проезд автомобиля через перекрёсток \cite{sparse}.
    \item Анализ изображений. Документ --- это одно изображение, его токены --- признаковое описание этого изображения, темы --- находящиесся на изображении объекты \cite{mml1}.
    \item Судовые журналы кораблей. Документ --- это одно путешествие, токен --- координаты корабля в определённый день, а темы --- различные <<миссии>>, такие как перевозка грузов в определённый порт или китобойный промысел \cite{dh_sea}.
    \item Банковские транзакции. Документ --- история покупок одного клиента, токен --- код продавца, темы --- типы потребительского поведения людей (например, <<ремонт квартиры>>) \cite{egorov2019topic}.
    \item Анализ электрокардиосигналов. Документ --- одна кардиограмма, токены --- кодограммы (полученные на основе амплитуд и интервалов), темы --- отдельные заболевания \cite{shapulin}
    \item Анализ музыкальных произведений. Документ --- одно произведение, токены --- последовательность тональных высотных классов (tonal pitch-classes), темы --- тональные профили \cite{moss2019transitions}
\end{itemize}

Для общности изложения далее мы будем использовать понятие \textbf{токена} вместо понятий термина или слова.

\section{Классические тематические модели}

Исторически тематическое моделирование вырастает из метода анализа многомерных данных, называемого \textit{факторным анализом} (ФА). Две основные цели ФА --- это выявление закономерностей в данных и нахождение более компактного способа описания данных. Как правило, в процессе ФА исследователь раскладывает данные на линейно независимые компоненты, называемые <<факторами>>, затем отсекает незначимые  факторы.

В работе \cite{deerwester1990indexing} было предложено применить факторный анализ к задаче информационного поиска (information retrieval). Этот подход получил название \textit{латентного семантического анализа} или \textit{латентной семантической индексации} (Latent Semantic Indexing, LSI).

По любому документу $d$ можно построить набор чисел $v$, в компоненте $i$ которого будет указано $n_{{w_i}, d}$ --- число вхождений слова $w_i$ в документ $d$. Этот набор чисел можно рассмотреть как вектор в пространстве высокой размерности. Также важную роль в латентном семантическом анализе играет \textit{терм-документная матрица}, элемент которой на позиции $(w, d)$ равен $n_{wd}$ --- числу вхождений токена $w$ в документ $d$ и её разложение в произведение трёх матриц определённого вида.

Ключевая идея латентного семантического анализа заключается в том, что документы следует отображать в пространство более низкой размерности. Таким образом выявляется скрытая структура коллекции документов, позволяющая, например, сравнивать друг с другом пары документов, не имеющие общих слов.

Латентный семантический анализ был математически проанализирован в работе \cite{papadimitriou1998latent}: авторы рассмотрели корпус документов, сгенерированных определённым вероятностным распределением, и доказали ряд результатов, касающихся геометрических свойств LSI. Использованное ими распределение фактически являлось упрощённой тематической моделью и описывалось в сходных терминах.

В 1999 году в работе \cite{hofmann1999probabilistic} Хофманн рассмотрел статистическое обобщение задачи латентного семантического анализа и сформулировал задачу вероятностного латентного семантического анализа (PLSA), которая является базовой моделью в вероятностном тематическом моделировании.
Введённые им понятия и метрики качества используются в тематическом моделировании и по сей день. Также в этой статье Хофман описал EM-алгоритм построения модели и экспериментально изучил его свойства.

PLSA можно считать прямым наследником LSA, поскольку его тоже можно проинтерпретировать в терминах матричного разложения. Терм-документная матрица представляется в виде произведения трёх матриц, задающих распределения вероятностей: $p(d|t)$, $p(w|t)$ и диагональной матрицы с диагональю $p(t)$.

Модель PLSA была обобщена Блеем, Ыном и Джорданом в работе  \cite{blei2003latent}. Они предложили накладывать дополнительное ограничение на модель, предполагая, что столбцы матриц $\Phi$ и $\Theta$ порождаются распределениями Дирихле. Модель латентного размещения Дирихле (Latent Dirichlet Allocation, LDA) отличается от PLSA тем, что присутствует двухуровневая вероятностная порождающая модель, а для определения её параметров было предложено использовать математический аппарат байесовского вывода вместо условий Каруша--Куна--Таккера для задачи математического программирования, как это было сделано для модели PLSA.

% В рамках LDA процесс генерации коллекции документов считается таким:

% \begin{itemize}
%     \item{Для каждого документа $d$:}
%     \begin{enumerate}
%     \item{Автор выбирает число слов $N$ в документе $d$ (как правило, из распределения Пуассона)}
%     \item{Из распределения Дирихле генерируется вектор $\theta$, который можно трактовать как распределение $p(t \mid d)$}
%     \item{Для каждого из $N$ слов $w$ в документе $d$:}
%         \begin{enumerate}
%         \item{Автор выбирает случайную тему $t$, руководствуясь распределением $p(t \mid d)$}
%         \item{Из распределения $p(w \mid t)$ выбирается случайное слово $w$}
%         \end{enumerate}
%     \end{enumerate}
% \end{itemize}

Главным достоинством LDA является способность генерировать новые документы. Также считается, что LDA более устойчива к переобучению, хотя в этом вопросе нет окончательной ясности.

Большинство тематических моделей сегодня строится на основе LDA.

\section{Аддитивная регуляризация тематических моделей}

Для обучения параметров тематической модели $\phi_{wt}=p(w|t)$ и $\theta_{td}=p(t|d)$ в модели PLSA применяется принцип максимума  правдоподобия:
\begin{equation}
\label{eq:logL}
    \sum_{d\in D} \sum_{w\in d} n_{dw} \ln
        \sum_{t\in T}
            \phi_{wt}\theta_{td}
    \;\to\; \max_{\Phi,\Theta}.
\end{equation}

В тематическом моделировании было предложено множество моделей, базирующихся на PLSA и LDA, которые постепенно усложняли вероятностную модель за счёт учёта связей между документами \cite{cohn2001missing,mccallum2005author,nallapati2008link}, метаданных о документах \cite{steyvers2004probabilistic},
времени и мультиязычности \cite{zosa-granroth-wilding-2019-multilingual}, и даже информации о порядке слов в документе \cite{gruber2007hidden,wallach2006topic}. Возможность подобных расширений играет важную роль в приложениях тематического моделирования \cite{fntir2017applications}.

Традиционный способ построения новых тематических моделей путём обобщения модели LDA в рамках байесовского обучения описан в  \cite{fntir2017applications}.
Рекомендации включают в себя:
введение новой вероятностной модели коллекции документов (которая не должна быть слишком вычислительно сложной, оставаясь при этом реалистичной);
нахождение нового алгоритма оценки апостериорного распределения параметров;
реализацию этого алгоритма (при этом новая модель может оказаться несовместимой с известными вычислительными оптимизациями);
валидацию результатов моделирования в экспериментах на реальных данных.
В целом можно заметить, что построение тематических моделей, удовлетворяющих нескольким различным требований одновременно, остаётся трудной задачей в рамках байесовского обучения.

\subsection{Тематическое моделирование в рамках подхода ARTM}

Чтобы обойти обозначенные недостатки описанных алгоритмов и в итоге получить простую, но гибкую и легко расширяемую модель для вероятностного тематического моделирования, был предложен подход аддитивной регуляризации тематических моделей (Additive Regularization for Topic Models, ARTM) \cite{vorontsov2014additive,vorontsov2014tutorial,vorontsov2015additive}.

В отличие от байесовского подхода (где тематическая модель описывается в терминах апостериорных распределений), в подходе ARTM тематическое моделирование рассматривается как некорректно поставленная задача неотрицательного матричного разложения, требующая введения дополнительных критериев~--- регуляризаторов для получения наиболее подходящего и устойчивого решения.

При заданной матрице частот слов в документах  $(n_{dw})$ размера $W{\times}D$, где $n_{dw}$ обозначает количество раз, которое слово  $w$ встретилось в документе $d$, необходимо найти стохастические матрицы $\Phi,~\Theta$, произведение которых приблизительно равно нормированной стохастической матрице $\bigl(\frac{n_{dw}}{n_d}\bigr)$. В качестве целевой метрики, измеряющей качество этого приближения, принято использовать логарифм правдоподобия (или перплексию).

Обобщением этой интерпретации является многокритериальный подход, в котором к основному критерию логарифма правдоподобия $L(\Phi, \Theta)$
добавляется взвешенная сумма регуляризаторов $R(\Phi, \Theta) = \sum_i \tau_i R_i(\Phi, \Theta)$. В результате получаем следующую оптимизационную задачу при ограничениях неотрицательности и~нормировки:
\begin{gather}
\label{eq:logL+R}
    \sum_{d\in D} \sum_{w\in d}
        n_{dw}\ln
        \sum_{t\in T}
            \phi_{wt}\theta_{td}
    + R(\Phi,\Theta)
    \;\to\; \max_{\Phi,\Theta};
    \qquad
    R(\Phi,\Theta)
    = \sum_{i=1}^k \tau_i R_i(\Phi,\Theta);
\\
\label{eq:logL.constraints}
    \sum_{w\in W} \phi_{wt} = 1;
    \quad
    \phi_{wt}\geq 0;
    \qquad
    \sum_{t\in T} \theta_{td} = 1;
    \quad
    \theta_{td}\geq 0.
\end{gather}

За счёт аддитивности регуляризаторов алгоритм ARTM  выводится в общем виде для произвольного~$R$, поэтому оптимизация любых моделей  производится при помощи одного и того же итерационного процесса~--- обобщённого EM-алгоритма  \cite{vorontsov2014additive,vorontsov2015}. Важнейшую роль в АРТМ играет следующая теорема:
\begin{Theorem}
\label{th:ARTM}
    Пусть функция $R(\Phi,\Theta)$ непрерывно дифференцируема.
    Тогда точка $(\Phi,\Theta)$ локального экстремума задачи
    \eqref{eq:logL+R} с~ограничениями \eqref{eq:logL.constraints}
    удовлетворяет системе уравнений со вспомогательными переменными $p_{tdw}=p(t\cond d,w)$,
    если из решения исключить нулевые столбцы матриц $\Phi$, $\Theta$:
    \begin{align}
    \label{eq:ARTM.Estep}
        p_{tdw} &=\norm_{t\in T} \bigl( \phi_{wt}\theta_{td} \bigr);
    \\\label{eq:ARTM.Mstep.phi}
        \phi_{wt} &=\norm_{w\in W}
            \biggl(
                n_{wt} + \phi_{wt} \frac{\partial R}{\partial \phi_{wt}}
            \biggr);
        &
        n_{wt} &= \sum_{d\in D} n_{dw}p_{tdw};&
    \\\label{eq:ARTM.Mstep.theta}
        \theta_{td} &=\norm_{t\in T}
            \biggl(
                n_{td} + \theta_{td} \frac{\partial R}{\partial \theta_{td}}
            \biggr);
        &
        n_{td} &= \sum_{w\in d} n_{dw}p_{tdw}.&
    \end{align}
\end{Theorem}

Получаемая в результате система уравнений допускает решение методом простых итераций, который можно интерпретировать как ЕМ-алгоритм. На E-шаге обновляются условные распределения по темам $p_{tdw}=p(t\cond d,w)$ для каждого слова $w$ в каждом документе $d$. На M-шаге обновляются значения $\Phi$ и $\Theta$:
\begin{equation} \label{eq:Mstep_Theta}
\phi_{wt}^{new} =\norm_{w\in W} \left( \sum_{d} n_{dw} p_{tdw} + \phi_{wt}^{old} \frac{\partial{R}}{\partial{\phi_{wt}}}\right)
\end{equation}
\[
\theta_{td}^{new}   =\norm_{d\in D} \left( \sum_{w} n_{dw} p_{tdw} + \theta_{td}^{old} \frac{\partial{R}}{\partial{\theta_{td}}}\right).
\]

Из формул видно, что для того, чтобы добавить новый регуляризатор, нужно знать лишь $\frac{\partial{R}}{\partial{\phi_{wt}}}, \frac{\partial{R}}{\partial{\theta_{td}}}$, его частные производные по параметрам модели. Таким образом, ARTM~---~это не одна частная тематическая модель, а общий подход к их построению и комбинированию.

\subsection{Мультимодальное тематическое моделирование}

Аналогичным образом можно учитывать дополнительные данные о документах.
В классическом тематическом моделировании считается, что документы содержат лишь слова $w \in W$, однако на практике данные о документах могут включать метки из конечных множеств другой природы~--- это, например, авторы документа, категории, теги, дата, место и источник публикации, данные о читателях или пользователях.

Будем считать, что эти метки имеют $m$ различных видов, называемых  \textit{модальностями}, одной из которых является модальность слов (помимо которой можно рассматривать, например, модальности авторов, тегов, времени и другие). Естественным образом обобщив вероятностную модель на этот случай, можно получить следующее выражение для максимизации взвешенного правдоподобия коллекции:
\[
    L(\Phi^m, \Theta) = \sum_m \tau_m \sum_{d\in D} \sum_{w \in W^m} n_{dw} \ln p(w \mid d) \rightarrow \max,
\]
где $\tau_m$ --- вес модальности.
Задачи такого вида можно единообразно решать при помощи модифицированного мультимодального ЕМ-алгоритма \cite{voron15nonbayesian}.

\section{Приложения тематического моделирования}

% Early work on topic modeling conceptualized it as an intermediate stage of information retrieval pipeline. The possibility of meaningful interpretation was an afterthought. For measuring the quality of topics when evaluated against human judgments, several metrics were proposed.

Тематические модели доказали свою применимость в широком спектре прикладных задач \cite{fntir2017applications}. Их используют для информационного поиска \cite{yi2009, wang2011}, категоризации документов \cite{rubin2012}, анализа социальных сетей \cite{varshney2014, pinto2016}, в рекомендательных системах \cite{wang2011,lee2015} и др.

Применения тематического моделирования можно разделить на две условных группы. Во-первых, выход тематической модели является способом векторного представления документа или слов. Такие представления можно использовать при решении задач классификации, ранжирования, информационного поиска в качестве основного или дополняющего признакового описания документов в алгоритмах машинного обучения. Во-вторых, тематическая модель позволяет выявлять тематическую структуру текстовой коллекции, что важно для понимания данных в целом.

\subsection{Анализ программных продуктов}

Тематическое моделирование также применяется для анализа программных артефактов \cite{sun2016exploring,chen2016survey}. Два самых популярных приложения охватывают около половины всех работ в этой области \cite{chen2016survey}.

Первое из них --- восстановление прослеживаемости (traceability link recovery), предназначенное для автоматического выявления связей между парами разнородных программных артефактов, таких как документы с исходным кодом и документы с требованиями \cite{asuncion2010software}. Возможность ответить на вопрос <<Какой файл (какие файлы) с исходным кодом реализует требование (функционал) X?>> важна как для разработчиков, так и для заказчика. Близкой является задача локализации ошибок (bug localization), при которой ищется связь между баг-репортами и исходным кодом.

Второе приложение --- локализация функционала (concept location/feature location), предназначенная для идентификации кусков исходного кода, связанных с реализацией данного функционала программной системы \cite{dit2013feature}. Здесь, в отличие от восстановления прослеживаемости, от модели требуется построить связи между однородными программными артефактами (т.е. между исходными кодами, лежащими в одном и том же репозитории). Эта задача в первую очередь актуальна для разработчиков, желающих отладить или улучшить данную функцию.

Также стоит отметить: кластеризацию и визуализацию кода  (Source Code Comprehension) для помощи в понимании и ориентировании в неструктурированном массиве текстов, основой которого служат идентификаторы переменных и комментарии; автоматическую разметку программных артефактов (software artifact labeling), предназначенную для характеризации большого документа коротким множеством ключевых слов \cite{de2012using}; прогнозирование возможных дефектов (Software Defects Prediction) посредством анализа исходников или логов выполнения; помощь в  устранении дефектов посредством статистической отладки и анализа причин (Statistical Debugging и Root Cause Analysis), включающую в себя интеллектуальную фильтрацию логов исполнения.

К прочим применениям можно отнести анализ истории изменений (Software History Comprehension), маршрутизацию задач к наиболее подходящему разработчику (Developer Recommendation/Bug Triaging), помощь в рефакторинге, приоритизацию исполнения тестовых кейсов, измерение дополнительных мер качества и так далее \cite{sun2016exploring,chen2016survey}.

\subsection{Разведочный информационный поиск}

Разведочный поиск включает в себя большой класс задач, связанных с систематизацией информации, суммаризацией текстов, переработкой знаний. Разведочный поиск --- это итерационный процесс, в котором пользователь несколько раз переформулирует свой запрос с целью разобраться в новой для себя предметной области.

Тематические модели оказываются наиболее актуальными для обработки длинных или сложных поисковых запросов, которые трудно  или невозможно сформулировать в виде короткого списка ключевых слов (в том числе и для поиска документов, семантически близких к данному). В недавних работах \cite{ianina2017multi, ianina2019regularized} показано, что аддитивно регуляризованные тематические модели по качеству поиска превосходят как асессоров, так и конкурирующие модели.

\section{Интерпретируемость тематических моделей}

Тематическое моделирование не обязательно показывает наилучшие результаты с точки зрения численных измерений качества для прикладных задач, но за ним явное преимущество, когда речь заходит об интерпретируемости. Каждую тему можно интерпретировать как вероятностное распределение над токенами в этой теме и описать частотным словарём слов или словосочетаний естественного языка. Это свойство тематических моделей делает их более подходящими для задач анализа данных, где важна возможность объяснения как самой модели, так и принимаемых с её помощью решений, в терминах, понятных экспертам.

Вторая область применения~--- извлечение структуры коллекции~--- является уникальной для тематического моделирования. Часто исследователь ищет ответы на вопросы о структуре и природе коллекции, а не пытается оптимизировать заданную меру качества. Такой подход, называемый <<дальним чтением>> (distant reading) \cite{milkova2019distant}), используется в  биологии \cite{Liu2016,funnell2019integrated} и гуманитарных науках \cite{fntir2017applications,antons2019content}. Благодаря ему возможно получить общее представление о тематической кластерной структуре больших объёмов данных, которое в противном случае могло бы быть упущено исследователем.

Примером такого использования тематического моделирования служит проект  Mining the Dispatch \cite{monsters_men,dispatch}, посвящённый анализу более ста тысяч статей из газеты Richmond Daily Dispatch времён Гражданской войны в США. Главная мотивация этого проекта состоит в том, что повседневная жизнь в Ричмонде, столице Конфедеративных Штатов Америки, до сих пор оставалась малоизученной.
В этой работе выявляются основные темы газеты и различные аномалии. Например, <<объявления о сбежавшем рабе>> (в результате этого анализа был замечен резкий и необъяснимый всплеск числа таких объявлений в начале 1862 года, что даёт основу для дальнейших исследований с использованием традиционных исторических методов), <<антисеверная пропаганда>> или <<патриотизм и поэзия>> (было замечено, что последние две темы очень часто появляются вместе). Автор отмечает, что подобные закономерности было бы практически невозможно обнаружить традиционными методами.

Ещё одной интересной областью гуманитарного применения тематического моделирования является литературный анализ. В работе \cite{buurma2015fictionality} анализируются работы Энтони Троллопа, известного романиста Викторианской эпохи. Исследователь выявляет ряд тем, часто встречающихся в творчестве писателя, и обосновывает точку зрения о том, что тематическая модель позволяет увидеть работы с непривычной точки зрения (например, наличие посвящённой письмам темы помогает обнаружить частые отсылки к эпистолярному жанру). Исследователь замечает, что полученные темы можно рассматривать как составные части книг, которые писатель мог бы гипотетически написать.

Также при помощи тематического моделирования удобно изучать историю развития темы, примером чего служит работа \cite{goldstone2012can}. Авторы показывают принципиальную возможность анализа статей в журнале Publications of the Modern Language Association of America при помощи латентного размещения Дирихле. Авторы строят две разные тематические модели, сравнивают их и отмечают, что сделанные на их основе выводы согласуются друг с другом. Затрагивается вопрос о том, может ли тематическая модель быть аргументом в пользу какой-то гипотезы.

Также можно упомянуть работу~\cite{wang2016catching}, в которой была построена тематическая модель, описывающая твиты Дональда Трампа. На основании пропорций тем, полученных из этой модели, была создана модель, предсказывающая число <<лайков>>. Авторы приходят к выводу, что самой <<любимой>> темой читателей Трампа является тема \texttt{Демократы}.

Актуальным кроссдициплинарным применением тематического моделирования является работа \cite{franz2020using}, в которой анализируется корпус сообщений с TeenHelp.org, форума  поддержки подростков в трудных и кризисных ситуациях. Исследователи фокусируются на категории постов, относящихся к самоповреждениям, суицидам, депрессии и семейным проблемам. Было показано, что темы модели, построенной на корпусе 2535 постов, соотносятся с экспертной разметкой, проведённой на случайной подвыборке из 500 постов. 10 из 15 построенных тем оказались хорошо интерпретируемыми. Были замечены статистические связи между распределением тем в сообщениях и демографическими характеристиками (пол, возраст) авторов сообщений.
Исследователи видят практическое применение своей работы в возможности раннего выявления предикторов риска аутодеструктивных проявлений у подростков, а также в способности более тонко дифференцировать значимые темы (депрессия, суицид), связанные с психическим здоровьем.

% Много внимания уделяется и анализу переписки разработчиков на сайте Stack Overflow \cite{barua2014developers, linares2013exploratory, allamanis2013and, rosen2016mobile}

\section{Проблемы тематического моделирования}

Приложения тематического моделирования в различных областях наталкиваются на проблемы плохой интерпретируемости тем, дублирующих, мусорных и вводящих в заблуждение тем, неустойчивости результатов моделирования.
Темы должна удовлетворять одновременно многим критериям, что вызывает необходимость как измерения, так и улучшения качества тематических моделей одновременно по множеству критериев.

\subsection{Неинтерпретируемые темы}

Не все построенные темы можно однозначно проинтерпретировать. Это ожидаемо: коллекции реальных текстов содержат в себе аномальные документы, неинформативные слова (к числу которых относятся как стоп-слова или слова общей лексики, так и слова, широко распространённые в конкретном корпусе), опечатки и другие артефакты. При этом тематическая модель должна успешно описывать весь корпус, и поэтому таким артефактам нужно найти место.

Тем не менее на практике важно минимизировать количество неинтерпретируемых тем и постараться сделать так, чтобы <<хорошие>> темы не перемешивались с <<плохими>> и не засорялись <<мусорными словами>>.

В работе \cite{mimno} были отмечены некоторые частые проблемы, связанные с темами низкого качества.
\begin{enumerate}
\item{Смешанные темы (mixed topics): набор слов, которые трудно проинтерпретировать вместе, но который содержит хорошо интерпретируемые подмножества слов. Можно сказать, что смешанные темы --- это несколько <<хороших>> тем, которые слиплись вместе.}
\item{Темы-химеры (chained topics): смешанные темы, объединённые общими словами. В качестве примера такой темы можно привести тему из слов <<старение - человек - глобин>>. Пары <<старение человек>> и <<глобин человек>> имеют смысл, но пару <<старение глобин>> объединяет лишь то, что оба слова часто встречаются рядом со словом <<человек>>. Темы-химеры часто возникают из-за слов-омонимов. Например, тема <<reagan, roosevelt, clinton, lincoln, honda, chevrolet, bmw>> объединяет в себе имена президентов США и марки автомобилей, слипшихся из-за неоднозначного слова <<Линкольн>>) или из-за иерархических отношений между словами (общее понятие <<налог>> включает в себя <<налог на автомобиль>> и <<налог на прибыль>>, из-за чего может появиться тема, содержащая в себе редкое сочетание <<автомобиль прибыль>>.}
\item{Темы с посторонними словами (intruded topics): в целом осмысленные темы, внутри которых есть слова, не имеющие ничего общего с остальными словами.}
\item{Случайные темы: набор слов, не имеющих отношения друг к другу.}
\item{Несбалансированные по охвату темы: содержат внутри себя термины как широкой направленности, так и крайне специфические. Например, <<сигнальная трансдукция>> (общее название передачи сигнала в молекулярной биологии) и <<сигнальный путь Notch>> (один из многих сигнальных путей, которые, в свою очередь, являются одним из аспектов сигнальной трансдукции).}
\end{enumerate}

Отметим, что многие из перечисленных здесь пунктов могут делать потенциально информативные темы некачественными.

Этот список проблем далее был уточнён и переработан в работе \cite{boydcare}, в которой дополнительно описываются темы с излишней общностью (состоящие из слишком частых и/или неспецифичных токенов), темы с избыточной специфичностью (состоящие из слишком редких и/или специфичных токенов), идентичные темы и темы, <<испорченные>> неполным списком стоп-слов.

Другой класс явных проблем можно определить через расстояние до известных <<мусорных>> тем: D-Background, W-Vacuous, W-Uniform, среднее геометрическое, равномерное распределение $p(t)$; или по другим мерам качества (более подробно эти меры качества изложены в следующей главе).

Несмотря на то что суммарное качество для модели хорошо коррелирует с общей интерпретируемостью, разбиение мер качества по отдельным темам не всегда коррелирует с экспертной оценкой интерпретируемости индивидуальных тем. В ряде случаев экстремальные по каким-либо мерам качества темы (то есть как <<самые хорошие>>, так и <<самые плохие>>) являются неинтерпретируемыми или низкокачественными с точки зрения экспертов.

\subsection{Вводящие в заблуждение темы}

В противовес явным проблемам, изложенным выше, работа \cite{dh_sea} предостерегает исследователей от ряда возможных неявных проблем: <<Упрощение тематических моделей для ученых-гуманитариев, которые не будут (и не должны) изучать лежащие в их основе алгоритмы, создаёт огромный потенциал для необоснованных --- или даже вводящих в заблуждение --- ``инсайтов''>>.

Примером такого проблематичного применения может служить исследователь, изучающий график зависимости вероятности темы от времени и рассматривающий одну тему отдельно от всех остальных. Взлёты и падения этой темы он интерпретирует в терминах изменения частоты понятия, характеризуемого её множеством из 10 верхних (наиболее частотных, <<топовых>>, top-10) слов. Автор работы призывает этого исследователя к осторожности и обосновывает свой тезис рядом демонстраций.

Первая демонстрация изучает роль верхних токенов  в описании темы. Для этого тематическое моделирование применяется в экзотической области анализа географических данных: роль корпуса документов играют судовые журналы, токены которых --- координаты корабля в один из дней его путешествия. Главной особенностью этого эксперимента является возможность визуализации всего распределения целиком --- задача, которая, как правило, нереалистична в большинстве практических приложений.

Автор демонстрирует нерепрезентативность верхних токенов при помощи карты Земли с нанесёнными на неё точками (визуализация заданной темы состоит из линий, соответствующих документам/путешествиям, и из точек, соответствующих верхним токенам/самым частым местоположениям). В большинстве случаев видно, что верхние токены дают очень плохое представление о теме; большинство верхних токенов расположено на границах маршрута и не несёт никакой информации о его форме.

Вторая демонстрация проверяет темы на однородность по времени. Коллекция статей журнала Proceedings of the Modern Language Association растянута по времени с 1889 года по 2006 год. Это даёт возможность <<разбить построенную тему на две части>>: отметить все отнесённые к ней слова во всех документах, затем поделить документы на две хронологические группы (с 1889 по 1959 год и с 1959 по 2006 год) и вычислить частоты этих слов отдельно внутри каждой группы. Полученные таким образом пары списков верхних слов можно сравнить. Около четверти всех тем имеет существенные различия и может иметь две несовместимые интерпретации для этих двух временных периодов.

На основании этих рассуждений автор заключает, что для популяризации тематического моделирования среди учёных-гуманитариев требуются средства визуализации, выходящие за рамки списка верхних слов, а также что инструментарий тематического моделирования должен быть обогащён множеством автоматических проверок. Сформированная в работе критика остаётся актуальной и в настоящее время \cite{paakkonen2020humanistic}.

\subsection{Неустойчивость}

Различные реализации тематических моделей могут использовать стохастические элементы в своей инициализации и обучении. Многопоточные реализации алгоритмов обучения могут быть подвержены состоянию гонки (race condition), что делает результат обучения недетерминированным даже в условиях переиспользования одной и той же фиксированной последовательности случайных чисел.

Также известно, что многие реализации тематического моделирования неустойчивы относительно порядка документов в корпусе \cite{agrawal2018wrong}. Более половины из 57 рассмотренных в работе \cite{agrawal2018wrong} статей в том или ином виде упоминает проблему неустойчивости латентного размещения Дирихле.

Это проблематично по ряду причин. Например, затрудняет применение тематических моделей для анализа того, как тематики корпуса эволюционируют во времени или при добавлении новых документов \cite{mehta_clustering_bank}. Если тематическая модель используется для принятия каких-то решений, то неустойчивость может привести к ошибочным выводам, основанным не на объективной структуре корпуса, а на случайных шумоподобных эффектах \cite{mantyla2018measuring}. Это может ввести пользователя в заблуждение и понизить эффективность других автоматических классификаторов, использующих выход тематической модели как обучающую выборку или дополнительные признаки \cite{agrawal2018wrong}.

\subsection{Доступность}

В монографии \cite{fntir2017applications} описываются рекомендации по разработке новой тематической модели: этот процесс включает в себя множество нетривиальных шагов, а читателю даётся предупреждение о том, что это не самое простое предприятие. В одной из завершающих глав авторы, авторитетные эксперты в области тематического моделирования, называют прозрачность и понятность, то есть доступность, моделирования важнейшей нерешённой проблемой в тематическом моделировании: <<Первоочередная исследовательская задача тематических моделей... сделать их более доступными>>.

Вышедшая в том же году публикация \cite{lee2017human} показывает, что не-специалисты нуждаются в понятных инструментах для непосредственного исправления <<плохих>> тем, а существующих средств недостаточно. Исследование предлагает несколько мер, улучшающих опыт взаимодействия (user experience) с существующими тематическими моделями, в основном касающимися визуализации темы для пользователей.

Статья от другой группы исследователей \cite{agrawal2018wrong} демонстрирует необходимость подбора гиперпараметров для популярных существующих тематических моделей. Если использовать данные модели <<с настройками по умолчанию>>, это не даст хороших результатов и может привести к большой неустойчивости.

Резюмируя всё вышеупомянутое, можно сказать, что у области тематического моделирования имеется высокий порог входа, не позволяющий исследователям в смежных специальностях эффективно использовать этот инструмент для решения своих задач.

\subsection{Подбор гиперпараметров}

Как показывает ряд недавних публикаций \cite{agrawal2018wrong,fan2019assessing}, настройка гиперпараметров даже такой простой модели, как латентное размещение Дирихле, может давать значительный прирост качества.  Автор работы \cite{dh_sea} тоже отмечает, что настройка гиперпараметров, скорее всего, поспособствовала бы устранению описанных им проблем, связанных с вводящими в заблуждение темами.  К сожалению, большинство прикладных работ либо не занимается настройкой гиперпараметров, либо производит её слабо специфицированным образом \cite{agrawal2018wrong}.

Ещё более проблематичным является вопрос определения гиперпараметра $|T|$, числа тем. Этот ключевой гиперпараметр присутствовал уже в двух самых ранних тематических моделях (PLSA и неотрицательное матричное разложение) и используется в широком числе тематических моделей, в том числе и в предложенных недавно, таких как \cite{zosa-granroth-wilding-2019-multilingual}. К сожалению, как показывает большой обзор 167 статей \cite{chen2016survey}, большинство работ либо не указывает использованное значение $T$ вовсе (45\%), либо приводит его без разъяснений о том, как оно было выбрано (33\%). Ряд работ использует крайне непрозрачную методологию определения этой величины.

В подходе АРТМ ситуация ещё более усложняется, поскольку в распоряжении исследователя оказывается внушительный арсенал регуляризаторов и их произвольных комбинаций, каждый из которых может иметь настраиваемые параметры.

Можно заключить, что подбор гиперпараметров --- <<болевая точка>> области тематического моделирования.



