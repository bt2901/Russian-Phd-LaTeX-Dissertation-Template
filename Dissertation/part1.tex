\chapter{Вероятностное тематическое моделирование}

\section{Задача тематического моделирования}

Тематическое моделирование – это метод анализа коллекции текстов, оперирующий понятиями \textit{токена} и \textit{темы}. Предполагается, что токен --– это просто слово, а тема описывается как распределение вероятностей над множеством всех токенов. Каждый документ из коллекции текстов связан с некоторым распределением над множеством тем.

% Например, тема, которую можно связать с понятием ``театр'',  будет представлять собой распределение, сконцентрированное в словах наподобие \texttt{актёр}, \texttt{пьеса}, \texttt{премьера}, \texttt{партер}, \texttt{зритель} (при этом вероятности слов \texttt{займ} и \texttt{млекопитающее} будут незначительными или даже нулевыми).

Тематическая модель описывается двумя распределениями: $p(w \mid t)$ (вероятность того, что тема $t$ породит токен $w$) и $p(t \mid d)$ (пропорция темы $t$ в документе $d$). Часто используется обозначения $\phi_{wt} = p(w \mid t), \theta_{td} = p(t \mid d)$. Два искомых распределения можно тогда представить в виде стохастических матриц $\Phi$ и $\Theta$. 

С точки зрения тематического моделирования корпус документов представляет собой последовательность трёхэлементных кортежей ${\Omega_n = \bigl\{ (w_i,d_i,t_i) \bigm| i=1,\dots,n \bigr\}}$.
Токены~$w_i$ и~документы~$d_i$ являются наблюдаемыми переменными,
а темы~$t_i$ являются скрытыми переменными.

Можно сказать, что тематическое моделирование формализует два наблюдения лингвистической природы: 
\begin{enumerate}
\item{существуют естественные кластера слов, употребляемых вместе (темы)}
\item{в разных документах эти темы встречаются в разной степени}
\end{enumerate}

Главное допущение, лежащее в основе тематического моделирования, проще всего сформулировать через процес генерации документов. Тематическая модель «считает», что каждый документ создаётся так: 
\begin{enumerate}
    \item{Автор выбирает случайную тему $t$, руководствуясь распределением $p(t \mid d)$}
    \item{Из распределения $p(w \mid d, t)$ выбирается случайное слово $w$}
\end{enumerate}

Кажущаясся нереалистичность этого допущения на практике смягчается тем, что порядок слов в документе игнорируется. Модель оперирует только с величинами $n_{dw}$, обозначающими число вхождений слова $w$ в документ $d$. Это предположение, называемое  \textit{гипотезой мешка слов}, сильно упрощает математический аппарат и поэтому часто используется в анализе текстов.

Ещё одно важное допущение, связанное с постановкой задачи, называется \textit{гипотезой условной независимости}. Все вероятностные зависимости токенов между собой объясняются исключительно их тематиками:
\begin{equation}
    p(w| d,t) = p(w| t).
\end{equation}

Тема может описывать множество терминов из какой-то области: например, тема «театр» включает в себя слова «зритель», «опера», «премьера» и не включает в себя слова «космонавтика», «эмпиризм», «кредит» или «гемоглобин». 

Однако, темы могут иметь и более гибкую природу. Например, при анализе художественных текстов можно выделить темы, связанные с определённым персонажем; при анализе обращений в техподдержку можно выделить темы про конкретные проблемы; для sentiment analysis можно выделять темы про отдельные эмоции \cite{TODO}.

Несмотря на то, что тематическое моделирование предназначалось в первую очередь для поиска скрытых тем в текстовых документах, существуют и более экзотические сферы его применения:

\begin{itemize}
    \item Анализ видеозаписей. Видеозапись является документом, в качестве токенов выступают признаки на его кадрах, а в качестве тем – события; например, проезд автомобиля\cite{sparse}.
    \item Анализ изображений. Документ --- это одно изображение, его токены --- признаковое описание этого изображения, темы --- находящиесся на изображении объекты \cite{mml1}.
    \item Судовые журналы кораблей. Документ --- это одно путешествие, токен --- координаты корабля в определённый день, а темы --- различные ''миссии'', такие как перевозка грузов в определённый порт или китобойный промысел \cite{dh_sea}. 
    \item Банковские транзакции. Документ --- история покупок одного клиента, токен --- код продавца, темы --- виды деятельности (например, ``ремонт квартиры'') \cite{TODO}.
    \item Кардиология \cite{TODO}
\end{itemize}

Поэтому для того чтобы подчеркнуть общность изложения далее мы будем использовать понятие \textbf{токена} вместо понятий термина/слова.

\section{Классические тематические модели}

Исторически тематическое моделирование вырастает из метода анализа произвольных многомерных данных, называемого \textit{факторным анализом}. Две основные цели ФА --- это выявление закономерностей в данных и нахождение более компактного способа описания данных. Как правило, в процессе ФА исследователь раскладывает данные на линейно независимые компоненты, называемые ''факторами'', а затем отсекает все незначительные факторы. 

В работе \cite{deerwester1990indexing} было предложено применить факторный анализ к задаче информационного поиска (information retrieval). Этот подход получил название \textit{латентного семантического анализа} или \textit{латентной семантической индексации}.

По любому документу $d$ можно построить набор чисел $v$, в компоненте $i$ которого будет указано $n_{{w_i}, d}$ --- число вхождений слова $w_i$ в документ $d$. Этот набор чисел можно рассмотреть как вектор в пространстве высокой размерности. Также важную роль в латентном семантическом анализе играет \textit{терм-документная матрица}, элемент которой на позиции $(w, d)$ равен $n_{wd}$ --- числу вхождений токена $w$ в документ $d$ и её разложение в произведение трёх матриц определённого вида.

Ключевая идея латентного семантического анализа --- в том, что документы следует отображать в пространство более низкой размерности. Таким образом выявляется скрытая структура коллекции документов, позволяющая, например, сравнивать друг с другом пары документов, не имеющие общих слов.

Латентный семантический анализ был математически проанализирован в работе \cite{papadimitriou1998latent}: авторы рассмотрели корпус документов, сгенерированных определённым вероятностным распределением и доказали ряд результатов, касающихся геометрических свойств LSI. Использованное ими распределение фактически являлось упрощённой тематической моделью и описывалось в схожих терминах.

В 1999 году в работе \cite{hofmann1999probabilistic} Хофманн рассмотрел статистическое обобщение задачи латентного семантического анализа и сформулировал задачу вероятностного латентного семантического анализа (PLSA), которая является основой задачи тематического моделирования. 

Введённые им понятия и метрики качества используются в тематическом моделировании и по сей день. Также в этой статье Хофман описал EM-алгоритм построения модели и экспериментально изучил его свойства.

PLSA можно считать прямым наследником LSA, поскольку его тоже можно проинтерпретировать в терминах матричного разложения. Терм-документная матрица представляется в виде произведения трёх матриц, задающих распределения вероятностей (а именно: $p(d|t)$, $p(t)$ и $p(w|t)$).

Модель PLSA была далее дополнена Дэвидом Блеем во влиятельной работе \cite{blei2003latent}. Здесь Блей описал полноценную вероятностную модель коллекции документов (латентное размещение Дирихле). Модель LDA отличается от PLSA тем, что в ней добавляется уровень генерации распределений $p(t|d)$ (считается, что они порождены распределением Дирихле).

В рамках LDA процесс генерации коллекции документов считается таким:

\begin{itemize}
    \item{Для каждого документа $d$:}
    \begin{enumerate}
    \item{Автор выбирает число слов $N$ в документе $d$ (как правило, из распределения Пуассона)}
    \item{Из распределения Дирихле генерируется вектор $\theta$, который можно трактовать как распределение $p(t \mid d)$}
    \item{Для каждого из $N$ слов $w$ в документе $d$:}
        \begin{enumerate}
        \item{Автор выбирает случайную тему $t$, руководствуясь распределением $p(t \mid d)$}
        \item{Из распределения $p(w \mid t)$ выбирается случайное слово $w$}
        \end{enumerate}
    \end{enumerate}
\end{itemize}

Главным достоинством LDA является способность генерировать новые документы. Также считается что LDA более устойчива к переобучению хотя в этом вопросе нет окончательной ясности.

Большинство тематических моделей сегодня строятся на основе LDA.

\section{Аддитивная регуляризация тематических моделей}

Тематическую модель достаточно естественно описывать в терминах байесовского вывода.  Задаваемые ей распределения вероятностей $p(w|t)$ и $p(t|d)$ – это отражение знаний некого условного агента, изучившего коллекцию текстов. Эти распределения задаются условием максимизации правдоподобия коллекции (L). 

В рамках байесова подхода было предложено множество моделей, базирующихся на подходе LDA, которые постепенно усложняли вероятностную модель за счёт учёта связей между документами \cite{cohn2001missing,mccallum2005author,nallapati2008link}, метаданных о документах \cite{steyvers2004probabilistic}, 
времени и мультиязычности \cite{zosa-granroth-wilding-2019-multilingual}, и даже информации о порядке слов в документе \cite{gruber2007hidden,wallach2006topic}. Возможность подобного расширения играет важную роль в популярности подхода LDA \cite{fntir2017applications}.

Традиционный способ построения новых тематических моделей описан в  \cite{fntir2017applications}. Рекомендации включают в себя: введение новой вероятностной модели коллекции документов (которая не должна быть слишком вычислительно сложной, оставаясь при этом реалистичной); нахождение нового алгоритма оценки апостериорного распределения параметров; реализацию этого алгоритма (при этом, новая модель может оказаться несовместимой с известными вычислительные оптимизациями); валидацию результатов. В целом можно заметить, что построение тематических моделей, удовлетворяющих нескольким различным требований одновременно, остаётся трудной задачей.

\subsection{Тематическое моделирование внутри подхода ARTM}

Чтобы обойти обозначенные недостатки описанных алгоритмов, и в итоге получить простую, но гибкую и легко расширяемую модель для вероятностного тематического моделирования, был предложен подход Аддитивной Регуляризации Тематических Моделей (Additive Regularization for Topic Models, ARTM) \cite{vorontsov2014additive,vorontsov2014tutorial,vorontsov2015additive}. 

В отличии от байесовского подхода (где тематическое моделирование описывается в терминах апостериорных распредлений), в подходе ARTM тематическое моделирование является задачей неотрицательного матричного разложения. При заданной матрице совстречаемостей ``слово в документе''  $n_{dw}$ (где $n_{dw}$ обозначает количество раз, которое слово  $w$ встретилось в документе $d$), необходимо найти стохастические матрицы $\Phi,~\Theta$ произведение которых приблизительно равно нормированной матрице $n_{dw}$. Правдоподобие (или перплексия) конкретной модели интерпретируется как целевая метрика, отражающая качество этого приближения.

Обобщением этой интерпретации является многокритериальный подход, в котором к основному критерию правдоподобия $L(\Phi, \Theta)$ 
добавляется взвешенная сумма регуляризаторов $R(\Phi, \Theta) = \sum_i \tau_i R_i(\Phi, \Theta)$. В результате получаем следующую оптимизационную задачу:

\begin{equation} \label{eq:EM}
L(\Phi, \Theta) + R(\Phi, \Theta) \to \max_{\Phi, \Theta},
\end{equation}

За счёт аддитивности алгоритм ARTM  выведен и реализован один раз в самом общем виде, и поэтому оптимизация любых моделей  производится при помощи одного и того же итерационного процесса  (обобщённого EM-алгоритма)  \cite{vorontsov2014additive,vorontsov2015}. Мы приведём здесь две основные формулы итеративного обновления параметров $\Phi$ и $\Theta$:

\begin{equation} \label{eq:Mstep_Theta}
\Phi_{wt}^{new}  \propto \left( \sum_{d} n_{dw} p_{tdw} + \Phi_{wt}^{old} \frac{\partial{R}}{\partial{\Phi_{wt}}}\right)_{+}.
\end{equation}
\[
\Theta_{td}^{new}  \propto \left( \sum_{w} n_{dw} p_{tdw} + \Theta_{td}^{old} \frac{\partial{R}}{\partial{\Theta_{td}}}\right)_{+}.
\]
где $\left( x \right)_{+}$ обозначает операцию ``положительной обрезки'' $\max(x, 0)$. Величина $p_{tdw}$ обозначает вероятность того, что слово $w$ в документе $d$ порожденно скрытой темой $t$, и вычисляется следующим образом: $p_{tdw} = \frac{\Phi_{wt} \Theta_{td}}{\sum_s \Phi_{ws} \Theta_{sd}}$.

Из формул видно, что для того, чтобы добавить новый регуляризатор, нужно знать лишь $\frac{\partial{R}}{\partial{\Phi_{wt}}}, \frac{\partial{R}}{\partial{\Theta_{td}}}$, его частные производные по параметрам модели. За счёт вышесказанного можно утверждать, что ARTM~---~это не одна частная тематическая модель, а общий подход к их построению и комбинированию.

\subsection{Мультимодальное тематическое моделирование}

Аналогичным образом можно и организовать учёт дополнительной информации. 

В классическом тематическом моделировании считается, что документы содержат лишь слова $w \in W$, однако на практике о документе часто известна какая-то дополнительная информация, представимая в виде меток из некого конечного множества другой природы (примерами таких меток могут быть авторы документа, категории, дата и места публикации, реклама на странице). 

Будем считать, что эти метки имеют $m$ различных видов, называемыми  \textit{модальностями}, одной из которых является модальность слов (помимо которой можно рассмотреть, например, модальность авторов, модальность библиографии, модальность времени и прочее). Естественным образом обобщив вероятностную модель на этот случай, можно получить следующее выражения для общего правдополобия коллекции:

\[
L(\Phi^m, \Theta) = \sum_m \tau_m \sum_{d\in D} \sum_{w \in W^m} n_{dw} \ln p(w \mid d) \rightarrow \max
\]

Задачи такого вида можно единообразно решать при помощи модифицированного ЕМ-алгоритма.

\section{Приложения тематического моделирования}

% Early work on topic modeling conceptualized it as an intermediate stage of information retrieval pipeline. The possibility of meaningful interpretation was an afterthought. For measuring the quality of topics when evaluated against human judgments, several metrics were proposed. 


Тематические модели доказали свою актуальность в широком диапазоне контекстов\cite{fntir}. Их используют для информационного поиска \cite{yi2009, wang2011}, категоризации документов \cite{rubin2012}, анализа социальных сетей \cite{varshney2014, pinto2016}, рекоммендационных систем \cite{wang2011}, \cite{lee2015} и других задач.

Применения тематического моделирования можно разделить на две условных групы. Во-первых, выход тематической модели является способом векторного представления документа или слов. Такие представления можно использовать для прикладных задач классификации, ранжирования, поиска (непосредственно или как дополнительное признаковое описание для какого-либо алгоритма машинного обучения). Во-вторых, тематическая модель позволяет выявить структуру коллекции документов, обнаружить в данных закономерности, помочь эксперту охватить всю коллекцию целиком.

\subsection{Анализ программных продуктов}

\cite{mml0}

\subsection{Разведочный поиск}

The first problem requires the researcher to tune their models to perform well on labelled datasets, thus, reducing the task to classification problem using obtained from topic model document embeddings as feature vectors. This makes topic models compete with various machine learning and deep learning algorithms in the field of this classical task. 

\section{Интерпретация тематических моделей людьми}

Тематическое моделирование не обязательно показывает наилучшие результаты с точки зрения численных измерений качества для прикладных задач, но за ним явное преимущество, когда речь заходит о интерпретируемости. Каждую тему можно интерпретировать как вероятностное распределение над токенами в этой теме, и каждая координата в представлении документа несёт смысл (вероятность того, что в этом документе будет содержаться определенная тема). Это свойство тематических моделей делает их более подходящими для областей ИИ, где важна ясность прогнозирования или где может быть необходимо ручное исправление нежелательных искажений.

При этом вторая область применения ~---~ извлечение структуры коллекции ~---~ является уникальной для тематического моделирования. Зачастую исследователь ищет ответы на вопросы о структуре и природе коллекции, а не пытается оптимизировать заданную меру качества. Такой подход (который можно назвать ``дальним чтением'' \cite{milkova2019distant}), например, был принят в области биологии \cite{Liu2016,funnell2019integrated} и гуманитарных наук \cite{fntir,antons2019content}. Благодаря ему возможно получить важное представление о больших данных, которое в противном случае могло бы быть упущено исследователем.

Примером такого использования математического аппарата тематического моделирования служит проект  Mining the Dispatch \cite{dispatch}, посвящённый анализу более ста тысяч статей из газеты Richmond Daily Dispatch времен американской гражданской войны. Главная мотивация этого проекта состоит в том, что повседневная жизнь в Ричмонде, столице Конфедеративных Штатов Америки, до сих пор во многом остаётся загадкой.

В этой работе выявляются основные темы, проявляющиесся в газете. К таким темам относится, например, "обьявление о сбежавшем рабе" (в результате этого анализа был замечен резкий и необьяснимый всплеск числа таких обьявлений в начале 1862 года, что даёт основу для дальнейших исследований с использованием традиционных исторических методов), "антисеверная пропаганда" или "патриотизм и поэзия" (было замечено, что последние две темы очень часто появляются вместе). Автор отмечает, что подобные закономерности было бы практически невозможно обнаружить более традиционными методами.

Ещё одной интересной областью гуманитарного применения тематического моделирования является литературный анализ. В работе \cite{buurma2015fictionality} анализируются работы Энтони Троллопа, известного романиста Викторианской эпохи. Исследователь выявляет ряд тем, часто встречающихся в творчестве писателя и обосновывает точку зрения о том, что тематическая модель позволяет увидеть работы с непривычной точки зрения (например, наличие посвящённой письмам темы помогает обнаружить частые отсылки к эпистолярному жанру). Исследователь замечает, что полученные темы можно рассматривать как составные части книг, которые писатель мог бы гипотетически написать.

Также при помощи тематического моделирования удобно изучать историю развития темы, примером чего служит работа \cite{goldstone2012can}. Авторы показывают принципиальную возможность анализа статей в журнале Publications of the Modern Language Association of America при помощи латентного размещения Дирихле. Авторы строят две разные тематические модели, сравнивают их и отмечают, что сделанные на основе их выводы согласуются друг с другом. Затрагивается вопрос о том, может ли тематическая модель быть аргументом в пользу какой-то гипотезы.

Здесь про твиттер Трампа \cite{wang2016catching}

Про Стэк Оверфлоу \cite{TODO_SO}

Про этнопроект \cite{TODO}

\subsection{Проблемы тематического моделирования}
В связи с этими и многими другими приложениями тематического моделирования возникает ряд очень важных вопросов.

В работе \cite{dh_sea}
И нужно рассказать про важную статью из DH.

% Возможность обмануться

% Явные проблемы

% Устойчивость



С точки зрения тематического моделирования тема — это просто набор слов, которые часто встречаются вместе. При этом исследователи неявно предполагают, что этот набор слов является семантически связанным; что тема стабильна (тема имеет одинаковый смысл в разных документах); что темы достаточно различны; что темы достаточно однородны по своей структуре.

Не все построенные темы можно однозначно проинтерпретировать.

Это не всегда справедливо. Например, Мимно \cite{Mimno} идентифицировал следующие частые проблемы, связанные с тематическими моделями:
\begin{enumerate}
\item{Темы-химеры (chained topics). В качестве примера такой темы можно привести тему из слов "старение - человек - глобин". Пары "старение человек" и "глобин человек" имеют смысл, но пару "старение глобин" обьединяет лишь то, что оба слова часто встречаются рядом со словом "человек".}
\item{Темы с посторонними словами (intruded topics): в целом осмысленные темы, внутри которых есть слова, не имеющие ничего общего с остальными словами}
\item{Случайные темы: набор слов, не имеющих отношения друг к другу}
\item{Несбалансированные темы: содержат внутри себя как термины как широкой направленности, так и крайне специфические. Например, "сигнальная трансдукция" (общее название передачи сигнала в молекулярной биологии) и "сигнальный путь Notch")}
\end{enumerate}

Этот список проблем далее был уточнён и переработан в работе \cite{TODO} 

\todo{Также стоит отметить работы где темы оценивались по significance.}

\todo{Также стоит отметить работы про банк тем}


Исследователь, изучающий график "вероятность темы в момент времени", часто рассматривает одну тему отдельно от всех остальных. Взлёты и падения этой темы он интерпретирует в терминах изменения частоты понятия, характеризуемого её множеством из топ 10 слов.




Topic modelling is a method to extract hidden word probability distributions called topics from text corpora. 



Jordan Boyd-Graber, Yuening Hu and David Mimno in their monograph \cite{fntir} show, that designing new topic model is a difficult enterprise and identify accessibility as the most important unsolved problem in topic modelling: ``the primary research challenge of topic models is... to make them more accessible''.

The same year another publication \cite{lee2017human} argued, that existing topic models do not provide non-expert users with direct means to alter topic features perceived by these users to be bad. The study suggested several improvements for existing topic modelling user experience, concentrating on topic visualisation for the users.

The article from another group \cite{agrawal2018wrong} demonstrate that existing popular topic models are failing to provide good results when used ``off-the-shelf'' with their default parameters.
