\chapter{Критерии качества тематических моделей}


В предыдущей главе было описано, для каких целей исследователи применяют тематические модели. Математический аппарат тематического моделирования всё чаще используется для того, чтобы сделать какие-то выводы, рассмотрев темы сами по себе.

Строго говоря, это не является использованием тематического моделирования <<по прямому назначению>>, поскольку изначально тематическое моделирование было всего лишь промежуточным шагом для последующей машинной обработки. Главным приложением семантической индексации был информационный поиск: показ пользователю документа, наиболее релевантного его запросу либо поиск документов, похожих на данный. 
В ранних работах, посвящённых тематическому моделированию, вопрос содержательной интерпретации тем не ставился. Главными критериями качества были либо основанные на правдоподобии метрики (такие как перплексия на обучающей или проверочной выборке), либо метрики, основанные на внешней задаче (точность нахождения документа в информационном поиске; качество классификации документа по его темеатическому векторному представлению).

Как правило, исследователи изучали несколько наиболее частотных токенов из каждой темы (<<верхние 10 токенов>>, 10 top-tokens), но это было лишь способом убедиться в общей адекватности модели; никаких строго сформулированных требований к семантической состоятельности тем в этот процесс не закладывалось.

Первая работа, занявшаяся изучением внутренней структуры тематических моделей \cite{rtl} появилась лишь в 2009 году. В ней приглашённые эксперты различными способами оценивали 10 верхних слов каждой темы, а также соответствие документа и тем, которые приписывает ему модель. 

Одним из важных результатов этой работы оказалось то, что перплексия отрицательно коррелирует с оценками экспертов. В частности, обнаружилось что Correlated Topic Model, традиционно считавшаяся лучшей моделью с точки зрения перплексии, предсказаний и правдоподобия, показывает наихудшую интерпретируемость по сравнению с остальными моделями.

Это поставило перед сообществом новую задачу: найти критерии для автоматического оценивания интерпретируемости тем без участия экспертов. 

\section{Экспертная оценка тематических моделей}

В работе \cite{rtl}, впервые занявшейся вопросами интерпретации тем, были введены две следующие метрики качества:

\begin{enumerate}
\item{Словесная интрузия: эксперту показывается шесть слов, пять из которых относятся к определённой теме, а шестое к этой теме не относится. Требуется определить постороннее слово. Предполагается, что в хороших темах постороннее слово будет бросаться в глаза, а в плохих темах будет невозможно определить его лучше, чем выбирая слово наугад.}
\item{Тематическая интрузия: эксперту показывается фрагмент документа и топ-слова из четырёх тем. Три из этих тем содержатся в этом документе (у них высокая $p(t|d)$), а четвёртая нет (низкая $p(t|d)$). Требуется определить постороннюю тему.}
\end{enumerate}

В работе \cite{mimno}, развивающей это направление исследований, экспертная оценка тем строилась по-другому. Два эксперта напрямую размечали темы по шкале <<хорошая --- средняя --- плохая>>; если их метки не совпадали, то они приходили к консенсусу через обсуждение. Дополнительно эксперты классифицировали темы низкого качества, относя их к классам тем-химер, случайных тем, тем с посторонними словами и несбалансированных тем.

Темы демонстрировались экспертам комплексным образом. Описание одной темы состояло из списка 30 верхних слов (упорядоченных по убыванию их вероятности в теме); списка верхних документов; самых частых последовательностей из двух и более слов, отнесённых к этой теме; четырёх тем, часто сочетающихся с данной темой в документах; а также распространённые метаданные документов, отнесённых к этой теме (взвешенные по idf слова из заголовков, ключевые слова, названия учреждений и журналов; тематическая модель не использовала их как дополнительные признаки).

Для того, чтобы отнести тему к числу <<хороших>> требовалось, чтобы её можно было соотнести с какой-либо единой целостной концепцией. В ряде случаев также дополнительно требовалось, чтобы в текстах верхних документов темы содержались слова, явно связанные с этой концепцией.

Дополнительно было проведено сравнение этой техники визуализации с задачей словесной интрузии. Было показано, что интрузийный подход имеет меньшую точность (в частности, для эксперта не представляет труда определить посторонее слово в <<плохой>> теме, если эта тема является химерой). Большинство последующих работ использовали похожий процесс, в котором темы непосредственно оценивались экспертами по их качеству.

% \subsection{Гипотеза условной независимости}
% \todo{спросить у КВ, что сюда писать}
% Bayesian checking

\section{Устойчивость}
Ещё одним важным свойством <<хороших>> тематических моделей является устойчивость. Многие практические применения требуют, чтобы построенная тематическая модель была <<\textit{обоснованной}>>, то есть отражала реальную структуру коллекции, была объективной и воспроизводимой (этот вопрос затрагивается в \cite{dh_sea} и \cite{agrawal2018wrong}).

Многие подходы к оценке устойчивости тематических моделей были разработаны в контексте проблемы определения числа тем. Ранняя работа \cite{Brunet4164} подходит к этой проблеме с другой стороны и фокусируется на воспроизводимости меток кластеров.

Каждому документу $d$ сопоставляется его наиболее вероятная тема (такое $t_0$, что $\theta_{t_0 d} \geq \theta_{t' d} \forall t'$). Затем для каждой пары документов $(d_1, d_2)$ отмечается, лежат ли они в одном и том же кластере. Результатом этой процедуры является матрица связности размера $D \times D$. Эта матрица усредняется по нескольким построениям тематической модели. Предлагаемая в работе мера устойчивости определяется как коэффициент кофенетической корреляции (cophenetic correlation coefficient, используемый в биостатистике) полученной усреднённой матрицы.

В этой схеме не используется разбиение корпуса на случайные подвыборки и изменение порядка документов, о которой пойдёт речь ниже. Вся рандомизация происходит исключительно вследствие случайных перезапусков. Библиотека TOM, в которой реализован данный метод, по умолчанию производит 10 случайных перезапусков. Это затрудняет применение предлагаемой схемы на корпусах большого размера.

Работа \cite{greene14howmany} предлагает методику подбора гиперпараметров тематических моделей с позиций анализа устойчивости. Этот подход восходит к оценке качества моделей кластеризации, таких, как k-средних или неотрицательное матричное разложение. Эвристическое нестрогое обоснование такого подхода состоит в следующем: разделение данных на <<неправильное>> число классов неустойчиво, поскольку модель кластеризации вынуждена произвольным образом либо объединять кластеры, либо разбивать их.

Предлагаемая авторами процедура измерения устойчивости заключается в следующем. Строятся модели двух видов: референтная модель для всего корпуса и ряд моделей, построенных на случайных подмножествах документов корпуса (порядок документов в подвыборках также случайно перемешивается). Верхние токены моделей по подвыборкам сравниваются с верхними токенами референтной модели при помощи индекса Жаккара. Число тем у всех моделей одинаково; перед вычислением индекса Жаккара темы двух сравниваемых моделей сопоставляются венгерским алгоритмом.

Программная реализация этой методики доступна на GitHub\footnote{https://github.com/derekgreene/topic-stability}, также есть независимая реализация внутри библиотеки TOM\footnote{https://github.com/AdrienGuille/TOM}, отличающаяся рядом деталей. В ней не строится референтная модель, вместо этого строится ряд <<равноправных>> моделей по подвыборкам, которые затем попарно сравниваются аналогичным образом.

Работа \cite{belford2018stability} развивает этот подход дальше. Общая идея сравнения моделей, построенных на случайных подвыборках, остаётся неизменной, однако авторы отказываются от референтной модели. Вводится ещё два метода измерения устойчивости: симметричная разность множеств верхних слов (верхние токены различных тем при этом <<перемешиваются>>, т.е. между моделями сравниваются множества токенов, характерных хотя бы для какой-то темы) и устойчивость кластеров (таким же образом, как и в \cite{Brunet4164} вычисляются матрицы связности, но мера устойчивости определяется как взаимная информация между всеми парами матриц). Предлагается два метода улучшения устойчивости тематических моделей, основанных на ансамбле методов (ensemble learning) и показывается, что более сложный из них повышает качество итоговой модели (для оценки качества используется когерентность верхних слов и сравнение кластеров с известной <<ground truth>>).

В работе \cite{agrawal2018wrong} предложен механизм оптимизации гиперпараметров LDA методом дифференцтальной эволюци (Differential Evolution) и показано, что он увеличивает устойчивость. Определение устойчивости базируется на методике из работы \cite{greene14howmany}, но не использует референтную модель. В работе \cite{derbanosov} показано, что введение модальности меток классов (известных для всего корпуса, либо для какой-то его доли) повышает устойчивость тематических моделей. Устойчивость измеряется схожим образом, но вместо коэффициента подобия Жаккара используется небольшая его вариация. Работа \cite{mantyla2018measuring} также изучает устойчивость LDA, применяя хорошо подходящую для этой цели метрику rank-biased overlap (RBO), учитывающую взаимный порядок верхних токенов (в отличии от  коэффициента подобия Жаккара).

\section{Различность тем}

Различность тем --- желательное свойство тематической модели. Формализовать это свойство оказывается проще, чем интерпретируемость, поэтому в литературе уделяется много внимания этому вопросу.

Во работе \cite{cao2009density} было предложено использовать различность тем как критерий выбора числа тем. Различность определялась как среднее попарное косинусное расстояние между темами. Этот подход получил развитие в работе \cite{deveaud2014accurate}, где утверждалось, что дивергенция Йенсена-Шеннона лучше подходит для этой цели, чем косинусное расстояние.

Дивергенция Йенсена-Шеннона использовалась также в \cite{mimno}, где  предлагался альтернативный способ построения тематической модели. Одним из обоснований его преимуществ служило среднее попарное расстояние между темами (т.е. темы новой модели являются более уникальными).

Работа \cite{tang14look} развивает эти идеи и добавляет к числу рассматриваемых метрик среднее Евклидово расстояние. Другой подход можно найти в работе \cite{tan2010topic}, где вместо максимизации расстояний минимизируется корреляции между $\phi_{wt}$ для разных $t$. Другая интересная идея содержится в работе \cite{wang2011topic}, где утверждается что более важными для понимания корпуса являются более распространённые и/или типичные темы; тема $t$ оказывается более приоритетной для демонстрации пользователю, если среднее расстояние Йенсена-Шеннона между ней и остальными темами мало.

Отдельно стоит рассмотреть расстояние на основе нормализованной дивергенции Кульбака-Лейблера, введённое в \cite{koltcov2014latent}. Сначала происходит сокращение словаря до только тех слов $w$, для которых $\exists t \phi_{wt} > \frac{1}{|W|}$; это делает измерение более устойчивым, уменьшая зависимость от шумов в хвостах распределений. Вычисление дивергенции Кульбака-Лейблера производится именно по оставшимся словам $w$. Полученное значение нормализуется так, чтобы степень похожести каждой пары тем лежала в $[0, 1]$, что достигается делением на максимальное значение $KL(t, t')$. 

Упомянутые выше работы были посвящены подбору числа тем. Это достаточно естественно, поскольку различность тем связана с гранулярностью, которая, в свою очередь, связана с количеством тем. В самом деле: при малом $|T|$ переход от $|T|$ к $|T|+1$ будет порождать новую тему, плохо описываемую предыдущей моделью; но если $|T|$ заданно слишком большим, то модель будет вынуждена построить много тем-дубликатов или тем, похожих друг на друга.

В этой связи стоит упомянуть работы, применяющие для анализа тематически моделей Silhouette Coefficient и Calinski-Harabasz Index. Эти меры качества обычно связаны с анализом сетей или кластеризацией.

В работе \cite{panichella2013effectively} строились тематические модели для анализа программных артефактов (software artifacts). Исходные коды программ достаточно похожи на тексты естественного языка, чтобы их можно было успешно анализировать средствами тематического моделирования. В то же время, различия между этими применениями ведут к тому, что эти две области требуют существенно различной настройки гиперпараметров. В описанной работе рассматривались три прикладных задачи и пять различных датасетов (каждый из которых представляет из себя исходный код определённой программной системы и различные метаданные). Подбор гиперпараметров и отбор моделей производился при помощи генетических алгоритмов; критерием качества служил Silhouette Coefficient, учитывающий межкластерные и внутрикластерные расстояния.

К сожалению, авторы работы \cite{panichella2013effectively} опустили ряд подробностей в описании методологии. В частности, из текста статьи не совсем понятно, между какими объектами рассчитывается расстояние: между тематическими векторами $\theta_{\ast d}$, между счётчиками слов в документе $n_{\ast d}$, или между $n_{\ast d}$, взвешенными по схеме tf-idf.

В другой работе \cite{mehta_clustering_bank}, использующей Silhouette Coefficient, ставилась задача извлечения устойчивых (или <<сильных>>) тематик на основе множественных перезапусков LDA на одном и том же корпусе текстов естественного языка. Авторами $L$ раз строилась тематическая модель, состоящая из $K$ тем. В результате получается серия из $L \times K$ векторов $\phi_{w \ast}$. Эти векторы кластеризуются одним из трёх классических алгоритмов кластеризации (K-Means, Hierarchical, Affinity Propagation) и полученные кластеры оцениваются при помощи Silhouette Coefficient. Кластеры тематик с высоким Silhouette Coefficient считаются <<сильными>> или <<устойчивыми>>. В работе утверждается, что такие темы оказываются лучше интерпретируемыми, чем темы с низким Silhouette Coefficient.

В работе \cite{karami2018fuzzy} предлагается новая тематическая модель fuzzy latent semantic analysis (FLSA). Для того, чтобы сравнить её с LDA на корпусе неразмеченных медицинских записей (nursing notes), была предложена следующая процедура. Сначала строится тематическая модель, содержащая какое-то количество тем (50, 100, 150 и 200). Затем полученные темы кластеризуются на несколько кластеров (были рассмотрены вариации от 2 до 8 кластеров) при помощи алгоритма K-Means. Наконец, вычисляется Calinski–Harabasz (CH) index, который и служит оценкой качества. Предложенная в статье модель FLSA оказывается лучше LDA во всех рассмотренных сценариях.

Работа \cite{krasnov19clustering} развивает описанные выше идеи и изучает применимость SilhC и Calinski-Harabasz для подбора числа тем. В этой работе отказываются от промежуточного шага кластеризации полученных тем, и вычисление Silhouette и Calinski-Harabasz производится напрямую над найденными темами. Для каждой из $K$ тем извлекается множество из 10 её верхних слов, эти слова отображаются в векторы при помощи предобученных векторных представлений GloVe, качество полученных таким образом $K$ кластеров из 10 объектов оценивается при помощи Silhouette, Calinski-Harabasz и Davies Bouldin Index.

\subsection{Расстояние до известных распределений}

Этот класс мер качества объединён следующей общей идеей. Рассматривается какое-то известное вероятностное распределение над элементами $\Phi$, $\Theta$ либо над какими-то величинами, выводимыми из них. Это распределение считается заведомо <<плохим>> или неинформативным. Искомая мера качества определяется как расстояние между этим распределением и аналогичным распределением, выведенным из рассматрвиаемой тематической модели (модель считается тем лучше, чем больше это расстояние).

В работе \cite{alsumait2009topic} определяются три <<мусорных>> или <<незначимых>> (Junk/Insignificant) распределения:

\textbf{W-Uniform}: равномерное распределение над всеми словами, $p(w|\Omega^U) = \frac{1}{|W|} \forall w$. Это распределение сравнивается с распределением $\phi_{wt}$ для фиксирванного $t$.

\textbf{W-Vacuous}: эмпирическое распределение частот слов по всей коллекции $p(w|\Omega^V_E)$. Это распределение не несёт в себе никакой нетривиальной информации. В случае тематической модели, хорошо описывающей коллекцию, распределение эмпирических частот должно приближаться маргинальными распределениями слов: 
$$
    p(w|\Omega^V_E) \approx p(w|\Omega^V) = \sum_{t \in T} \phi_{wt}p(t), 
$$
где $p(t)$ можно вычислить как $\frac{1}{n}\sum_d \theta_{td} n_d$ или как $\frac{1}{n}\sum_d \theta_{td}$ (в рассматриваемой статье использовался второй, возможно менее математически корректный, вариант).

В работе \cite{boydcare} авторы обсуждают применение этого распределения на практике. Было замечено, что темы, близкие к эмпирическому распределению, не всегда являются плохими. Если корпус содержит очень много текстов, посвященных определённому вопросу, то близкие к эмпирическому распределению темы могут касаться этого вопроса и быть полезными интерпретируемыми темами. Напротив, расстояние до эмпирического распределения способно выявлять неинформативные темы в ситуации, когда документы содержат похожие устойчивые формулировки, шаблонные фразы или канцеляризмы.

\textbf{D-Background}: равномерное $p(d|\Omega^B) = \frac{1}{|D|} \forall d$. <<Незначимость>> этого распределения вызвана тем, что каждая тема должна покрывать какое-то сравнительно небольшое количество документов; размазанные по многим документам темы не являются информативными. 

Заметим, что напрямую сравнивать $\theta_{k d}, k \in T$ с $\frac{1}{|D|}$ некорректно, поскольку вектор $\theta_{k \ast}$ не является распределением вероятностей. Для того, чтобы сравнение было корректным, необходимо провести перенормировку; к сожалению, описание этой перенормировки, приведённое в статье, несколько расплывчато и допускает две неэквивалентных трактовки.

Первая трактовка основана на переходе от $\theta_{td} = p(t|d)$ к распределению $p(d|t)$, которое вычисляется как 
$$
    p(d|t) = \theta_{td} \frac{n_d}{n} \frac{1}{p(t)} = \theta_{td} \frac{n_d}{\sum_d \theta_{td} n_d}.
$$
Это распределение можно проинтерпретировать как вероятность того, что если выбрать случайное слово из темы $t$, то оно окажется внутри документа $d$.

Вторая трактовка подразумевает перенормировку равномерного распределения $p(d|t)$ и сравнение этого распределения с модельным $\theta_{td}$:
$$
    p(t|d, \Omega^B) = \frac{n}{|D| n_d}. 
$$

Стоит отметить, что реализованный в библиотеке BigARTM регуляризатор разреживания $\Theta$ связан с третьей трактовкой фонового распределения по темам: $p(t|d, \Omega^B) = \frac{1}{|D|} $

Далее в рассматриваемой работе предлагаются три способа измерения расстояния между модельными темами и мусорными: дивергенция Кульбака-Лейблера, косинусная близость и коэффициент корреляции Пирсона. Подсчитанные таким образом расстояния нормируются и усредняются с эмпирически подобранными весами; полученная таким образом величина и является оценкой <<значимости>> темы.

Ещё одно <<плохое>> распределение неявно вводится в работе \cite{plavin}. Здесь предлагается способ подбора числа тем, основанный на регуляризации: тематическая модель инициализируется заведомо избыточным числом тем, затем большая часть этих тем обнуляется посредством специального разреживающего регуляризатора. Функция, оптимизируемая этим регуляризатором имеет следующий вид:
\[
    \textup{KL}\infdiv*{u(t)}{p(t)} = \textup{KL}\infdiv*{\frac{1}{T}}{\sum_d \theta_{td} \frac{n_d}{n}} \rightarrow \max,
\]
и представляет собой дивергенцию Кульбака-Лейблера между равномерным распределением $u(t) = \frac{1}{T}$ и распределением $p(t)$, вычисленным по модельной матрице $\Theta$. Заметим, что каждое слагаемое в дивергенции можно рассматривать по отдельности, что даёт оценку качества для каждой темы. После раскрытия обозначений и отбрасывания константных множителей и слагаемых, получаем следующую функцию от $t$: 
$f(t) = - \log \sum_d \theta_td \frac{n_d}{n} = -\log p(t)$.

\section{Разреженность}

Разреженность --- свойство тематической модели, полезное на практике и соответствующее интуитивному представлению о том, что каждый документ $d$ и каждый термин $w$ связан с небольшим числом тем $t$. В качестве метрики разреженности используется просто процент нулевых элементов в распределениях $\phi_{wt}$ и $\theta_{td}$. На практике  действительно достигается разреженность, большая $80\%$ \cite{potapenko2013robust}.

У разреженных моделей есть множество преимуществ. Их можно более компактно хранить в памяти; они менее подверженны переобучению; просмотр верхних слов даёт более полную картину темы.

Разреженность также можно связать с расстоянием до известного распределения. Простейший способ контролировать разреженность внутри EM-алгоритма  --- прибавлять или вычитать из счётчиков $n_{wt},~n_{td}$ какие-то постоянные значения перед нормировкой. Известно, что это соответствует регуляризатору, минимизирующему либо максимизирующему KL-дивергенцию до равномерного распределения.

\section{Значимость}

A three-phase approach to document clustering based on topic significance degree: из модели на N тем отбираются M лучших тем, потом (как я понял), выкидывают из Теты все темы кроме лучших M (с перенормировкой), получают M-мерное пространство, в котором каждому из D документов соответствует одна точка. Через k-means++ находят хорошие документы для начального приближения кластеризации, потом через k-means кластеризуют документы в этом пространстве.


Критерий качества: topic significance degree. Умножаем все p(t|d) в Тете на -log p(d|t). Суммируем эту величину по всем докумнтам, получаем меру качества отдельной темы (чем больше, тем лучше). Далее M лучших тем отсекаются по относительному порогу.

(причём
$p(d|t)  = \theta_{td} / [\sum_{d' \in D} theta_{td'}]$, то есть без учёта длин документов
)


\section{Теоретико-информационные критерии}

Следующее семейство критериев учитывает точность описания моделью данных, штрафуя при этом за число параметров модели. Эти критерии основаны на разных теоретических допущениях, но дают формулы, очень схожие друг с другом (приведены в таблице ниже).

Another method is the usage of  Bayesian Information Criterion (BIC) which balances the goodness of fit and model complexity. 

Байесовский информационный критерий (BIC) подробно рассмотрен в работе \cite{soleimani14parsimonious}. Предложена новая тематическая модель, явно оптимизирующая BIC по своим дополнительным параметрам.

Примером использования информационного критерия Акаике (AIC) и байесовского информационного критерия (BIC) служит работа \cite{than2012fully}, в которой предложенная авторами разреженная тематическая модель сравнивается с классическими аналогами.

В нескольких работ был использовал принцип минимальной длины описания (minimum description length, MDL) \cite{image_segm}\cite{gerlach2018network}. В работе \cite{mml0}, изучавшей подбор числа тем, сравнивались AIC, BIC, MDL и подход, основанный на матрице информации Фишера.

Близкий к MDL принцип сообщения минимальной длины (minimum message length, MML) также применялся к тематическому моделированию. Nizar Bouguila --- исследователь, чья область интересов посвящена этой теме \cite{mml1}\cite{mml2}\cite{mml3}\cite{mml4}\cite{mml5}. 

Применение MML требует вывода функционала качества для конкретной тематической модели, использующего её априорные распределения; не в полной степени ясно, можно ли адаптировать MML к полувероятностному подходу АРТМ. В формулах измерения AIC, BIC и MDL участвуют величины $N_p$ (число свободных параметров модели) и $\mathfrak{L}(\Phi,\Theta)$ (правдоподобие модели). 

Правдоподобие модели $\mathfrak{L}(\Phi,\Theta)$ допустимо вычислять не только на проверочной выборке, но и на обучающей. Библиотека BigARTM вычисляет правдоподобие в ходе ищмерения перплексии; поле \texttt{raw} у скора перплексии позволяет получить искомую величину. Стоит заметить, что в работе \cite{gerlach2018network} утверждается, что вместо правдоподобия следует использовать маргинальное правдоподобие. Его нахождение требует интегрирования над априорными распределениями параметров модели и ведёт к сложной формуле, специфичной для рассматриваемой модели.

Число свободных параметров модели $N_p$ можно определить двумя неэквивалентными способами: либо как размер матрицы $\Phi$ (то есть $(|W|-1) \times |T|$, где учитывается ограничение на нормированность столбцов), либо как количество ненулевых элементов в ней ($\#\Phi$). Это соответствует рекомендации из работы \cite{than2012fully}, в которой утверждается, что число свободных параметров для LDA и разреженных моделей должно рассчитываться по-разному. В таблице формул приведены обе вариации, как обычные и sparse-разновидности критериев.

\begin{table}[h]
    \centering
    \begin{tabular}{lll}
    \toprule
               & $N_p$       & Formula                                             \\
    \midrule
    \textbf{sparse AIC} & $\#\Phi$       & $2 N_p - 2 \mathfrak{L}$                \\
    \textbf{AIC}        & $(W - 1) * T$  & $2 N_p - 2 \mathfrak{L}$                \\
    \textbf{sparse BIC} & $\#\Phi$       & $N_p \log(D) - 2 \mathfrak{L}$          \\
    \textbf{BIC}        & $(W - 1) * T$  & $N_p \log(D) - 2 \mathfrak{L}$          \\
    \textbf{sparse MDL} & $\#\Phi$       & $N_p \log(TD) - 2 \mathfrak{L}$         \\
    \textbf{MDL}        & $(W - 1) * T$  & $N_p \log(TD) - 2 \mathfrak{L}$\\
    \bottomrule
    \end{tabular}
\end{table}

\section{Энтропия} 

В работе \cite{koltcov2018application} проводится аналогия между тематическими моделями и термодинамическими системами. Каждая частица может принимать какой-либо из $T$ возможных состояний (каждое слово может быть порождено одной из $T$ тем). Это позволяет перенести принцип минимизации свободной энергии на область тематического моделирования.

Предполагается, что система может находиться в равновесном состоянии, при этом температура системы будет соответствовать оптимальному числу тем в термодинамическом смысле. Свободная энергия системы измеряется при помощи множества элементов матрицы $\Phi$, имеющих ``достаточно высокую'' вероятность: $S \hm= \{(w, t) \hm\mid \phi_{wt} \hm> (W)^{-1} \} \hm\subset \Phi$. Далее вычисляется энергия $E \hm= -\log \sum_{(w,t) \in S} \phi_{wt}$, свободная энергия  $E_f \hm= E - T \log\bigl(|S| / (W * T)\bigr)$ и энтропия Реньи $S^R = -E_f / (T - 1)$.

\section{Величины, полезные для диагностики}

Работа \cite{boydcare} описывает ряд мер качества, полезных для обнаружения аномальных тем. Каждая из этих величин в отдельности предназначена не для оценки качества модели в целом, а для выявления потенциально проблематичных тем. Взятые в совокупности, эти меры качества могут служить средством автоматической оценки качества тем. 

Также их можно применять для полуавтоматического сравнения нескольких тематических моделей: эксперт изучает тематики с необычными значениями этих показателей и вручную указывает, являются ли эти тематики проблематичными. Затем модели сравниваются по количеству (или доле) подтверждённо проблематичых тем.

\begin{itemize}
    \item \textbf{Мощность темы} (topic size): определяется как $\sum_w n_{wt}$. Описывающие очень малое количество токенов темы часто являются плохими (также это упоминается в работе \cite{mimno}).
    \item \textbf{Длина верхних слов}: средняя длина верхних слов (в символах). Темы, слова которых аномально коротки, часто содержат в себе стоп-слова (возможно, стоп-слова языка, не основного в корпусе), короткие аббревиатуры, артефакты неправильной лемматизации и/или токенизации.
    \item{\textbf{Расстояние до эмпирического распределения}: было описано выше.}
    \item{\textbf{Документная частота токенов} (Difference between token and document frequencies): для каждого токена темы подсчитывается количество документов, его содержащих и отнесённых к этой теме (по всей видимости, имеется в виду количество таких документов $d$, что $n_wd p_{tdw} > 1$).
    Искомой мерой качества становится разница между документной частотой токенов и вероятностью токенов в теме. Существенная разница между этими распределениями может объясняться редкими токенами, часто встречающимисся в одном длинном документе. Это связано с лингвистическом феноменом burstiness.}
    \item{\textbf{Значимость темы в документах} (Prominence within documents): количество документов, в которых данная тема является существенной. Предлагается два способа измерения этой характеристики: число документов $d$, таких что $\theta_{td} > \epsilon$ или число документов $d$, таких что $\theta_{td} = \max_{k \in T} \theta_{kd}$.}
\end{itemize}

\section{Анализ верхних токенов}

Анализ верхник токенов темы остаётся ``золотым стандартом'' оценки тем на интерпретируемости. Многие рассмотренные в этой главе меры качества так или иначе связаны с анализом верхних токенов тем.

В качестве примера можно привести меру NKLS, которая была напрямую ``заземленна'' в анализ верхних токенов. Экспериментально было проверено, что значения NKLS, превосходящие 0.9, соответствуют парам тем, в которых практически совпадают списки 30-50 верхних слов, отсортированные по вероятностям.

Приведённые ниже меры качества оперируют со списком верхних токенов более непосредственно. Их достоинством является очевидная связь с процессом изучения тем вручную, однако недостатком является низкая чувствительность к изменениям в тематической модели. Так, например, многие меры когерентности игнорируют информацию об относительном порядке токенов в теме; ни одна из них не способна ``заметить'' изменения $\phi_{wt}$, не влияющие на относительный порядок токенов. 

Некоторым исключением являются lift и blei-score, в которых по множеству верхних токенов темы вычисляется некоторая величина, учитывающая их вероятности внутри модели.

В следующей главе будет подробнее рассказано об общих недостатках этих мер качества.

\subsection{Похожесть списков верхних токенов}
Сравнение тем по спискам их верхних слов тесно связано с коэффициентом сходства Жаккара, также используемом в машинном обучении. Как правило, коэффициент Жаккара встречается в контексте сравнения построенной тематической модели с эталонной \cite{greene14howmany} или для сравнения моделей друг с другом \cite{mantyla2018measuring}, однако есть ряд работ, где его используют для измерения похожести тем между собой \cite{bulatov}\cite{yanginferring}.

Стоит отметить, что в работе \cite{mantyla2018measuring} использована мера rank-biased overlap (RBO), учитывающая относительный порядок верхних токенов.

\subsection{Когерентность}
На основе работы \cite{rtl} в работе \cite{newman2010automatic} была предложена метрика когерентности, которая основывалась на PMI. Тема называется когерентной, если её топ-слова встречаются вместе чаще, чем можно было бы ожидать от случайного распределения. Было обнаружено, что эта метрика коррелирует с экспертными оценками.

Далее в статье \cite{mimno2011} была предложена метрика "размер темы" (ожидаемое число слов темы в коллекции), а также альтернативная формула для когерентности, основанная не на совстречаемости терминов, а на условной вероятности (вероятность менее частого термина с учётом более частого). Приглашённых экспертов просили классифицировать темы (показанные им в виде топ-слов) как "плохие", "промежуточные" или "хорошие". Также был вопроизведён эксперимент со словесной интрузией. 

Было показано, что новая метрика когерентности коррелирует с экспертными оценками лучше, чем размер темы и лучше, чем изначальный вариант когерентности. Также было отмечено, что оценки по методу интрузии являются худшим показателем, чем просто экспертные оценки, поскольку найти постороннее слово в теме-химере несложно.

В ряде дальнейших работ были изучены небольшие нюансы имеющихся формул: по какому корпусу вычислять статистику совстречаемости слов; на каком расстоянии друг от друга должны находиться слова $w$ и $w'$ для того, чтобы можно было считать их встречающимисся вместе (в частности, иногда используется ``окно величиной с документ'', т.е. $w$ и $w'$ считаются встречающимисся вместе, если они вместе содержатся в одном и том же документе); как лучше всего оценивать вероятности пар, не встречающихся вместе, чтобы избежать деления на ноль; стоит ли нормировать полученную величину; вычислять ли совстречаемости у пар слов или у множеств слов.

В работе \cite{boydcare} рассматриваются три вариации подсчёта когерентности: TC-PMI (описанный ранее в \cite{newman2010automatic}), TC-LCP описанный ранее в \cite{mimno} и TC-NZ (введена авторами; определяется как количество пар верхних токенов, которые совсем не встречаются вместе). Было продемонстрированно, что эти три показателя позволяют идентифицировать разные виды проблематичных тем. TC-PMI выявляет темы, содержащие неспецифичные частые термины. TC-LCP показывает интерпретируемые темы, загрязнённые посторонними словами. TC-NZ помогает обнаружить темы, состоящие из имён.

Следующая новая идея была предложена в работе \cite{aletras2013evaluating}. Она заключается в построении вектора контекстов (число совстречаемости данного слова с каждым из остальных) для каждого слова и последующем сравнении этих векторов между собой. В качестве мотивирующего примера можно привести две марки автомобилей: эти слова будут редко встречаться рядом, но они будут похожим образом сочетаться со словами "дорога", "скорость", "бензин". Проведённое в другой работе \cite{mrtl} сравнение подтвердило, что этот подход показывает хорошую корреляцию с экспертными оценками (другим хорошим кандидатом была метрика, основанная на NPMI).

В статье \cite{roder2015exploring} авторы рассмотрели все предложенные к тому моменту метрики когерентности, описали общую схему, в которую они укладываются и единообразно сравнили их все. Кроме того, авторы нашли в этой схеме несколько новых вариаций, которые показывают лучшее качество. 

Также стоит отметить существование другого рода модификаций этого метода. Десять слов, демонстрируемых эксперту либо участвующих в оценке когерентности, можно отбирать не по критерию максимума $p(w|t)$, а по некоторой более сложной формуле \cite{blei2009topic} \cite{ldavis2014}

% Тем не менее, все эти методы основаны на анализе ограниченного числа слов, что составляет маленькую долю от всей коллекции документов и вряд ли даёт представление о все тематической модели целиком.

% Пока что неясно, можно ли считать этот вопрос решённым.

% However, we believe that this approach suffers from several fundamental limitations. We argue that these limitations bring into question the common practice of treating coherence and interpretability as equivalent.

% Although not reflected in scientific literature, another reasonable approach is to build many models with different $T$ and pick the one that gives the highest coherence value % \cite{mlplus}\cite{sathi2016quality}\cite{del2020emerging}. 

\subsection{Качество кластеризации}

В работе \cite{krasnov19clustering} предлагается метод, похожий на подсчёт когерентности на основе дистрибутивной семантики. Сначала 10 верхним словам каждой темы сопоставляются их векторные представления (в работе использовались вектора из GloVe). Затем полученные таким образом $|T|$ кластеров из 10 объектов оцениваются с помощью различных метрик кластеризации. Утверждется, что переход к векторным представлениям необходим, поскольку понятие ``расстояния'' не совсем применимо к вероятностным распределениям, составляющим темы.

% я бы лучше lexical kernel брал...

\subsection{Lift}
Работа \cite{taddy2012estimation} была посвящена альтернативной параметризации тематических моделей. Для того, чтобы показать интерпретируемость полученной модели, автор демонстрирует темы в виде списка из 5 характерных токенов. Для отбора этих токенов используется не максимум вероятности, как было принято до этого момента, а новая величина $\textup{Lift}(w, t)$, определяемая как отношение $\phi_{wt}$ к средней частоте слова $w$ по коллекции.

Эта идея получила развитие в работе \cite{ldavis2014}, посвящённой визуализации тематических моделей для пользователей. В системе LDAVis, описываемой в этой статье, рекомендуется комбинированный подход к выбору токенов для показа пользователю: 

$$r(w, t)_{\lambda} = \lambda \log(\phi_{wt}) + (1 - \lambda) \log \textup{Lift}(w, t),$$

где значение $\lambda = 0.6$ было подобрано в рамках эмпирического исследования.

В работе \cite{fan2019assessing} было предложено использовать $\log \textup{Lift}$ как метрику качества тематической модели, а не просто метод визаулизации тем (точка зрения, популяризованная предыдущими работами). Предлагаемый авторами мера качества $\textup{LogLift}(t)$ рассчитывается через усреднение $\log \textup{Lift}(w_i, t)$ для 30 наиболее вероятных токенов темы $t$ (то есть 30 верхних токенов). Величина $\textup{LogLift}(t)$ оказывается связанной с частотой неинформативных слов в темах и лучше коррелирует с экспертными оценками качества тем в ситуациях, когда в верхних токенах темы присуствуют неинформативные слова.

Усреднение $\textup{LogLift}(t)$ по всем темам даёт меру качества модели в целом, также коррелирующий с объективными измерениями качества.

\subsection{Blei-score}

В обучающем материале \cite{Blei_lafferty} предлагается интересный способ визуализации темы, основанный не на максимуме вероятности, а на величине

$$\text{term-score}_{t,w} = \phi_{wt} \log \bigg( \frac{\phi_{wt}}{\sqrt[T]{\prod_k \phi_{wk}}}\bigg)$$,

введение которой объясняется следующим образом: <<
Эта формула вдохновлена величиной TF-IDF, используемой для взвешивания слов в информационном поиске. Первый
множитель сродни частоте термина; второй множитель похож
на документную частоту и уменьшает значимость терминов, имеющих высокую вероятность во всех темах. Другие методы определения разницы между
темой и остальными темами можно найти в (Tang and MacLennan, 2005)>>. Последняя ссылка приведена авторами, вероятно, ошибочно (не имеет отношения к тематическому моделированию и анализу текстов).

Можно заметить, что это выражение имеет такую же форму, как и одно слагаемое в расчёте KL-дивергенции между $\phi_{wt}$ и неким распределением $Q$, которое является просто средним геометрическим по всем $\phi_{k}$. Таким образом, $\sum_{w} \text{term-score}_{t,w}$ можно рассмотреть как ещё одну меру качества, отражающую различность тем.

По аналогии с Lift (где метод сортировки токенов темы для отображения превратился в меру качества) можно просуммировать term-score по верхним 30 словам темы. Получим величину, которая гладко возрастает при обучении, отражает "чистоту" верхних токенов модели и имеет понятную интерпретацию как расстояние от известного неинформативного распределения.

