\chapter{Критерии качества тематических моделей}


В секции \todo{про DH} было описано как исследователи смотрят на тематические модели. Тут возникает ряд вопросов. ПОдчернкём, что это не использование ТМ по назначению. 

Более того, изначально тематическое моделирование было всего лишь промежуточным шагом для последующей машинной обработки. Главным приложением семантической индексации был информационный поиск: показ пользователю документа, наиболее релевантного его запросу либо поиск документов, похожих на данный. 

В ранних работах, посвящённых тематическому моделированию, вопрос содержательной интерпретации не ставился. Главными критериями качества были либо основанные на правдоподобии метрики (такие как перплексия), либо метрики, основанные на внешней задаче (точность нахождения документа в информационном поиске; качество классификации документа при помощи пропорций тем в документе).

Как правило, исследователи изучали несколько наиболее вероятных слов из каждой темы ("топ 10 слов"), но это было лишь способом убедиться в общей адекватности модели; никаких строго сформулированных требований к семантической состоятельности тем в этот процесс не закладывалось.

Первая работа, занявшаясся изучением внутренней структуры тематических моделей \cite{rtl} появилась лишь в 2009 году. В ней приглашённые люди-эксперты различными способами оценивали топ 10 слов каждой темы, а также соответствие документа и тем, которые приписывает ему модель. 

Одним из важных результатов этой работы оказалось то, что перплексия отрицательно коррелирует с оценками экспертов. В частности, обнаружилось что Correlated Topic Model, традиционно считавшаясся лучшей моделью с точки зрения перплексии, предсказаний и правдоподобия, показывает наихудшую интерпретируемость по сравнению с остальными моделями.

Это поставило перед сообществом новую задачу: найти критерии оценки интерпретируемости тем. 

\subsection{Экспертная оценка тематических моделей}

В работе \cite{TODO} впервые занявшейся вопросами интерпретации тем были введены две следующие метрики качества:

\begin{enumerate}
\item{Словесная интрузия: человеку показывается шесть слов, пять из которых относятся к определённой теме, а шестое к этой теме не относится. Требуется определить постороннее слово. Предполагается, что в хороших темах постороннее слово будет бросаться в глаза, а в плохих темах будет невозможно определить его лучше, чем выбирая слово наугад.}
\item{Тематическая интрузия: челвоеку показывается фрагмент документа и топ-слова из четырёх тем. Три из этих тем содержатся в этом документе (у них высокая $p(t|d)$), а четвёртая нет (низкая $p(t|d)$). Требуется определить постороннюю тему.}
\end{enumerate}

\todo{Мимно потом ещё пишет что лучше просто пусть баллы ставят.}

\subsection{Perplexity}
The classic intrinsic approach is based on hold-out perplexity \cite{griffiths2004finding} (\textbf{holdPerp}). The work of \cite{zhao15heuristic} enhances this method by considering rates of perplexity change (\textbf{RPC}) instead of raw perplexities (essentially, they take into account the slope of perplexity curve instead of absolute values).

\subsection{Гипотеза условной независимости}
\todo{спросить у КВ, что сюда писать}
Bayesian checking


This paper mainly focuses on intrinsic metrics, which will be discussed in this section. The section is organized into several broad categories, each category containing a number of somewhat related ideas found in the literature. 


\subsection{Устойчивость}
Ещё одним важным свойством ``хороших'' тематических моделей является устойчивость. Многие практические применения требуют того, чтобы построенная тематическая модель была \textit{``обоснованной''}, то есть отражала реальную структуру коллекции, была объективной и воспроизводимой (этот вопрос затрагивается в \cite{dh_sea} и \cite{agrawal2018wrong}).

Многие подходы к оценке устойчивости тематических моделей были разработаны в контексте проблемы определения числа тем. Раняя работа \cite{Brunet4164} подходит к этой проблеме с другой стороны и фокусируется на воспроизводимости меток кластеров.

Каждому документу $d$ сопоставляется его наиболее вероятная тема (такое $t_0$, что $\theta_{t_0 d} \geq \theta_{t' d} \forall t'$). Затем для каждой пары документов $(d_1, d_2)$ отмечается, лежат ли они в одном и том же кластере. Результатом этой процедуры является матрица связности размера $D \times D$. Эта матрица усредняется по нескольким построениям тематической модели. Предлагаемая в работе мера устойчивости определяется как коэффициент кофенетической корреляции (cophenetic correlation coefficient, используемый в биостатистике) полученной усреднённой матрицы.

В этой схеме не используется разбиение корпуса на случайные подвыборки и изменение порядка документов, о которой пойдёт речь ниже. Вся рандомизация происходит исключительно благодаря случайным перезапускам. Библиотека TOM, в которой реализован данный метод, по умолчанию производит 10 случайных перезапусков. Это затрудняет применение предлагаемой схемы на корпусах большого размера.

Работа \cite{greene14howmany} предлагает методику подбора гиперпараметров тематических моделей с позиций анализа устойчивости. Этот подход восходит к оценке качества моделей кластеризации (таких как k-средних или неотрицательное матричное разложение). Эвристическое нестрогое обоснование такого подхода состоит в следующем: разделение данных на ``неправильное'' число классов неустойчиво, поскольку модель кластеризации вынуждена произвольным образом либо объединять кластера, либо разбивать их.

Предлагаемая авторами процедура измерения устойчивости заключается в следующем. Строятся модели двух видов: референтная модель для всего корпуса и ряд моделей, построенных на случайных подмножествах документов корпуса (порядок документов в подвыборках также случайно перемешивается). Верхние токены моделей по подвыборкам сравниваются с верхними токенами референтной модели при помощи индекса Жакара. Число тем у всех моделей одинаково; перед вычислением индекса Жакара темы двух сравниваемых моделей сопоставляются венгерским алгоритмом.

Программная реализация этой методики доступна на GitHub\footnote{https://github.com/derekgreene/topic-stability}, также есть независимая реализация внутри библиотеки TOM\footnote{https://github.com/AdrienGuille/TOM}, отличающаясся рядом деталей. В ней не строится референтная модель, вместо этого строится ряд ``равноправных'' моделей по подвыборкам, которые затем попарно сравниваются аналогичным образом.

Работа \cite{belford2018stability} развивает этот подход дальше. Общая идея сравнения моделей, построенных на случайных подвыборках, остаётся неизменной, однако авторы отказываются от референтной модели. Вводится ещё два метода измерения устойчивости: симметричная разность множеств верхних слов (верхние токены различных тем при этом ``перемешиваются'', т.е. между моделями сравниваются множества токенов, характерных хотя бы для какой-то темы) и устойчивость кластеров (таким же образом, как и в \cite{Brunet4164} вычисляются матрицы связности, но мера устойчивости определяется как взаимная информация между всеми парами матриц). Предлагается два метода улучшения устойчивости тематических моделей, основанных на ансамбле методов (ensemble learning) и показывается, что более сложный из них повышает качество итоговой модели (для оценки качества используется когерентность верхних слов и сравнение кластеров с известной ground truth).

В работе \cite{agrawal2018wrong} предложен механизм прямой оптимизации устойчивости при помощи Differential Evolution. Определение устойчивости базируется на методике из работы \cite{greene14howmany}, но не использует референтной модели. В работе \cite{derbanosov} показано, что введение модальности меток классов (известных для всего корпуса либо для какой-то его доли) повышает устойчивость тематических моделей. Устойчивость измеряется схожим образом, но вместо коэффициента подобия Жакара используется небольшая его вариация.

% On the other hand, the authors in \cite{greene14howmany} do not guarantee that the reference point model is good, which we believe is important.

\section{Различность тем}

Различность тем --- желательное свойство тематической модели. Формализовать это свойство оказывается проще, чем интерпретируемость, поэтому в литературе уделяется много внимания этому вопросу.

Во влиятельной работе \cite{cao2009density} было предложено использовать различность тем, как критерий выбора числа тем. Различность определялась как среднее попарное косинусное расстояние между темами. Этот подход получил развитие в работе \cite{deveaud2014accurate}, где утверждалось, что дивергенция Дженсена-Шэннона превосходит косинусное расстояние.

Дивергенция Дженсена-Шэннона использовалась и в другой важной работе \cite{Mimno}. В работе  предлагался альтернативный способ построения тематической модели; одним из обоснований его преимуществ служило среднее попарное расстояние между темами (т.е. темы новой модели являются более уникальными).

Работа \cite{tang14look} развивает эти идеи и добавляет к числу рассматриваемых метрик среднее Евклидово расстояние. 

Другой подход можно найти в работе \cite{tan2010topic}, где минимизируется не какое-то расстояние, а корреляции между $\phi_{wt}$ для разных $t$.

Отдельно стоит рассмотреть расстояние на основе нормализованной дивергенции Кульбака-Лейблера, введённое в \cite{koltcov2014latent}. Сначала происходит сокращение словаря до только тех слов $w$, для которых $\exists t \phi_{wt} > \frac{1}{|W|}$; это делает измерение более устойчивым, уменьшая зависимость от шумов в хвостах распределений. Вычисление дивергенции Кульбака-Лейблера производится именно по оставшимся $w$. Полученное значение нормализуется так, чтобы степень похожести каждой пары тем лежала в $[0, 1]$ (это достигается делением на максимальное значение $KL(t, t')$). 

Описанные выше работы \cite{} \cite{} \cite{} были посвящены подбору числа тем (многие из них являются очень влиятельными в этой области). Это достаточно естественно, поскольку различность тем связана с гранулярностью, которая, в свою очередь, связана с количеством тем. В самом деле: при малом $|T|$ переход от $|T|$ к $|T|+1$ будет порождать новую тему, плохо описываемую предыдущей моделью; но если $|T|$ заданно слишком большим, то модель будет вынуждена построить много тем-дубликатов или тем, похожих друг на друга.

В этой связи стоит упомянуть работы, применяющие для анализа тематически моделей Silhouette Coefficient и Calinski-Harabasz Index. Эти меры качества обычно связаны с анализом сетей или кластеризации.

В работе \cite{panichella2013effectively} строились тематические модели для анализа программных артефактов (software artifacts). Исходные коды програм достаточно похожи на тексты на естественном языке, чтобы их можно было успешно анализировать средствами тематического моделирования. В то же время, различия между этими применениями ведут к тому, что эти две области требуют существенно различной настройки гиперпараметров. В описанной работе рассматривались три прикладных задачи и пять различных датасетов (каждый из которых представляет из себя исходный код определённой программной системы и различные метаданные). Подбор гиперпараметров и отбор моделей производился при помощи генетических алгоритмов; критерием качества служил Silhouette Coefficient, учитывающий межкластерные и внутрикластерные расстояния.

К сожалению, авторы работы \cite{panichella2013effectively} опустили ряд подробностей в своей методологии. В частности, из текста статьи не совсем понятно, между какими объектами рассчитывается расстояние: между тематическими векторами $\theta_{\ast d}$, между счётчиками слов в документе $n_{\ast d}$, или между $n_{\ast d}$, взвешенными по схеме tf-idf.

Дргая работа \cite{mehta_clustering_bank}, использующая Silhouette Coefficient, занималась извлечением устойчивых (или ``сильных'') тематик на основе множественных перезапусков LDA на одном и том же корпусе текстов естественного языка. Авторами $L$ раз строилась тематическая модель, состоящая из $K$ тем. В результате получается серия из $L \times K$ векторов $\phi_{w \ast}$. Эти вектора кластеризуются одним из трёх классических алгоритмов кластеризации (K-Means, Hierarchical, Affinity Propagation) и полученные кластера оцениваются при помощи Silhouette Coefficient. Кластера тематик с высоким Silhouette Coefficient считаются ``сильными'' или ``устойчивыми''; в работе утверждается, что такие темы оказываются лучше интерпретируемыми, чем темы с низким Silhouette Coefficient.

В работе \cite{karami2018fuzzy} предлагается новая тематическая модель fuzzy latent semantic analysis (FLSA). Для того, чтобы сравнить её с LDA на корпусе неразмеченных медицинских записей (nursing notes), была предложена следующая процедура. Сначала строится тематическая модель, содержащая какое-то количество тем (50, 100, 150 и 200). Затем полученные тематики кластеризуются на несколько кластеров (были рассмотрены вариации от 2 до 8 кластеров) при помощи алгоритма K-Means. Наконец, вычисляется Calinski–Harabasz (CH) index, который и служит оценкой качества. Предложенная в статье модель FLSA оказывается лучше LDA во всех рассмотренных сценариях.

Работа \cite{krasnov19clustering} развивает описанные выше идеи и изучает применимость SilhC и Calinski-Harabasz для подбора числа тем. В этой работе отказываются от промежуточного шага кластеризации полученных тематик, вычисление Silhouette и Calinski-Harabasz производится напрямую над найденными темами. Для каждой из $K$ тем извлекается множество из 10 её верхних слов, эти слова отображаются в вектора при помощи предобученных векторных представлений GloVe, качество полученных таким образом $K$ кластеров из 10 объектов оценивается при помощи Silhouette, Calinski-Harabasz и Davies Bouldin Index.

\subsection{Расстояние до известных распределений}

Этот класс мер качества объединён следующей общей идеей. Расматривается какое-то известное вероятностное распределение над элементами $\Phi$, $\Theta$ либо над какими-то величинами, выводимыми из них. Это распределение считается заведомо ``плохим'' или неинформативным. Искомая мера качества определяется как расстояние между этим распределением и аналогичным распределением, выведенным из рассматрвиаемой тематической модели (модель считается тем лучше, чем больше это расстояние).

В работе \cite{alsumait2009topic} определяются три ``мусорных'' или ``незначимых'' (Junk/Insignificant) распределения:

\textbf{W-Uniform}: равномерное распределение над всеми словами, $p(w|\Omega^U) = \frac{1}{|W|} \forall w$. Это распределение сравнивается с распределением $\phi_{wt}$ для фиксирванного $t$.

\textbf{W-Vacuous}: эмпирическое распределение частот слов по всей коллекции $p(w|\Omega^V_E)$. Это распределение не несёт в себе никакой нетривиальной информации. В случае тематической модели, хорошо описывающей коллекцию, распределение эмпирических частот должно приближаться маргинальными распределениями слов: 

$$p(w|\Omega^V_E) \approx p(w|\Omega^V) = \sum_{t \in T} \phi_{wt}p(t), $$

где $p(t)$ можно вычислить как $\frac{1}{n}\sum_d \theta_{td} n_d$ или как $\frac{1}{n}\sum_d \theta_{td}$ (в рассматриваемой статье использовался второй, возможно менее математически корректный, вариант).

\textbf{D-Background}: равномерное $p(d|\Omega^B) = \frac{1}{|D|} \forall d$. ``Незначимость'' этого распределения вызвана тем, что каждая тема должна покрывать какое-то сравнительно небольшое количество документов; размазанные по многим документам темы не являются информативными. 

Заметим, что напрямую сравнивать $\theta_{k d}, k \in T$ с $\frac{1}{|D|}$ некорректно, поскольку вектор $\theta_{k \ast}$ не является распределением вероятностей. Для того, чтобы сравнение было корректным, необходимо провести перенормировку; к сожалению, описание этой перенормировки, приведённое в статье, несколько расплывчато и допускает две неэквивалентных трактовки.

Первая трактовка основана на переходе от $\theta_{td} = p(t|d)$ к распределению $p(d|t)$, которое вычисляется как 

$$p(d|t) = \theta_{td} \frac{n_d}{n} \frac{1}{p(t)} = \theta_{td} \frac{n_d}{\sum_d \theta_{td} n_d}$$.

Это распределение можно проинтерпретировать как вероятность того, что если выбрать случайное слово из темы $t$, то оно окажется внутри документа $d$.

Вторая трактовка подразумевает перенормировку равномерного распределения $p(d|t)$ и сравнение этого распределения с модельным $\theta_{td}$:

$$p(t|d, \Omega^B) = \frac{n}{|D| n_d} $$

Стоит отметить, что реализованный в библиотеке BigARTM регуляризатор разреживания $\Theta$ связан с третьей трактовкой фонового распределения по темам: $p(t|d, \Omega^B) = \frac{1}{|D|} $

Далее в рассматриваемой работе предлагаются три способа измерения расстояния между модельными темами и мусорными: дивергенция Кульбака-Лейблера, косинусная близость и коэффициент корреляции Пирсона. Подсчитанные таким образом расстояния нормируются и усредняются с эмпирически подобранными весами; полученная таким образом величина и является оценкой ``значимости'' темы.

Ещё одно ``плохое'' распределение неявно вводится в работе \cite{plavin}. Здесь предлагается способ подбора числа тем, основанный на регуляризации: тематическая модель инициализируется заведомо избыточным числом тем, затем большая часть этих тем обнуляется посредством специального разреживающего регуляризатора. Функция, оптимизируемая этим регуляризатором имеет следующий вид:

\[
\textup{KL}\infdiv*{u(t)}{p(t)}=\textup{KL}\infdiv*{\frac{1}{T}}{\sum_d \theta_{td} \frac{n_d}{n}} \rightarrow \max,
\]

и представляет собой дивергенцию Кульбака-Лейблера между равномерным распределением $u(t) = \frac{1}{T}$ и распределением $p(t)$, вычисленным по модельной матрице $\Theta$. Заметим, что каждое слагаемое в дивергенции можно рассматривать по отдельности, что даёт оценку качества для каждой темы. После раскрытия обозначений и отбрасывания константных множителей и слагаемых, получаем следующую функцию от $t$: $f(t) = - \log \sum_d \theta_td \frac{n_d}{n} = -\log p(t)$.

\section{Разреженность}

Разреженность --- свойство тематической модели, полезное на практике и соответствующее интуитивному представлению о том, что 


Разреженность также можно связать с расстоянием до известного распределения. Простейший способ контролировать разреженность внутри EM-алгоритма  --- прибавлять или вычитать из счётчиков $n_{wt},~n_{td}$ какие-то постоянные значения перед нормировкой. Известно, что это соответствует регуляризатору, минимизирующему либо максимизирующему KL-дивергенцию до равномерного распределения.

В работе \cite{Mimno} упоминается разреженность с другой стороны: 


% Another important development (\textbf{D-Spectral}) was proposed in \cite{arun2010finding} which integrates information in $\Phi$ and $\Theta$ matrices by considering spectral values of $\Phi$ and rows of unnormalized $\Theta$. The proposed \textit{Spectral Divergence Measure} reflects the degree of orthogonality between topic vectors.

\subsection{Теоретико-информационные критерии}

Следующее семейство критериев учитывает точность описания моделью данных, штрафуя при этом за число параметров модели. Эти критерии основаны на разных теоретических допущениях, но дают формулы, очень схожие друг с другом (приведены в таблице ниже).

Another method is the usage of  Bayesian Information Criterion (BIC) which balances the goodness of fit and model complexity. 

Байесовский информационный критерий (BIC) подробно рассмотрен в работе \cite{soleimani14parsimonious}. Предложена новая тематическая модель, явно оптимизирующая BIC по своим дополнительным параметрам.

Примером использования информационного критерия Акаике (AIC) и байесовского информационного критерия (BIC) служит работа \cite{than2012fully}, в которой предложенная авторами разреженная тематическая модель сравнивается с классическими аналогами.

В нескольких работ был использовал принцип минимальной длины описания (minimum description length, MDL) \cite{image_segm}\cite{gerlach2018network}. В работе \cite{mml0}, изучавшей подбор числа тем, сравнивались AIC, BIC, MDL и подход, основанный на матрице информации Фишера.

Близкий к MDL принцип сообщения минимальной длины (minimum message length, MML) также применялся к тематическому моделированию. Nizar Bouguila --- исследователь, чья область интересов посвящена этой теме \cite{mml1}\cite{mml2}\cite{mml3}\cite{mml4}\cite{mml5}. 

Применение MML требует вывода функционала качества для конкретной тематической модели, использующего её априорные распределения; не в полной степени ясно, можно ли адаптировать MML к полувероятностному подходу АРТМ. В формулах измерения AIC, BIC и MDL участвуют величины $N_p$ (число свободных параметров модели) и $\mathfrak{L}(\Phi,\Theta)$ (правдоподобие модели). 

Правдоподобие модели $\mathfrak{L}(\Phi,\Theta)$ допустимо вычислять не только на проверочной выборке, но и на обучающей. Библиотека BigARTM вычисляет правдоподобие в ходе ищмерения перплексии; поле \texttt{raw} у скора перплексии позволяет получить искомую величину. Стоит заметить, что в работе \cite{gerlach2018network} утверждается, что вместо правдоподобия следует использовать маргинальное правдоподобие. Его нахождение требует интегрирования над априорными распределениями параметров модели и ведёт к сложной формуле, специфичной для рассматриваемой модели.

Число свободных параметров модели $N_p$ можно определить двумя неэквивалентными способами: либо как размер матрицы $\Phi$ (то есть $(|W|-1) \times |T|$, где учитывается ограничение на нормированность столбцов), либо как количество ненулевых элементов в ней ($\#\Phi$). Это соответствует рекомендации из работы \cite{than2012fully}, в которой утверждается, что число свободных параметров для LDA и разреженных моделей должно рассчитываться по-разному. В таблице формул приведены обе вариации, как обычные и sparse-разновидности критериев.

\begin{table}[h]
    \centering
    \begin{tabular}{lll}
    \toprule
               & $N_p$       & Formula                                             \\
    \midrule
    \textbf{sparse AIC} & $\#\Phi$       & $2 N_p - 2 \mathfrak{L}$                \\
    \textbf{AIC}        & $(W - 1) * T$  & $2 N_p - 2 \mathfrak{L}$                \\
    \textbf{sparse BIC} & $\#\Phi$       & $N_p \log(D) - 2 \mathfrak{L}$          \\
    \textbf{BIC}        & $(W - 1) * T$  & $N_p \log(D) - 2 \mathfrak{L}$          \\
    \textbf{sparse MDL} & $\#\Phi$       & $N_p \log(TD) - 2 \mathfrak{L}$         \\
    \textbf{MDL}        & $(W - 1) * T$  & $N_p \log(TD) - 2 \mathfrak{L}$\\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{Entropy} 

В работе \cite{koltcov2018application} проводится аналогия между тематическими моделями и термодинамическими системами. Каждая частица может принимать какой-либо из $T$ возможных состояний (каждое слово может быть порождено одной из $T$ тем). Это позволяет перенести принцип минимизации свободной энергии на область тематического моделирования.

Предполагается, что система может находиться в равновесном состоянии, при этом температура системы будет соответствовать оптимальному числу тем в термодинамическом смысле. Свободная энергия системы измеряется при помощи множества элементов матрицы $\Phi$, имеющих ``достаточно высокую'' вероятность: $S \hm= \{(w, t) \hm\mid \phi_{wt} \hm> (W)^{-1} \} \hm\subset \Phi$. Далее вычисляется энергия $E \hm= -\log \sum_{(w,t) \in S} \phi_{wt}$, свободная энергия  $E_f \hm= E - T \log\bigl(|S| / (W * T)\bigr)$ и энтропия Реньи $S^R = -E_f / (T - 1)$.

\section{Top-Tokens Analysis}

\todo{Анализ верхник токенов темы остаётся важным методом. }

Многие рассмотренные в этой главе меры качества так или иначе связаны с анализом верхних токенов тем.

В качестве примера можно привести NKLS отличает то, что она была ``заземленна''. Экспериментально было проверено, что значения NKLS, превосходящие 0.9, соответствуют парам тем, в которых практически совпадают списки 30-50 верхних слов, отсортированные по вероятностям.

Сравнение тем по спискам их верхних слов тесно связано с коэффициентом сходства Жаккара, также используемом \todo{в машинном обучении? в информационном поиске?} Как правило, коэффициент Жаккара встречается в контексте сравнения построенной тематической модели с эталонной \cite{} \cite{} \cite{}, однако есть ряд работ, где его используют для измерения похожести тем между собой \cite{bulatov}\cite{yanginferring}. 

\todo{тут что-то ещё было}

Достоинством Жаккара является его связь с изучением списков слов вручную, однако недостатком является низкая чувствительность к изменениям вероятностей.

Жаккар не учитывает вероятности
Мимно учитывает порядок

Нужно заметить, что лифт учитывает модель и способен уловить маленькие изменения

В главе <> будет подробнее рассказано об общих недостатках этих мер качества.



\subsection{Когерентность}
На основе работы \cite{rtl} в работе \cite{mrtl} была предложена метрика когерентности, которая основывалась на понятиях из теории информации. Тема называется когерентной, если её топ-слова встречаются вместе чаще, чем можно было бы ожидать от случайного распределения. Было обнаружено, что эта метрика коррелирует с экспертными оценками.

Далее в статье \cite{mimno2011} была предложена метрика "размер темы" (ожидаемое число слов темы в коллекции), а также альтернативная формула для когерентности, основанная не на совстречаемости терминов, а на условной вероятности (вероятность менее частого термина с учётом более частого). Приглашённых экспертов просили классифицировать темы (показанные им в виде топ-слов) как "плохие", "промежуточные" или "хорошие". Также был вопроизведён эксперимент со словесной интрузией. 

Было показано, что новая метрика когерентности коррелирует с экспертными оценками лучше, чем размер темы и лучше, чем изначальный вариант когерентности. Также было отмечено, что оценки по методу интрузии являются худшим показателем, чем просто экспертные оценки, поскольку найти постороннее слово в теме-химере несложно.

В ряде дальнейших работ были изучены небольшие нюансы имеющихся формул: по какому корпусу вычислять статистику совстречаемости слов; на каком расстоянии друг от друга должны находиться слова $w$ и $w'$ для того, чтобы можно было считать их встречающимисся вместе; как лучше всего оценивать вероятности пар, не встречающихся вместе, чтобы избежать деления на ноль; стоит ли нормировать полученную величину; вычислять ли совстречаемости у пар слов или у множеств слов.

Следующая новая идея была предложена в работе \cite{aletras2013evaluating}. Она заключается в построении вектора контекстов (число совстречаемости данного слова с каждым из остальных) для каждого слова и последующем сравнении этих векторов между собой. В качестве мотивирующего примера можно привести две марки автомобилей: эти слова будут редко встречаться рядом, но они будут похожим образом сочетаться со словами "дорога", "скорость", "бензин".

В статье \cite{roder2015exploring} авторы рассмотрели все предложенные к тому моменту метрики когерентности, описали общую схему, в которую они укладываются и единообразно сравнили их все. Кроме того, авторы нашли в этой схеме несколько новых вариаций, которые показывают лучшее качество. 

Также стоит отметить существование другого рода модификаций этого метода. Десять слов, демонстрируемых эксперту либо участвующих в оценке когерентности, можно отбирать не по критерию максимума $p(w|t)$, а по некоторой более сложной формуле \cite{blei2009topic} \cite{ldavis2014}

% Тем не менее, все эти методы основаны на анализе ограниченного числа слов, что составляет маленькую долю от всей коллекции документов и вряд ли даёт представление о все тематической модели целиком.

% Пока что неясно, можно ли считать этот вопрос решённым.

% However, we believe that this approach suffers from several fundamental limitations. We argue that these limitations bring into question the common practice of treating coherence and interpretability as equivalent.

Although not reflected in scientific literature, another reasonable approach is to build many models with different $T$ and pick the one that gives the highest coherence value \cite{mlplus}\cite{sathi2016quality}\cite{del2020emerging}. \textit{Coherence} is widely used quality metric for topic models, which is computed by using co-occurrence counts of top $10$ most probable words of each topic (top-tokens).


\subsection{Качество кластеризации}

Krasnov et al.\cite{krasnov19clustering} offers an interesting variation: top $10$ words were replaced with their dense embeddings (GloVe representations were used) and the number of topics was successfully chosen in accordance with Davies–Bouldin index, an indicator of clustering quality.


% кажется, что он взял топ10 слов каждой темы и сделал им ворд2век. Дальше он взял метки классов из ТМ и расстояния из эмбеддингов и посчитал на этом DBI
% я бы лучше lexical kernel брал...
% и почему он не считал в этом пространстве силуэт?

% Notably, the model investigated by Krasnov is sparse. The work about sparse TM also considers sparse models and you can see that AIC and BIC are useless for T determination.


\subsection{Lift}
Величина Lift была введена в работе \cite{taddy2012estimation}, где рекомендовалось использовать её при показе темы пользователю (а именно, сортировать слова темы по Lift, а не по их вероятностям). $\textup{Lift}(w, t)$ определяется как отношение $\phi_{wt}$ к средней частоте слова $w$ по коллекции. В недавней работе \cite{fan2019assessing} была предложена метрика качества Log-Lift, основанная на этой величине: для 30 слов $w_i$, наиболее вероятных в теме $t$, вычисляется $\frac{1}{30}\sum_{i=1}^{30}\log \textup{Lift}(w_i, t)$, в дальнейшем усредняемая по всем темам. Было показано, что рассчитанная таким образом величина связана с долей неинформативных слов в темах, а также имеет существенную корреляцию с экспертными оценками качества тем.



\subsection{Blei-score}

В обучающем материале \cite{Blei_lafferty} предлагается интересный способ визуализации темы:

Visualizing a topic. Exploring a corpus through a topic model typically begins with visualizing the posterior topics through their per-topic
term probabilities $\phi_{t}$. The simplest way to visualize a topic is to order the
terms by their probability. However, we prefer the following score,

$$\text{term-score}_{t,w} = \phi_{wt} \log \bigg( \frac{\phi_{wt}}{\sqrt[T]{\prod_k \phi_{wk}}}\bigg)$$

This is inspired by the popular TFIDF term score of vocabulary terms used
in information retrieval Baeza-Yates and Ribeiro-Neto (1999). The first
expression is akin to the term frequency; the second expression is akin to
the document frequency, down-weighting terms that have high probability
under all the topics. Other methods of determining the difference between
a topic and others can be found in (Tang and MacLennan, 2005).

Последняя ссылка приведена авторами, вероятно, ошибочно (не имеет отношения к тематическому моделированию и анализу текстов).

Дальше я стал применять этот критерий на практике для того, чтобы показывать топ-токены. Как правило (на нормально декоррелированных моделях) у меня получалось, что максимум вероятности и blei-score выдают одни и те же токены в том же порядке, но с другими числами. blei-score при этом лучше отражает, насколько данный токен характерен для этой темы.

Можно заметить, что это выражение имеет такую же форму, как и одно слагаемое в расчёте KL-дивергенции между $\phi_{wt}$ и неким распределением $Q$, которое является просто средним геометрическим по всем $\phi_{k}$. Таким образом, $\sum_{w} \text{term-score}_{t,w}$ можно рассмотреть как ещё одну меру качества, отражающую различность тем.

По аналогии с Lift (где метод сортировки токенов темы для отображения превратился в меру качества) можно просуммировать term-score по верхним 30 словам темы. Получим величину, которая гладко возрастает при обучении, отражает "чистоту" верхних токенов модели и имеет понятную интерпретацию как расстояние от известного неинформативного распределения.

