\chapter{Повышение интерпретируемости тематических моделей при помощи регуляризации}


\section{Относительные коэффициенты регуляризации}

% Задача этого текста --- обьяснить теоретическую подоплёку относительных коэффициентов регуляризации и дать ряд практических советов по их применению.

% В данный момент открытая библиотека bigARTM умеет работать с относительными коэффициентами для $\phi$-регуляризаторов. Таким же образом можно подбирать и коэффициенты для $\theta$-регуляризаторов (возможность, которая сейчас в bigARTM отсутствует). Более того, таким же образом можно и подбирать веса дополнительных модальностей (возможность, которая и не снилась нашим мудрецам).

Относительные коэффициенты регуляризации были впервые введены в работе \cite{doykov} в общем виде.

\dscl{Общая формула взята из ВКР Никиты Дойкова. Важные частные случаи --- выведены мной, успешно проверены на практике}
В исследовании \cite{doykov} приведена формула для общего случая $k$ произвольных гладких регуляризаторов $R_i(\Phi, \Theta)$. Здесь и далее: $\tau_i$ (тау) обозначает абсолютный коэффициент регуляризации, а $\lambda_i$ - относительный коэффициент регуляризации

Общая формула для регуляризаторов $\Phi$:
\[
\phi_{wt} = \norm_{w \in W}\Bigg( 
    n_{wt} + \sum_{i=1}^k \lambda_i \Big[
        \gamma_i \frac{n_t}{r_{it}} + (1-\gamma_i)\frac{n}{r_i}
        \Big] 
    \phi_{wt} \frac{\partial R_i}{\partial \phi_{wt}}
\Bigg), \label{rel_phi_general}
\]

где: 
\begin{itemize}
    \item{$r_{it} = \sum_{w\in W} \Big| \phi_{wt} \frac{\partial R_i}{\partial \phi_{wt}} \Big| $ --- воздействие регуляризатора на тему}
    \item { $r_{i} = \sum_{t\in T} r_{it}$ --- суммарное воздействие регуляризатора на коллекцию.}
    \item { $\lambda_i$ - относительный коэффициент регуляризации, показывающий, \emph{во сколько раз} соответствующий регуляризатор влияет на оценку $\phi_{wt}$ больше, чем коллекция}
    \item {Выражение в квадратных скобках - \textit{фактор балансировки}, гарантирующий ''равновесие'' между $n_{wt}$ и регуляризационной добавкой}
\end{itemize}

Введённый в этой формуле коэффициент $\gamma_i$ отвечает за степень индивидуализации. Он позволяет плавно переходить от равномерной регуляризации по всем темам ($\gamma = 0$) к индивидуальному подходу к каждой теме ($\gamma = 1$).

Важно заметить следующие особенности: 
\begin{enumerate}
    \item {В формуле участвуют $n_t, r_{it}, r_{i}$:  величины, которые могут изменяться в ходе итераций EM-алгоритма. Следовательно, в общем случае
    абсолютные коэффициенты и относительные  коэффициенты задают различные множества возможных траекторий регуляризации.}
    \item  {В формуле участвуют $n_t, r_{it}, r_{i}$: переменные, значения которых определяются в ходе ЕМ-алгоритма.}
\end{enumerate}

Общая формула для регуляризаторов $\Theta$:

\[
\theta_{td} = \norm_{t \in T} \Bigg( 
    n_{td} + \sum_{i=1}^k \lambda_i \Big[
        \gamma_i \frac{n_d}{r_{id}} + (1-\gamma_i)\frac{n}{r_i}
        \Big] 
    \theta_{td} \frac{\partial R_i}{\partial \theta_{td}}
\Bigg), \label{rel_theta_general}
\]

где: 
\begin{itemize}
    \item { $r_{id} = \sum_{t\in T} \Big | \theta_{td} \frac{\partial R_i}{\partial \theta_{td}} \Big | $ --- воздействие регуляризатора на документ}
    \item { $r_{i} = \sum_{d\in D} r_{id}$ --- суммарное воздействие регуляризатора на коллекцию.}
    \item { $\lambda_i$ - относительный коэффициент регуляризации, показывающий, \emph{во сколько раз} соответствующий регуляризатор влияет на оценку $\theta_{td}$ больше, чем коллекция}
    \item {Выражение в квадратных скобках - \textit{фактор балансировки}, гарантирующий ''равновесие'' между $n_{td}$ и регуляризационной добавкой}
\end{itemize}

Вычисления по формуле \ref{rel_theta_general} являются проблематичными для онлайнового и пакетных ЕМ-алгоритмов. Это связано с тем, что алгоритм разбивает коллекцию документов на пакеты, обрабатываемые параллельно и связанные только посредством матрицы $\Phi$. Поскольку в общем случае вычисление $r_i$ требует информацию о всех документах сразу, то формула \ref{rel_theta_general} несовместима с архитектурой эффективного ЕМ-алгоритма.

Тем не менее, для ряда важных частных случаев подобрать относительные коэффициенты регуляризации всё-таки возможно.

\subsection{Важные частные случаи}
\dscl{Здесь есть неотвеченный вопрос: как в БигАРТМ работает относительная декорреляция. Фактор балансировки там скачет от итерации к итерации, мне кажется что этот вопрос стоит доисследовать}

\textbf{Утверждение}. Для регуляризаторов сглаживания и разреживания, каждый абсолютный коэффициент регуляризации математически равносилен некоторому относительному коэффиценту и наоборот.

\textbf{Сглаживание и разреживание Фи}. Формула M-шага, сглаживающего ($\tau > 0$) или разреживающего ($\tau < 0$) распределение $\phi_{wt}$:

\[
\phi_{wt} = \norm_{w \in W} \big( n_{wt} + \tau \big)
\]

Интуитивный смысл этого преобразования прост: мы либо ''притягиваем'' Фи к равномерному распределению $\beta = \frac{1}{|W|}$, либо ''отталкиваем'' её от него же (возможно, даже зануляя при этом какие-то компоненты).

Оказывается, можно провести репараметризацию, которая строго это продемонстрирует. 

Пусть $\beta = \frac{1}{|W|}$ --- равномерное распределение, а текущие значения $n_{wt}$ и $\tau \in \mathbb{R}$ таковы, что на этой итерации M-шага зануления компонент не происходит (то есть либо $\tau > 0$, либо $\tau < 0$, но $n_{wt} + \tau > 0$).

Тогда операцию положительной обрезки можно проигнорировать:

\[
\phi_{wt} = \norm_{w \in W} \big( n_{wt} + \tau \big) = \frac{n_{wt} + \tau }{\sum_{w \in W} n_{wt} + \tau } = \frac{n_{wt} + \tau }{n_{t} + \tau |W|}  
\]

Попробуем представить это выражение, как выпуклую комбинацию распределений  $\frac{n_{wt}}{n_t}$ (оценки максимума правдоподобия) и $\frac{1}{|W|}$ (равномерного распределения). 


\[
\frac{n_{wt} + \tau}{n_{t} + \tau |W|} = (1-\lambda) \frac{n_{wt}}{n_t} + \lambda \frac{1}{|W|} \iff \tau  = \frac{n_t \lambda}{(1-\lambda) |W|} \label{sp_phi_rel2abs} 
\]

Значит, сглаживание Фи можно трактовать, как нахождение компромисса между $\phi_{wt} = \frac{n_{wt}}{n_t}$ и $\phi_{wt} = \frac{1}{|W|}$. 

Что это означает на практике? Допустим, мы хотим провести регуляризацию так, чтобы $\phi_{wt}$ на 50\% состояла из оценки максимума правдоподобия, и на 50\% из априорного распределения $\frac{1}{|W|}$. Для этого достаточно вычислить $\tau$ по формуле и подставить в модель.

Важно заметить следующие особенности: 

\begin{enumerate}
    \item  {Полученный коэффициент регуляризации зависит от $n_t$, то есть разные темы будут сглаживаться/разреживаться с разной силой}
    \item  {Поскольку значение $n_t$ может изменяться в ходе итераций EM-алгоритма, то абсолютная величина коэффициента тоже может меняется со временем}
    \item  {Зависит от $|W|$. Это значит, что один и тот же абсолютный коэффициент сглаживания/разреживания может по-разному влиять на коллекцию в зависимости от размера словаря.  Формула \ref{sp_phi_rel2abs} делает эту зависимость более наглядной. Теперь можно эвристически прикинуть, как нужно пересчитать $\tau$ при изменении словаря (например, после выкидывания слишком редких или частых слов).}
\end{enumerate}

Особенность (1) добавляет гибкости, но может быть проблематична на практике (слишком много дополнительных степеней свободы, возможен оверфиттинг).

В работе \cite{doykov} было предложено усреднить по всем темам:

\[
\tau = \frac{1}{|T|} \sum_t \tau_t = \frac{n}{|T|\cdot|W|} \frac{\lambda}{(1-\lambda)}
\]

\textbf{Сглаживание и разреживание Теты}. Вспомним, что M-шаг для Теты выглядит таким образом:

\[
\theta_{td} = \norm_{t \in T} \big( n_{td} + \tau \big)
\]

Аналогично выведем формулу:

\[
\frac{n_{td} + \tau}{n_d + \tau |T|} = (1-\lambda) \frac{n_{td}}{n_d} + \lambda \frac{1}{|T|} \iff \tau = \frac{n_d \lambda}{(1-\lambda) |T|} \label{sp_theta_rel2abs} 
\]

Полученный коэффициент регуляризации зависит от $n_d$, то есть разные документы будут сглаживаться/разреживаться с разной силой. Можно также усреднить все $\tau$ по документам:

\[
\tau = \frac{\lambda \sum_d n_d }{(1-\lambda) |D| \cdot |T|} 
\]

Заметим, что обе формулы для $\tau$ \textit{стабильны}. В отличии от $n_t$, значение $n_d$ в ходе обучения \emph{не меняется}. Все использованные здесь величины заранее известны и постоянны.

Следовательно, если рассматривать лишь усреднённые коэффиценты, то каждый относительный коэффициент регуляризации математически равносилен некоторому абсолютному коэффиценту и наоборот.

\textbf{Многомодальное тематическое моделирование}


Вспомним, что вероятность появления термина $w$ из $k$-й модальности в документе $d$ задаётся следующей формулой:
$p(w^k \cond d) = \sum_t \phi_{wt}^k \theta_{td}$, а общее правдоподобие представляется так:

\[
L(\Phi^m, \Theta) = \sum_m \tau_m \sum_{d\in D} \sum_{w \in W^m} n_{dw} \ln p(w \cond d) \rightarrow \max, \label{modal_likelihood}
\]
где коэффиценты $\tau_m$ показывают \textit{вес} модальности $m$.

Известно, что $EM$-алгоритм для мультимодальной тематической модели структурно похож на классический $EM$-алгоритм \cite{yanina}\cite{vorontsov2015non}\cite{bulatov}. 

Формулу \ref{modal_likelihood} можно проинтерпретировать, как введение $M-1$ дополнительного регуляризатора с коэффициентами $\tau_m$ \cite{yanina}:

\[
\tau_m R(\Phi, \Theta)_m = \tau_m L^{(m)}(\Phi, \Theta) = \tau_m  \sum_d \sum_w n_{dw}^{(m)} \ln \sum_t \phi_{wt}^{(m)}\theta_{td} = 
\sum_d \sum_w \check{n}_{dw}^{(m)} \ln \sum_t \phi_{wt}^{(m)}\theta_{td},
\]

где для удобства было введено ''обобщённое число слов'' $\check{n}_{dw}^{(m)} = \tau_m n_{dw}$ и аналогичные величины $\check{n}_{dt}^{(m)} = \sum_w \check{n}_{dt}^{(m)} p_{tdw}, \check{n}_t = \sum_d \check{n}_{dt}$.

Несложно показать, что выражение $r_{md}$ для этого случая выглядит следующим образом:

\[
r_{md} = \sum_t |\theta_{td} \frac{\partial R_m}{\partial \theta_{td}}| = \sum_t \sum_w \check{n}_{dw}^{(m)} p_{tdw} = \sum_t  \check{n}_{dt}^{(m)} = \check{n}_{d}^{(m)}
\]

Иными словами, $r_{md}$ равняется просто числу токенов модальности $m$ в документе $d$ (с учётом коэффициента $\tau_m$), а $r_m$ равняется просто суммарному количеству токенов этой модальности во всей коллекции (также с учётом коэффициента $\tau_m$).

Без ограничения общности примем, что в модели нет других регуляризаторов и задана лишь одна дополнительная модальность. Подставим найденные значения в формулу \ref{rel_theta_general}:

\[
\theta_{td} = \norm_{t \in T} \Bigg( 
    n_{td} + \lambda_m \Big[
        \gamma_m \frac{n_d}{r_{md}} + (1-\gamma_m)\frac{n}{r_m}
        \Big] 
    \theta_{td} \frac{\partial R_m}{\partial \theta_{td}}
\Bigg) = 
\]
\[
\norm_{t \in T} \Bigg( 
    n_{td} + \lambda_m \Big[
        \gamma_m \frac{n_d}{\check{n}_d^{(m)}} + (1-\gamma_m)\frac{n}{\sum_d \check{n}_d^{(m)}}
        \Big] 
    \theta_{td} \frac{\partial R_m}{\partial \theta_{td}}
\Bigg)
\]

Иными словами, фактор балансировки, ''уравновешивающий'' влияние слов и дополнительной модальности $m$, задаётся через отношение числа токенов модальности $m$ к числу слов в документе (либо во всей коллекции).

Заметим, что фактор балансировки выражается через константы, известные ещё на этапе построения коллекции.

Более того, если рассматривать лишь усреднённые коэффиценты, то каждый относительный вес модальности математически равносилен некоторому абсолютному весу и наоборот.

\[
\tau_m = \lambda_m \frac{n}{\sum_d \check{n}_d^{(m)}} \iff 
\lambda_m = \tau_m \frac{\sum_d \check{n}_d^{(m)}}{n}
\]


\subsection{Использование на практике}

Как я уже показал выше, можно адаптивно сглаживать/разреживать тету. Ещё один метод, полезный на практике, связан с весами модальносстей.

Формулы, задающие перевод из одного в другое, не требуют сложных выкладок. Их можно вычислить не только в ядре bigARTM, но даже внутри внешнего python-интерфейса.

В отличии от абсолютных коэффицентов/весов, 
относительные коэффициенты/веса можно интерпретировать, что упрощает процесс настройки тематической модели.

Можно уверенно утверждать, что относительные коэффициенты/веса являются мощным инструментом для тематического моделирования в рамках bigARTM и заслуживают повсеместного использования. 

Абсолютные и относительные коэффициенты сглаживания/разреживания для $\Theta$ эквивалентны. То же самое можно утверждать про абсолютные и относительные веса модальностей. 


\subsection{Библиотека BigARTM}


\section{Мотивация}

\subsection{Важность матрицы тем в документах}
С точки зрения вероятностного вывода матрица слова-темы $(\Phi)$ и матрица документы-темы $(\Theta)$ имеют одинаковую важность, поскольку обе являются скрытыми параметрами вероятностной модели. Однако, на практике, исследователи часто считают матрицу $\Phi$ более важной и относятся к матрице $\Theta$ как к чему-то вспомогательному, что может быть легко восстановлено из известных данных.

В первую очередь нужно упомянуть интерпретируемость. Интерпретируемость является желательным свойством хорошей тематической модели. Оценка интерпретируемости человеком обычно состоит из выбора небольшого набора самых вероятных слов для каждой темы и представления этого набора эксперту-человеку \cite{roder2015exploring}. В этом процессе используется только матрица $\Phi$.

Вторым примером подобного подхода является основополагающая работа \cite{rtl}, в которой измерялась интерпретируемость нескольких тематических моделей. Построенные модели вместе с использованной разметкой, были выложены в открытый доступ. Однако, была опубликована только матрица $\Phi $ (возможно, потому что авторы посчитали ее более ценной, или из-за неявного расчёта на то, что недостающее распределение $\Theta$ может быть восстановлено по выложенным данным).

К второстепенности матрицы $\Theta$ можно прийти и из практических соображений.

Во-первых, на практике часто встречаются задачи, требующие динамического расширения коллекции документов (например, анализ новостных потоков). Такое расширение может существенно увеличить $\mid D\mid$, практически не изменив размер словаря $\mid W \mid$ (что объясняется законом Ципфа \todo{cite something else, not powers-1998-applications}).

Во-вторых, требование вычислительной эффективности естественным образом приводит к использованию параллельных, распределённых или онлайновых реализаций алгоритмов тематического моделирования. Наиболее эффективным является следующий подход, реализованный в открытой библиотеке BigARTM: алгоритм разбивает входные данные на пакеты, которые обрабатываются разными потоками \cite{frei2016parallel}. В результате, алгоритмы библиотеки никогда не хранят всю матрицу $\Theta$, вместо этого элементы матрицы рассчитываются, когда они необходимы.

Сложившуюся ситуацию можно назвать парадоксальной. Для многих исследователей качество тематической модели эквивалентно прежде всего качеству матрицы $\Phi$. Но с точки зрения самой тематической модели, $\Phi$ и $\Theta$ являются равноправными, и появление слов в документах коллекции объясняется при помощи обеих этих матриц. Качество матрицы $\Theta$ при этом никак не контролируется, поэтому тематическая модель может скомпенсировать ``плохую'' $\Phi$ специально подогнанной матрицей $\Theta$.

\subsection{Матрица тем в документах и EM-алгоритм}
\label{sec:theta_inference}

Реализации тематического моделирования (особенно  восстанавливающие элементы $\Theta$ ``на лету'') часто используют следующую эвристику: для получения $\theta_{td}$ конкретного документа $d$ повторяются несколько итераций EM-алгоритма с фиксированной $\Phi$. В этой процедуре вектор $\theta_{\ast d}$ сначала инициализируется некоторым образом (как правило, используется равномерное распределение), а затем итеративно обновляется по формуле $\theta_{td}  \propto \sum_{w} n_{dw} p_{tdw}$ с пересчётом $p_{tdw}$. Обновление может происходить какое-то установленное количество итераций либо продолжаться до сходимости.

Отметим, что этот процесс может привести к переобучению, поскольку $\Theta$ целенаправленно оптимизируется для того, чтобы соответствовать заданной $\Phi$. Кроме того, время обучения модели линейно зависит от числа этих итераций, и поэтому слишком большое их количество может существенно замедлить обучение.

Чтобы сделать роль $\Phi$ в EM-алгоритме более значимой, мы предлагаем заменить исходную оптимизационную задачу \ref{eq:EM} на следующую:
\begin{equation} \label{eq:tEM}
L(\Phi, f(\Phi) ) + R(\Phi, f(\Phi) ) \to \max_{\Phi},
\end{equation}

где $f$ --- это некоторая функция, которая отображает матрицу темы-слова в матрицу документы-темы. Решение задачи \ref{eq:tEM} может отличаться от решения задачи \ref{eq:EM}.

В данной работе ставится две цели: вывести оптимизационный алгоритм для \ref{eq:tEM} для функции $f$ конкретного вида, а также проанализировать поведение предложенного алгоритма на реальных текстовых коллекциях.

\section{EM алгоритм}
\subsection{Функция зависимости матриц документы-темы и темы-слова}
Для дальнейшего изложения нужно определить функцию зависимости $\Phi$ и $\Theta$. Другими словами; указать, как рассчитать вероятности тем в документах $p(t\mid d)$, зная только вероятности слов в темах $p(w\mid t)$.

Подчеркнём, что есть бесконечное множество возможных функций зависимости. Например, можно рассмотреть следующее бесконечное семейство: берётся какое-то начальное приближение для $\Theta$, которое затем уточняется на протяжении $k, k \in \mathbb{N}$ итераций.

Однако, мы требуем, чтобы искомая функция была интерпретируемой, простой для анализа и лёгкой для вычислений. Естественным вариантом является усреднение распределений тем слов по всем словам, встречающимся в документе. Более формально:  
\[
P(t \mid d) \propto \sum_w n_{dw} P(t \mid w),
\]
где $P(t \mid w)$ получены по формуле Байеса, предполагая, что распределение $p(t)$ равномерно:
\[
P(t \mid w) = \frac{P(w \mid t)}{\sum_{s=1}^T P(w \mid s)} = \frac{\Phi_{wt}}{\sum_s \Phi_{ws}}.
\]
Если мы обозначим $n_{dw}(\sum_w n_{dw})^{-1}$ за $B_{dw}$, то
\begin{equation} \label{eq:thetaform}
\Theta_{td} = \sum_{w} B_{dw} \frac{\Phi_{wt}}{\sum_s \Phi_{ws}}
\end{equation}

Эту формулу также можно проинтерпретировать как результат первой итерации процесса, описанного в \ref{sec:theta_inference}.

\subsection{Вывод EM алгоритма}
Для вывода EM алгоритма будут использоваться следующие обозначения:\\

$p_{tdw} = \displaystyle\frac{\Phi_{wt} \Theta_{td}}{\sum_s \Phi_{ws} \Theta_{sd}}$,~~$A_{dw} = \displaystyle\frac {n_{dw}}{\sum_s \Phi_{ws} \Theta_{sd}}[n_{dw} > 0]$,

$B_{dw} = \displaystyle\frac{n_{dw}}{\sum_w n_{dw}}$,~~$h_w = \displaystyle\frac{1}{\sum_s \Phi_{ws}}$,~~$C =  A \Phi + \displaystyle\frac{\partial{R}}{\partial{\Theta_{td}}}$.

%\begin{Th} 
\label{thetaless_em}
В EM алгоритме для \ref{eq:tEM} c формулой зависимости \ref{eq:thetaform} E-шаг останется без изменений, а М-шаг будет выглядеть следующим образом:
\begin{equation} \label{eq:Mstep_noTheta}
\Phi_{wt}^{new}  \propto \left( \sum_{d} n_{dw} p_{tdw} + \Phi_{wt}^{old} \left( \frac{\partial{R}}{\partial{\Phi_{wt}}} + h_w (C^T B)_{tw} - h_w^2 ({\Phi^{old}} C^T B)_{ww} \right) \right)_{+}
\end{equation}
%\end{Th}
\begin{proof}
Для вывода формул воспользуемся выводом обобщённого ЕМ алгоритма (GEM). В GЕМ алгоритме на каждой итерации на Е-шаге $\Phi$ и $\Theta$  фиксируются, считаются $p_{tdw}$ и строится функционал: 
\[
Q(\Phi, \Theta) = \sum_{dtw} n_{dw} p_{tdw} \left( \ln \Phi_{wt} + \ln \Theta_{td}\right) + R(\Phi, \Theta).
\]
Изменения функционала данного $Q$ (значений между разными итерацияим) являются нижней оценкой на изменения исходного регуляризированного логарифма правдоподобия. То есть, если $\Delta Q > 0$, то $\Delta(L + R) > 0$.  Поэтому цель М-шага увеличить значение данного функционала по сравнению с $\Phi$ и $\Theta$ с предыдущей итерации.

То, что в предложенном новом подходе $\Theta$ --- это функция от $\Phi$, не меняет тот факт, что изменения $Q$ --- это нижняя оценка изменения $L + R$. Так как это основное требование к функционалу на E-шаге, то поскольку оно выполняется, E-шаг нового алгоритма останется без изменений. Теперь цель М-шага --- подобрать $\Phi$, чтобы увеличить значение по сравнению с $\Phi$ с предыдущей итерации следующий функционал: 
\[
\sum_{dtw} n_{dw} p_{tdw} \left( \ln \Phi_{wt} + \ln (\Theta(\Phi))_{dt}\right) + R(\Phi, \Theta(\Phi)).
\]
Найдём его производные:
\[
\frac{\partial{Q}}{\partial{\Phi_{vr}}} = \frac{1}{\Phi_{vr}} \left( \sum_{d} n_{dv} p_{rdv} + \Phi_{vr} \frac{\partial{R}}{\partial{\Phi_{vr}}} + \sum_{dtw} n_{dw} p_{tdw} \frac{1}{\Theta_{td}} \frac{\partial{\Theta_{td}}}{\partial{\Phi_{vr}}} +  \sum_{dt} \frac{\partial{R}}{\partial{\Theta_{td}}} \frac{\partial{\Theta_{td}}}{\partial{\Phi_{vr}}} \right).
\]
Обозначим за $C_{dt} = \displaystyle\frac{\sum_w n_{dw} p_{tdw} }{\Theta_{td}} + \frac{\partial{R}}{\partial{\Theta_{td}}}$, тогда
\[
\frac{\partial{Q}}{\partial{\Phi_{vr}}} = \frac{1}{\Phi_{vr}} \left( \sum_{d} n_{dv} p_{rdv} + \Phi_{vr}\left( \frac{\partial{R}}{\partial{\Phi_{vr}}} + \sum_{dt} C_{dt} \frac{\partial{\Theta_{td}}}{\partial{\Phi_{vr}}} \right) \right).
\]
Остаётся только найти $\displaystyle\frac{\partial{\Theta_{td}}}{\partial{\Phi_{vr}}}$.
\[
\Theta_{td} = \sum_{w} B_{dw} \frac{\Phi_{wt}}{\sum_s \Phi_{ws}} = \sum_{w} B_{dw} \Phi_{wt} h_w.
\]
\[
\frac{\partial{\Theta_{td}}}{\partial{\Phi_{vr}}} =  \sum_{w} B_{dw}~h_w \delta_{vwrt} +  \sum_{w} B_{dw} \Phi_{wt} \frac{\partial{h_w}}{\partial{\Phi_{vr}}} = 
\]
\[
= \sum_{w} B_{dw} h_w \delta_{vwrt} - \sum_{w} B_{dw}~\Phi_{wt}~h_w^2~\delta_{vw} =
B_{dv}~h_v~\delta_{rt} - B_{dv}~\Phi_{vt}~h_w^2,
\]
где $\delta$ это символ Кронекера. Теперь
\[
\sum_{dt} C_{dt} \frac{\partial{\Theta_{td}}}{\partial{\Phi_{vr}}} = \sum_{dt} C_{dt} \left( B_{dv}~h_v~\delta_{rt} - B_{dv}~\Phi_{vt}~h_v^2 \right) =  
\]
\[
= h_v~\sum_d C_{dr} B_{dv} -  h_v^2~\sum_{dt} C_{dt} B_{dv} \Phi_{vt} = h_v (C^T B)_{rv} - h_v^2 (\Phi C^T B)_{vv}.
\]
Итого
\[
\frac{\partial{Q}}{\partial{\Phi_{vr}}}  = \frac{1}{\Phi_{vr}} \left( \sum_{d} n_{dv} p_{rdv} + \Phi_{vr}\left( \frac{\partial{R}}{\partial{\Phi_{vr}}} + h_v (C^T B)_{rv} - h_v^2 (\Phi C^T B)_{vv} \right) \right).
\]
Далее, выписав условия Каруша-Куна-Таккера, по аналогии с \cite{vorontsov2015} получаем, что
\[
\Phi_{vr}^{new}  \propto \left( \sum_{d} n_{dv} p_{rdv} + \Phi_{vr}^{old} \left( \frac{\partial{R}}{\partial{\Phi_{vr}}} + h_v (C^T B)_{rv} - h_v^2 ({\Phi^{old}} C^T B)_{vv} \right) \right)_{+}.
\]
Так как $p_{tdw}$ считается по $\Phi^{old}$ и $\Theta^{old}$, то значение $C$ можно упростить:
\[
\sum_{w} n_{dw} p_{tdw} \frac{1}{\Theta_{td}}  = \sum_{dtw} n_{dw}\frac{\Phi_{wt}}{\sum_s \Phi_{ws} \Theta_{sd}}  = \sum_{dtw} A_{dw} \Phi_{wt} = \left( A \Phi \right)_{dt},
\]
после чего получаем исходное утверждение.
\end{proof}

\subsection{Анализ асимптотической сложности работы}
На М-шаге предложенного алгоритма потребуется найти матрицы  $C$, $C^T B$ и диагональ матрицы $\Phi C^T B$. 

В силу разреженности $A$ и $B$, умножение на эти матрицы  может быть эффективно реализовано за $O(N S)$, где $N$ -- суммарная длина коллекции, а $S$ --- общая размерность умножаемых матриц. В данном случае это матрицы $\Phi$ и $C$, поэтому $S$ равно числу тем.  

Поскольку матрицы $\Phi$ и  $C^T B$ уже подсчитаны, то диагональные элементы матрицы $\Phi C^T B$ могут быть найдены за $O(W T)$.

Так как вычисление $p_{tdw}$ выполняется за такое же асимптотическое время, то изменение М-шага не приведёт с изменению асимптотики времени работы алгоритма алгоритма.

\section{Эксперименты}
\subsection{Описание}
Эксперименты проводились на трёх стандартных текстовых коллекциях: 20newsgroups (auto,  motorcycles, baseball, hockey, crypt, electronics, med, space), NIPS Conference Papers 1987-2015 Data Set и Twitter Sentiment140 Data Set
При построении моделей использовались $\mid T\mid = 25$ для 20newsgroup, $\mid T\mid = 50$ для NIPS and Twitter.

Сравнивались несколько подходов в тематическом моделировании. \texttt{PLSA}, \texttt{LDA} и \texttt{sparse LDA} обозначают стандартные PLSA и LDA с сглаживающим и разреживающим значением параметра априорного распределения Дирихле. Два варианта предлагаемого подхода были рассмотрены: выведенный в теореме \ref{thetaless_em} (\texttt{TARTM}) и популярный эвристический вывод \texttt{naive TARTM} где на каждой итерации $\Theta$ рассчитывается согласно \eqref{eq:Mstep_Theta} (вместо \eqref{eq:Mstep_noTheta}) и $\Theta$ отбрасывается после расчёта $p_{tdw}$. Для проведения численных экспериментов на языке Python был реализован отдельный модуль, позволяющий проверять различные варианты реализации EM и ARTM. 

\textbf{Использование в качестве регуляризатора}. Если сравнить формулы \ref{eq:Mstep_Theta} и \ref{eq:Mstep_noTheta}, то можно заметить, что они отличаются на слагаемое, имеющее такой же вид, как и $\frac{\partial{R}}{\partial{\Phi_{wt}}}$. Это означает, что вышеописанный итерационный процесс можно ``сэмулировать'' внутри традиционного подхода ARTM, введя фиктивный регуляризатор специального вида\footnote{Заметим, что это новое слагаемое внутри M-шага может не быть  производной какой-либо функции.} и положив, что $\Theta$ должна получаться из $\Phi$ за одну итерацию EM-алгоритма (то есть по формуле \ref{eq:thetaform}) \footnote{Библиотека BigARTM позволяет добиться последнего, если установить \texttt{num\_document\_passes\ =\ 1}.}.

Мы реализовали эту идею на практике, построив специальный регуляризатор внутри библиотеки TopicNet. TopicNet -- открытая надстройка над библиотекой BigARTM, предоставляющая более удобные возможности по работе с пользовательскими регуляризаторами  \cite{bulatov2020topicnet}. Наличие такого регуляризатора будет дополнительным фактором достоверности результатов эксперимента за счёт реализации в сторонней библиотеке и проверке на встроенной и поставляемой вместе с библиотекой текстовой коллекции 20NG. 

Также это позволяет напрямую сравнить результаты TARTM и ARTM с традиционным набором регуляризаторов (сглаживание фоновых тем, разреживание предметных тем, декорреляция) и проверить взаимодействие предложенной формулы с дополнительными регуляризаторами. Для этой реализации сравнивались не значения метрик на итерациях, а финальные значения метрик после заданного числа итераций; мы положили параметр $\mid T\mid = 20$.

\subsection{Метрики}

Для оценки качества полученных тематических моделей использовалось несколько мер качества: разреженность матрицы $\Phi$, cредняя мера Жаккара между верхними токенами (измерялась по 30 токенам), среднее расстояние до ближайшей темы (в качестве метрики использовалось расстояние Дженсена-Шеннона),  LogLift (по множеству из 30 верхних токенов), PMI верхних токенов (по множеству из 10 токенов). Все они описаны в \ref{chap:literature}

\subsection{Результаты}
На Рис. 3-5 изображены основные результаты экспериментов. Во-первых, они показывают, что  самые разреженные модели были ожидаемо получены разреживающим LDA. Тем не менее, TARTM даёт модели сравнимой разреженности, отдельно этого не оптимизируя. Во-вторых, модели TARTM демонстрируют наилучший результат по мере Жаккара (это можно объяснить тем, что TARTM и LDA по-разному обрабатывают  частые, но неинформативные слова; Рис. 1 демонстрирует это на примере трёх сравнимых тем).

На Рис.2 приведены результаты, полученные через библиотеку TopicNet. Они подтверждают ранее описанные результаты, а также показывают, что формула \ref{eq:Mstep_noTheta} эффективно комбинируется с другими регуляризаторами ARTM, что позволяет дополнительно увеличивать метрики качества.

Основное улучшение наблюдается в метриках PPMI и LogLift. TARTM превосходит все другие подходы на коллекциях NIPS и 20newsgroups, но уступает naive TARTM в наборе данных Twitter (однако TARTM сходится за меньшее количество итераций). Мы предполагаем, что это в основном из-за небольшого размера документа в коллекции Twitter, что делает оценку темы более восприимчивой к выбросам. В целом, мы видим, что отбрасывание матрицы $\Theta$ (наивным или строгим образом) дает более согласованные и интерпретирумые тематические модели.

Основное объяснение полученных результатов следующее. PLSA и LDA предсказывают появление слов в документах как с помощью матрицы $\Phi$, так и с помощью матрицы $\Theta$, в то время как TARTM использует только матрицу $\Phi$. Это означает, что PLSA и LDA могут ``скорректироовать'' недостатки матрицы $\Phi$ за счёт правильного подбора матрицы $\Theta$, а TARTM может ``исправлять'' эти недостаки только меняя саму $\Phi$.

Приведём простой пример, иллюстрирующий данные рассуждения. Допустим, коллекция состоит из 7 слов и 6 документов:
\begin{verbatim}
medicine and spices
herbs and spices
herbs and spices and chicken
honey and spices
medicine and herbs
medicine and honey 
\end{verbatim}

Как могла бы выглядеть ``хорошая'' тематическая модель из 4 тем, построенная на этой коллекции? Неинформативное слово \texttt{"and"} может либо лежать в какой-то единственной ``фоновой'' теме ($\exists t_0: \phi_{wt_0} > 0, \phi_{w\ast} = 0$ для $w=$\texttt{"and"}), либо распределиться между несколькими ``информативными'' темами. Первый вариант кажется более естественным. Из 1000 запусков PLSA и TARTM с разными начальными приближениями PLSA ни разу не выделил \texttt{"and"} в отдельную тему, в то время как TARTM сделал это в 365 случаях. При этом матрица $\Theta$, полученная в TARTM содержала от 3 до 7 нулей, а в PLSA от 12 до 17. Это показывает как PLSA с помощью нулей в матрице $\Theta$ ``прячет'' недостатки, вызванные шумами в виде наличия \texttt{"and"} во всех темах.

\section{Заключение}

В данной работе была предложена модификация оптимизационной задачи в тематическом моделировании, которая уменьшает количество оптимизируемых параметров и повышает уникальность и когерентность получаемых тем. Предложенный алгоритм не увеличивает вычислительную сложность или количество необходимых обучающих примеров. Эксперименты на реальных данных показывают, что предложенный алгоритм действительно улучшает качество тем.

Важным аспектом предлагаемого алгоритма является его совместимость с подходом ARTM, что позволяет включать произвольное количество дополнительных регуляризаторов, чтобы точно настроить решение поставленной задачи.

Практически наиболее значимым улучшением подхода является расширение его на случай мультимодальных тематических моделей. При наличии сразу нескольких матриц $\Phi_1, \dots, \Phi_M$ вместо одной $\Phi $, непонятно, как правильно вывести значение $\Theta $. Самый простой способ --- положить $\Theta = f(\Phi_1) $, но он игнорирует значительную часть доступной информации. Дальнейшее исследование может найти более эффективный способ постановки оптимизационной задачи в данном случае.


\clearpage
\begin{figure*}[!h]
\begin{center}
    \begin{small}
    \begin{tabular}{ | p{8.4cm}| p{8cm} |}
    \hline
    TARTM &  LDA \\ \hline
    game team player play season hockey hit league fan baseball \textbf{last} run watch throw pitcher ball stat year sport score & game year team player get \textbf{last} \textbf{good} baseball win play \textbf{go} season hit fan \textbf{think} time \textbf{make} \textbf{well} \textbf{say} league \\ \hline
    car bike buy engine sell speed drive price mile road ride owner dealer drive model driver motorcycle tire detector brake & car bike \textbf{get} engine buy new \textbf{also} drive mile \textbf{make} speed look tire \textbf{well} dealer brake wheel \textbf{go} \textbf{good} road \\ \hline
    period st series vs playoff pt shot king canada ranger lead cup toronto play wing pittsburgh buffalo blue chicago round & period gm vs pt st chicago power pp april shot play buffalo pittsburgh islander flame series lead \textbf{first} scorer cup \\ \hline
    \end{tabular}
    \end{small}
\caption{20newsgroups, примеры наиболее вероятных слов в темах. Слова общей лексики выделены жирным. TARTM убирает подобные слова из тем в отличие от LDA.}\end{center}
\end{figure*}

\begin{figure*}[!h]
\begin{center}
    \begin{small}
    \begin{tabular}{ | p{2.8cm}| p{2.7cm} | p{2.5cm} | p{1.6cm} | p{1.5cm} | p{3.5cm} |}
    \hline
    Алгоритм & Разреженность & Средняя мера Жаккара & PPMI & LogLift & Среднее расстояние до ближайшей темы   \\ \hline
    sparse LDA    & 0.896 & 0.044 & 1.570 & 0.503 & 0.587\\ \hline
    smooth LDA    & 0     & 0.043 & 1.509 & 0.479 & 0.632\\ \hline
    PLSA          & 0.869 & 0.050 & 1.517 & 0.459 & 0.586\\ \hline
    ARTM + $Reg$  & 0.898 & 0.027 & 1.710 & 0.590 & 0.661 \\ \hline
    TARTM         & 0.893 & 0.007 & 1.716 & 0.952 & 0.895 \\ \hline
    TARTM + $Reg$ &\textbf{0.929} & \textbf{0.003} & \textbf{1.788} & \textbf{1.020} & \textbf{0.953} \\ \hline
    \end{tabular}
    \end{small}
\caption{Результаты эксперимента в TopicNet.}\end{center}
\end{figure*}

\clearpage
\todo{Здесь должны быть новые графики}

% \begin{figure*}[!ht]
% \centering
  % \begin{tabular}{@{}cc@{}}
    % \includegraphics[width=\linewidth]{nips_result.eps}
  % \end{tabular}
  % \caption{NIPS, $T = 50$}
% \end{figure*}

% \begin{figure*}[!ht]
% \centering
  % \begin{tabular}{@{}cc@{}}
    % \includegraphics[width=\linewidth]{twitter_result.eps} 
  % \end{tabular}
  % \caption{Twitter, $T = 50$}
% \end{figure*}

% \begin{figure*}[!ht]
% \centering
  % \begin{tabular}{@{}cc@{}}
    % \includegraphics[width=\linewidth]{20news_result.eps} 
  % \end{tabular}
  % \caption{20newsgroups, $T = 25$}
% \end{figure*}


\subsection{The intuition behind performance}
The intuition behind TARTM performance is simple: while the PLSA model predicts observed word counts with the help of $\Theta$ matrix, TARTM model has no such luxury. That means that PLSA can ``hide'' the $\Phi$ imperfections with a carefully crafted $\Theta$ matrix, but TARTM has no choice bu to ``chisel away'' at these imperfections.

% a dirty hack to force n_dw appear on the bottom of this page 
\enlargethispage{1\baselineskip}

We can demonstrate this by the following example. Suppose the corpus consists of three documents: \texttt{herbs and spices}, \texttt{spices and medicine}, \texttt{herbs and medicine}. Hence, the $n_{dw}$ matrix equals to

\begin{center}
\begin{tabular}{l|llll}
$n_{dw}$   & and & herbs & spices & medicine \\ \hline
doc1       & 1   & 1     & 1      & 0        \\
doc2       & 1   & 0     & 1      & 1        \\
doc3       & 1   & 1     & 0      & 1       
\end{tabular}
\end{center}

We are looking for a topic model with four topics. The natural solution is to place each word in a separate topic (in other words, $\Phi = I$ and $\Theta = \frac{1}{3} n_{dw}$):
\begin{minipage}[t]{0.25\textwidth}
\[
\Phi_{tw} = 
\begin{pmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1 \\
\end{pmatrix},
\]
\end{minipage}\begin{minipage}[t]{0.2\textwidth}
\[
\Theta_{td} = \frac{1}{3} 
\begin{pmatrix}
    1 & 1 & 1 & 0 \\
    1 & 0 & 1 & 1 \\
    1 & 1 & 0 & 1 \\
\end{pmatrix}.
\]
\end{minipage}

However, there's no guarantee that PLSA will infer these particular topics. Indeed, many initial approximations give solutions similar to the following:
\[
\Phi_{tw} = 
\begin{pmatrix}
    0.044 & 0.956 & 0 & 0 \\
    0.488 & 0 &  0 & 0.512 \\
    0.488 & 0 & 0.512 & 0 \\
    0.281 & 0 & 0.279 & 0.44 \\
\end{pmatrix},
\]
\[
\Theta_{td} = 
\begin{pmatrix}
    0.349 & 0 & 0.651 & 0 \\
    0 & 0.008 & 0.244 & 0.748 \\
    0.348 & 0.652 & 0 & 0 \\
\end{pmatrix}.
\]
Note that $\Phi \cdot \Theta \approx \frac{1}{3} n_{dw}$. As you can see, the inferred topics are a mixture of ``ideal'' ones, but $\Theta$ distribution masks these imperfections.\\
On the contrary, such mixtures are suboptimal for TARTM model: without a way to fine-tune $\Theta$, such topics are unstable, high-perplexity solutions. Our numeric results corroborate this: TARTM obtained golden standard topics in 998 out of 1000 cases while PLSA failed in every one of 1000 cases.
