\chapter{Повышение интерпретируемости тематических моделей при помощи регуляризации}

В разделе будет предложен метод подбора коэффициентов сглаживания, коэффициентов разреживания и весов дополнительных модальностей. Также мы рассмотрим псевдорегуляризатор, основанный на допущении функциональной зависимости $\Theta = f(\Phi)$ и привносящий в тематическую модель некоторые полезные свойства.

В данной главе ставится цель проанализировать поведение предложенного алгоритма на реальных текстовых коллекциях.

\section{Относительные коэффициенты регуляризации} \label{sec:relative}

% Задачей является объяснить теоретическую подоплёку относительных коэффициентов регуляризации и дать ряд практических советов по их применению.

% В данный момент открытая библиотека bigARTM умеет работать с относительными коэффициентами для $\phi$-регуляризаторов. Таким же образом можно подбирать и коэффициенты для $\theta$-регуляризаторов (возможность, которая сейчас в bigARTM отсутствует). Более того, уже представляется возможность подбирать веса дополнительных модальностей.

Относительные коэффициенты регуляризации были впервые введены в работе \cite{doykov} в общем виде.  В исследовании \cite{doykov} приведена формула для общего случая $k$ произвольных гладких регуляризаторов $R_i(\Phi, \Theta)$.

Предлагаемый подход был реализован в библиотеке BigARTM в следующих регуляризаторах: 

Каждый из этих регуляризаторов имеет параметры \texttt{tau} и \texttt{gamma} (помимо других параметров). Параметр \texttt{gamma} может принимать либо значение из $[0, 1]$, либо равняться \texttt{None}. Если \texttt{gamma=None} (поведение по умолчанию), то \texttt{tau} имеет смысл как абсолютный коэффициент регуляризации. Иначе численное значение \texttt{gamma} означает \textit{степень индивидуализации} каждой темы, а \texttt{tau} интерпретируется как относительный коэффициент регуляризации.

Во избежание путаницы мы отойдём от этих обозначений. Здесь и далее: $\tau_i$ обозначает абсолютный коэффициент регуляризации, а $\lambda_i$~--- относительный коэффициент регуляризации.

Несмотря на поддержку внутри ядра BigARTM, данный подход нечасто используется на практике. Цель текущей секции~--- проанализировать различные аспекты его практического применения. Ниже мы выведем некоторые важные частные случаи и обсудим их значимость.

\subsection{Регуляризация $\Phi$ и важные частные случаи}

Общая формула для регуляризаторов $\Phi$:

\[
\phi_{wt} = \norm_{w \in W}\Bigg(
    n_{wt} + \sum_{i=1}^k \lambda_i \Big[
        \gamma_i \frac{n_t}{r_{it}} + (1-\gamma_i)\frac{n}{r_i}
        \Big]
    \phi_{wt} \frac{\partial R_i}{\partial \phi_{wt}}
\Bigg), \label{rel_phi_general}
\]

где:

\begin{itemize}
    \item{$r_{it} = \sum_{w\in W} \Big| \phi_{wt} \frac{\partial R_i}{\partial \phi_{wt}} \Big| $ --- воздействие регуляризатора на тему}
    \item { $r_{i} = \sum_{t\in T} r_{it}$ --- суммарное воздействие регуляризатора на коллекцию.}
    \item { $\lambda_i$ - относительный коэффициент регуляризации, показывающий, \emph{во сколько раз} соответствующий регуляризатор влияет на оценку $\phi_{wt}$ больше, чем коллекция}
    \item {Выражение в квадратных скобках - \textit{фактор балансировки}, гарантирующий <<равновесие>> между $n_{wt}$ и регуляризационной добавкой}
\end{itemize}

Введённый в этой формуле коэффициент $\gamma_i$ отвечает за степень индивидуализации. Он позволяет плавно переходить от равномерной регуляризации по всем темам ($\gamma = 0$) к индивидуальному подходу к каждой теме ($\gamma = 1$). 

В рамках данной работы рассматривается случай $\gamma = 0$. В этом случае формула выглядит следующим образом:

\[
\phi_{wt} = \norm_{w \in W}\Bigg(
    n_{wt} + \sum_{i=1}^k \lambda_i \Big[
        \frac{n}{r_i}
        \Big]
    \phi_{wt} \frac{\partial R_i}{\partial \phi_{wt}}
\Bigg), \label{rel_phi_general}
\]

Отметим, что величина $r_{i}$ может изменяться в ходе итераций EM-алгоритма. Следовательно, в общем случае абсолютные коэффициенты и относительные  коэффициенты задают различные множества возможных траекторий регуляризации.

Однако, это не так в частном случае регуляризатора сглаживания/разреживания. Известно, что для этого регуляризатора 

\[
\phi_{wt} \frac{\partial R}{\partial \phi_{wt}} = \tau
\]

Тогда

\[
r_{it} = \sum_{w\in W} \Big| \tau \Big| = |W| \cdot |\tau|,
\]

откуда получем, что $r_{it} = \sum_{t\in T} r_{it} = |T| \cdot |W| \cdot |\tau|$. Заметим, что здесь $T$ может быть как множеством всех тем модели, так и множеством тем, на которые действует данный регуляризатор (аналогичное утверждение справедливо и для $W$).

Теперь запишем формулу $M-$шага:
\[
    \phi_{wt} = \norm_{w \in W}\Bigg(
        n_{wt} + \lambda \Big[
            \frac{n}{|W||T|}
            \Big]
    \Bigg)
\]

Если сравнить её с $\phi_{wt} = \norm_{w \in W}(n_{wt} + \tau)$, то становится видно, что сглаживание/разреживание с относительным коэффициентом $\lambda$ эквивалентно сглаживанию/разреживанию с абсолютным коэффициентом $\tau=\lambda \frac{n}{|W||T|}$.

Также естественной может быть другая эквивалентная репараметризация. В случае, когда текущие значения $n_{wt}$ и $\tau \in \mathbb{R}$ таковы, что на этой итерации M-шага зануления компонент не происходит (то есть либо $\tau > 0$, либо $\tau < 0$, но $n_{wt} + \tau > 0$), в формуле M-шага можно раскрыть обозначения:

\[
\phi_{wt} = \norm_{w \in W} \big( n_{wt} + \tau \big) = \frac{n_{wt} + \tau }{\sum_{w \in W} n_{wt} + \tau } = \frac{n_{wt} + \tau }{n_{t} + \tau |W|}
\]

Это выражение можно представить как выпуклую комбинацию распределений  $\frac{n_{wt}}{n_t}$ (оценки максимума правдоподобия) и $\frac{1}{|W|}$ (равномерного распределения):

\[
\frac{n_{wt} + \tau}{n_{t} + \tau |W|} = (1-\lambda) \frac{n_{wt}}{n_t} + \lambda \frac{1}{|W|} \iff \tau_t  = \frac{n_t \lambda}{(1-\lambda) |W|}
\]

Для того, чтобы иметь единый коэффициент для всех тем, произведём усреднение по всем темам: 

\[
\tau = \frac{1}{|T|} \sum_t \tau_t = \frac{n}{|T|\cdot|W|} \frac{\lambda}{(1-\lambda)}  \label{sp_phi_rel2abs}
\]

Значит, сглаживание Фи можно трактовать как нахождение компромисса между $\phi_{wt} = \frac{n_{wt}}{n_t}$ и $\phi_{wt} = \frac{1}{|W|}$, причём приведённая выше репараметризация позволяет напрямую управлять этими пропорциями.

Например, мы хотим провести регуляризацию так, чтобы $\phi_{wt}$ на $\lambda=0.25$ состояла из априорного распределения $\frac{1}{|W|}$ и на $1-\lambda=0.75$ из оценки максимума правдоподобия. Тогда соответствующий $\tau$ будет равен $\tau = \frac{n}{|T|\cdot|W|} \frac1{3}$.

Также заметим, что формула пересчёта $\lambda$ в $\tau$ зависит от $|W|$. Это значит, что один и тот же абсолютный коэффициент сглаживания/разреживания может по-разному влиять на коллекцию в зависимости от размера словаря.  Формула \ref{sp_phi_rel2abs} делает эту зависимость более наглядной. При помощи неё можно эвристически предположить, как нужно пересчитать $\tau$ при изменении словаря (например, после отбрасывания слишком редких или частых слов).

\subsection{Регуляризация $\Theta$ и важные частные случаи}
Общая формула для регуляризаторов $\Theta$:

\[
\theta_{td} = \norm_{t \in T} \Bigg(
    n_{td} + \sum_{i=1}^k \lambda_i \Big[
        \gamma_i \frac{n_d}{r_{id}} + (1-\gamma_i)\frac{n}{r_i}
        \Big]
    \theta_{td} \frac{\partial R_i}{\partial \theta_{td}}
\Bigg), \label{rel_theta_general}
\]

где:

\begin{itemize}
    \item { $r_{id} = \sum_{t\in T} \Big | \theta_{td} \frac{\partial R_i}{\partial \theta_{td}} \Big | $ --- воздействие регуляризатора на документ}
    \item { $r_{i} = \sum_{d\in D} r_{id}$ --- суммарное воздействие регуляризатора на коллекцию.}
    \item { $\lambda_i$ - относительный коэффициент регуляризации, показывающий, \emph{во сколько раз} соответствующий регуляризатор влияет на оценку $\theta_{td}$ больше, чем коллекция}
    \item {Выражение в квадратных скобках - \textit{фактор балансировки}, гарантирующий >>равновесие>> между $n_{td}$ и регуляризационной добавкой}
\end{itemize}

Вычисления по формуле \ref{rel_theta_general} являются проблематичными для онлайнового и пакетных ЕМ-алгоритмов. Это связано с тем, что алгоритм разбивает коллекцию документов на пакеты, обрабатываемые параллельно и связанные только посредством матрицы $\Phi$. Поскольку в общем случае вычисление $r_i$ требует информацию обо всех документах сразу, то формула \ref{rel_theta_general} несовместима с архитектурой эффективного ЕМ-алгоритма.

Тем не менее для ряда важных частных случаев подобрать относительные коэффициенты регуляризации возможно.

\textbf{Сглаживание и разреживание $\Theta$}. Вспомним, что M-шаг для $\Theta$ выглядит таким образом:

\[
\theta_{td} = \norm_{t \in T} \big( n_{td} + \tau \big)
\]

Аналогично выразим эту формулу как выпуклую сумму $\frac{n_{td}}{n_d}$ и $\frac{1}{|T|}$:

\[
\frac{n_{td} + \tau}{n_d + \tau |T|} = (1-\lambda) \frac{n_{td}}{n_d} + \lambda \frac{1}{|T|} \iff \tau = \frac{n_d \lambda}{(1-\lambda) |T|} \label{sp_theta_rel2abs}
\]

Полученный коэффициент регуляризации зависит от $n_d$, то есть разные документы будут сглаживаться/разреживаться с разной силой. Можно также усреднить все $\tau$ по документам:

\[
\tau = \frac{\lambda \sum_d n_d }{(1-\lambda) |D| \cdot |T|} \label{sp_theta_rel2abs_avg}
\]

\textbf{Многомодальное тематическое моделирование}. Вспомним, что вероятность появления термина $w$ из $k$-й модальности в документе $d$ задаётся следующей формулой: $p(w^k \cond d) = \sum_t \phi_{wt}^k \theta_{td}$, а общее правдоподобие представляется так:

\[
L(\Phi^m, \Theta) = \sum_m \tau_m \sum_{d\in D} \sum_{w \in W^m} n_{dw} \ln p(w \cond d) \rightarrow \max, \label{modal_likelihood}
\]

где коэффициенты $\tau_m$ показывают \textit{вес} модальности $m$.

Известно, что $EM$-алгоритм для мультимодальной тематической модели структурно похож на классический $EM$-алгоритм \cite{voron15nonbayesian}. Формулу \ref{modal_likelihood} можно проинтерпретировать как введение $M-1$ дополнительного регуляризатора с коэффициентами $\tau_m$ \cite{voron15nonbayesian}:

\[
\tau_m R(\Phi, \Theta)_m = \tau_m L^{(m)}(\Phi, \Theta) = \tau_m  \sum_d \sum_w n_{dw}^{(m)} \ln \sum_t \phi_{wt}^{(m)}\theta_{td} =
\sum_d \sum_w \check{n}_{dw}^{(m)} \ln \sum_t \phi_{wt}^{(m)}\theta_{td},
\]

где для удобства было введено <<обобщённое число слов>> $\check{n}_{dw}^{(m)} = \tau_m n_{dw}$ и аналогичные величины $\check{n}_{dt}^{(m)} = \sum_w \check{n}_{dt}^{(m)} p_{tdw}, \check{n}_t = \sum_d \check{n}_{dt}$.

Несложно показать, что выражение $r_{md}$ для этого случая выглядит следующим образом:

\[
r_{md} = \sum_t |\theta_{td} \frac{\partial R_m}{\partial \theta_{td}}| = \sum_t \sum_w \check{n}_{dw}^{(m)} p_{tdw} = \sum_t  \check{n}_{dt}^{(m)} = \check{n}_{d}^{(m)}
\]

Иными словами, $r_{md}$ равняется просто числу токенов модальности $m$ в документе $d$ (с учётом коэффициента $\tau_m$), а $r_m$ равняется просто суммарному количеству токенов этой модальности во всей коллекции (также с учётом коэффициента $\tau_m$).

Без ограничения общности примем, что в модели нет других регуляризаторов и задана лишь одна дополнительная модальность. Подставим найденные значения в формулу \ref{rel_theta_general}:

\[
\theta_{td} = \norm_{t \in T} \Bigg(
    n_{td} + \lambda_m \Big[
        \gamma_m \frac{n_d}{r_{md}} + (1-\gamma_m)\frac{n}{r_m}
        \Big]
    \theta_{td} \frac{\partial R_m}{\partial \theta_{td}}
\Bigg) =
\]

\[
\norm_{t \in T} \Bigg(
    n_{td} + \lambda_m \Big[
        \gamma_m \frac{n_d}{\check{n}_d^{(m)}} + (1-\gamma_m)\frac{n}{\sum_d \check{n}_d^{(m)}}
        \Big]
    \theta_{td} \frac{\partial R_m}{\partial \theta_{td}}
\Bigg)
\]

Иными словами, фактор балансировки, <<уравновешивающий>> влияние слов и дополнительной модальности $m$, задаётся через отношение числа токенов модальности $m$ к числу слов в документе (либо во всей коллекции).

Отметим, что фактор балансировки выражается через константы, известные ещё на этапе построения коллекции.  Более того, если рассматривать лишь усреднённые по всем документам коэффициенты, то каждый относительный вес модальности математически равносилен некоторому абсолютному весу и наоборот.

\[
\tau_m = \lambda_m \frac{n}{\sum_d \check{n}_d^{(m)}} \iff
\lambda_m = \tau_m \frac{\sum_d \check{n}_d^{(m)}}{n}
\]

\subsection{Использование на практике}

Взятые вместе, формулы \ref{sp_theta_rel2abs_avg} и \ref{sp_phi_rel2abs} дают следующий результат:

\textbf{Утверждение}. Если рассматривать лишь усреднённые коэффициенты (т.е. с $\gamma=0$), то для регуляризаторов сглаживания и разреживания (как $\Phi$ так и $\Theta$) каждый абсолютный коэффициент регуляризации математически равносилен некоторому относительному коэффициенту и наоборот.

Формулы, задающие перевод из одного в другое, не требуют сложных выкладок. Их можно вычислить не только в ядре bigARTM, но и даже внутри внешнего python-интерфейса.  В отличие от абсолютных коэффициентов/весов,  относительные коэффициенты/веса можно интерпретировать, что упрощает процесс настройки тематической модели.

Это позволяет без каких-либо потерь переформулировать в терминах относительного сглаживания и разреживания любую существующую модель. Отсюда вытекает возможность переносить существующую схему сглаживания/разреживания на другую коллекцию схожей структуры: 1) перевести все абсолютные коэффициенты сглаживания/разреживания в относительные, используя статистики изначальной коллекции; 2) перевести полученные относительные коэффициенты в абсолютные, используя статистики новой коллекции. Аналогичным образом можно адаптировать веса модальностей для другой коллекции.

Можно уверенно утверждать, что относительные коэффициенты/веса являются мощным инструментом для тематического моделирования в рамках bigARTM и заслуживают повсеместного использования.



\section{Аддитивная регуляризация тематических моделей c быстрой векторизацией текста}

С точки зрения вероятностного вывода, матрица слова-темы $(\Phi)$ и матрица документы-темы $(\Theta)$ имеют одинаковую важность, поскольку обе являются скрытыми параметрами вероятностной модели. Однако на практике исследователи часто считают матрицу $\Phi$ более важной и относятся к матрице $\Theta$ как к чему-то вспомогательному, что может быть легко восстановлено из известных данных.

В первую очередь нужно упомянуть интерпретируемость. Интерпретируемость является желательным свойством хорошей тематической модели. Оценка интерпретируемости человеком обычно состоит из выбора небольшого набора самых вероятных слов для каждой темы и представления этого набора эксперту-человеку \cite{roder2015exploring}. В этом процессе используется только матрица $\Phi$.

Вторым примером подобного подхода является основополагающая работа \cite{rtl}, в которой измерялась интерпретируемость нескольких тематических моделей. Построенные модели вместе с использованной разметкой были выложены в открытый доступ. Однако была опубликована только матрица $\Phi$ (возможно, потому, что авторы посчитали её более ценной или неявно рассчитывали на то, что недостающее распределение $\Theta$ может быть восстановлено по выложенным данным).

К второстепенности матрицы $\Theta$ можно прийти и из практических соображений.

Во-первых, на практике часто встречаются задачи, требующие динамического расширения коллекции документов (например, анализ новостных потоков). Такое расширение может существенно увеличить $\mid D\mid$, практически не изменив размер словаря $\mid W \mid$, поскольку размер словаря увеличивается по~сублинейному степенному закону Хипса~\cite{egghe07untangling}. Рост словаря может быть ограничен и~принудительно, путём отбрасывания наименее частотных слов. Таким образом, высокий темп роста размерности модели при расширении коллекции в основном обусловлен расширением матрицы $\Theta$, линейно зависящей от~числа документов в~коллекции.

Во-вторых, требование вычислительной эффективности естественным образом приводит к использованию параллельных, распределённых или онлайновых реализаций алгоритмов тематического моделирования. Наиболее эффективным является следующий подход, реализованный в открытой библиотеке BigARTM: алгоритм разбивает входные данные на пакеты, которые обрабатываются разными потоками \cite{frei2016parallel}. В результате алгоритмы библиотеки никогда не хранят всю матрицу $\Theta$, вместо этого элементы матрицы рассчитываются, когда они необходимы.

Реализации тематического моделирования, восстанавливающие элементы $\Theta$ <<на лету>>, часто используют следующую эвристику: для получения $\theta_{td}$ конкретного документа $d$ повторяются несколько итераций EM-алгоритма с фиксированной $\Phi$. В этой процедуре вектор $\theta_{\ast d}$ сначала инициализируется некоторым образом (как правило, используется равномерное распределение), а затем итеративно обновляется по формуле $\theta_{td}  \propto \sum_{w} n_{dw} p_{tdw}$ с пересчётом $p_{tdw}$. Обновление может происходить какое-то установленное количество итераций либо продолжаться до сходимости.

Отметим, что этот процесс может привести к переобучению, поскольку $\Theta$ целенаправленно оптимизируется для того, чтобы соответствовать заданной $\Phi$. Кроме того, время обучения модели линейно зависит от числа этих итераций, и поэтому слишком большое их количество может существенно замедлить обучение.

Сложившуюся ситуацию можно назвать парадоксальной. Для многих исследователей качество тематической модели эквивалентно прежде всего качеству матрицы $\Phi$. Но с точки зрения самой тематической модели, $\Phi$ и $\Theta$ являются равноправными, и появление слов в документах коллекции объясняется при помощи обеих этих матриц. Качество матрицы $\Theta$ при этом никак не контролируется, поэтому тематическая модель может скомпенсировать <<плохую>> $\Phi$ специально подогнанной матрицей $\Theta$.

В связи с вышесказанным кажется естественным наложить ограничение $\Theta = f(\Phi)$, исключив таким образом переменные $\theta_{td}$ из постановки задачи. 

\subsection{EM-алгоритм с быстрой векторизацией документов}

Естественный кандидат для функции зависимости $\Phi$ и $\Theta$ --- одна итерация процесса, описанного выше с равномерным начальным приближением $\theta^0_{td} = \frac1{|T|}$. После раскрытия обозначений получаем:

\begin{equation}
\label{eq:thetaform}
    \theta_{td}(\Phi)
    = \norm_{t\in T} \biggl( \sum_{w\in W} n_{dw} p_{tdw} \biggr)
    = \sum_{w\in d} \frac{n_{dw}}{n_d} \frac{\phi_{wt} \theta^0_{td}}{\sum_s \phi_{ws} \theta^0_{sd}}
    = \sum_{w\in d} p_{wd} \frac{\phi_{wt}}{\sum_s \phi_{ws}},
\end{equation}

где $p_{wd} = \frac{n_{dw}}{n_d} = \hat p(w\cond d)$~--- частотная оценка условной вероятности терма в~документе. Формулу можно проинтерпретировать как усреднение $n_{dw} P(t \mid w)$) по всем $w \in d$.

Таким образом, мы предлагаем заменить исходную оптимизационную задачу \ref{eq:logL+R} на следующую:

\begin{equation} \label{eq:tEM}
L(\Phi, f(\Phi) ) + R(\Phi, f(\Phi) ) \to \max_{\Phi},
\end{equation}

Отметим, что эта постановка задачи соответствует формализации эвристического подхода, принятого на практике. Решение задачи \ref{eq:tEM} может отличаться от решения задачи \ref{eq:logL+R}. Следующая теорема доказывает, что это действительно так:

\begin{Theorem}
\label{th:TARTM}
    Пусть функция $R(\Phi,\Theta)$ непрерывно дифференцируема, а $\Theta$ находится в функциональной зависимости от $\Phi$ согласно формуле \ref{eq:thetaform}.
    Тогда точка $\Phi$ локального экстремума задачи
    \eqref{eq:tEM} с~ограничениями \eqref{eq:logL.constraints}
    удовлетворяет системе уравнений со вспомогательными переменными, перечисленными ниже:

\begin{align*}
    h_w         &= \bigl( \textstyle\sum_t \phi_{wt} \bigr)^{-1}; \\
    \theta_{td} &= \sum_{w\in d} p_{wd} \phi_{wt} h_w; \\
    p_{tdw}     &= \norm_{t\in T} \bigl(\phi_{wt}\theta_{td} \bigr); \\
    c_{td}      &= \frac1{\theta_{td}}\sum_{w\in d} n_{dw}p_{tdw} + \frac{\partial R}{\partial \theta_{td}}; \\
    \gamma_{dw} &= \sum_{t\in T} \phi_{wt} c_{td}; \\
    p'_{tdw}    &= p_{tdw} + n_d^{-1} \phi_{wt}h_w (c_{td}-h_w\gamma_{dw});
\\
    \phi_{wt} &= \norm_{w\in W}
        \biggl(\,
        \sum_{d\in D} n_{dw} p'_{tdw}
        + \phi_{wt} \frac{\partial{R}}{\partial{\phi_{wt}}}
        \biggr).
\end{align*}
\end{Theorem}

Доказательство теоремы приведено в \cite{thetaless}. Также там описан численный эксперимент с использованием отдельного модуля на языке Python, позволяющего проверять различные варианты реализации EM и ARTM. В рамках текущей диссертационной работы было проведено исследование свойств аддитивно регуляризованной модели с быстрой векторизацией, реализованной на базе открытой библиотеки TopicNet.

\textbf{Использование в качестве регуляризатора}. Отметим, что если раскрыть обозначение $p'_{tdw}$ , то формулу обновления $\Phi$ можно записать в следующем эквивалентном виде:

\begin{equation} \label{eq:Mstep_noTheta}   
    \phi_{wt} = \norm_{w\in W}
        \biggl(\,
        \sum_{d\in D} n_{dw} p_{tdw}
        + \sum_{d\in D} n_{dw} n_d^{-1} \phi_{wt}h_w (c_{td}-h_w\gamma_{dw})
        + \phi_{wt} \frac{\partial{R}}{\partial{\phi_{wt}}}
        \biggr).
\end{equation}

Если сравнить формулы \ref{eq:Mstep_Theta} и \ref{eq:Mstep_noTheta}, то можно заметить, что они отличаются на слагаемое, имеющее такой же вид, как и $\phi_{wt} \frac{\partial{R}}{\partial{\Phi_{wt}}}$. Воспользуемся тем, что программная реализация ещё одного регуляризатора\footnote{Это верно как для BigARTM, так и для TopicNet; отличается лишь используемый язык программирования, \cpp или Python.}  заключается в реализации функции, вычисляющей ещё одно слагаемое в формуле M-шага; никаких проверок того, что это слагаемое является производной какой-либо функции, не происходит (подразумевается, что пользователь сам вычисляет все нужные производные снаружи библиотеки).

Это означает, что вышеописанный итерационный процесс можно <<сэмулировать>> внутри традиционного подхода ARTM, введя фиктивный псевдорегуляризатор специального вида и положив, что $\Theta$ должна получаться из $\Phi$ за одну итерацию EM-алгоритма (то есть по формуле \ref{eq:thetaform}) \footnote{Библиотека BigARTM позволяет добиться последнего, если установить \texttt{num\_document\_passes\ =\ 1}.}.

% TopicNet -- открытая надстройка над библиотекой BigARTM, предоставляющая более удобные возможности по работе с пользовательскими регуляризаторами  \cite{bulatov2020topicnet}. Наличие такого регуляризатора будет дополнительным фактором достоверности результатов эксперимента за счёт реализации в сторонней библиотеке и проверки на встроенной и поставляемой вместе с библиотекой текстовой коллекции 20NG.

\subsection{Эксперименты}

Эксперименты проводились на известной текстовой коллекции 20newsgroups. Сравнивались несколько тематических моделей:

\begin{itemize}
    \item \texttt{PLSA}.
    \item \textbf{LDA}. Рассматривалось три вариации LDA:
    \begin{itemize}
        \item \texttt{lda\_symmetric} --- LDA с симметричным приором на $\Phi$ и $\Theta$: $\eta = \alpha = \frac{1}{T}$.
        \item \texttt{lda\_asymmetric} --- как было рекомендовано в работе \cite{wallach2009rethinking}, используется симметричный приор на $\Phi$ и асимметричный приор на $\Theta$: $\eta=\frac{1}{T}$, $\alpha_{td}\propto\frac{1}{\sqrt{t + T}},~0\leq t \leq T$)
        \item \texttt{lda\_heuristic} --- для сглаживания использовались эвристически рекомендованные величины $\alpha=\frac{50}{T}$ и $\eta=0.01$, ранее использованные в \cite{biggers2014configuring}\cite{rosen2016mobile}
    \end{itemize}

    \item \texttt{sparse\_model} --- модель с одной дополнительной фоновой темой и сглаживанием/разреживанием
    \item \texttt{just\_decorrelated} --- модель с регуляризатором декорреляции
    \item \texttt{heavily\_regularized} --- модель с одной дополнительной фоновой темой и сглаживанием/разреживанием, а также с регуляризатором декорреляции (воздействующим только на предметные темы)
    \item \textbf{TARTM}. Рассматривалось четыре вариации тематической модели с псевдорегуляризатором быстрой векторизации:
    \begin{itemize}
        \item \texttt{thetaless} --- модель с псевдорегуляризатором
        \item \texttt{thetaless\_decorrelated} --- модель с регуляризатором декорреляции и псевдорегуляризатором быстрой векторизации
        \item \texttt{sparse\_thetaless} --- модель с одной дополнительной фоновой темой и сглаживанием/разреживанием, а также с псевдорегуляризатором быстрой векторизации
        \item \texttt{thetaless\_regularized} --- модель с одной дополнительной фоновой темой и сглаживанием/разреживанием, а также с регуляризатором декорреляции (воздействующим только на предметные темы) и псевдорегуляризатором быстрой векторизации
    \end{itemize}
\end{itemize}

Для каждой модели было проведено 5 случайных перезапусков. При построении моделей использовалось $|T|= 20$ (для моделей с фоновой темой использовалось $|T|= 20+1$).

Для всех моделей с регуляризатором декорреляции использовался один и тот же относительный коэффициент $\tau = 0.02$ (при $\gamma=0$). Для всех моделей с фоновой темой использовался один и тот же набор регуляризаторов: сглаживание фоновой темы производилось с относительным коэффициентом $0.05$ (как для $\Phi$, так и для $\Theta$), разреживание предметных тем производилось  с относительным коэффициентом $-0.05$ (как для $\Phi$, так и для $\Theta$). Относительные коэффициенты сглаживания/разреживания впоследствии были пересчитаны в абсолютные по формулам из секции \ref{sec:relative}.

\subsection{Метрики}

Для оценки качества полученных тематических моделей использовалось несколько мер качества: разреженность матрицы $\Phi$, cредняя мера Жаккара между верхними токенами (измерялась по 30 токенам), среднее расстояние до ближайшей темы/среднее попарное расстояние между темами (использовалось несколько метрик расстояния), LogLift (по множеству из 30 верхних токенов). Все они описаны в \ref{chap:metrics}

\subsection{Результаты}
\begin{figure}
\setlength\tabcolsep{0pt} % default value: 6pt
\begin{tabular}{ccc}
    \includegraphics[width=54mm]{images/CH4_baselines_diversity_cosine_False.eps} &   \includegraphics[width=54mm]{images/CH4_baselines_diversity_cosine_True.eps} & \includegraphics[width=54mm]{images/CH4_baselines_jaccard_sim_30.eps} \\

    \includegraphics[width=54mm]{images/CH4_baselines_diversity_euclidean_False.eps} &   \includegraphics[width=54mm]{images/CH4_baselines_diversity_euclidean_True.eps} & \includegraphics[width=54mm]{images/CH4_baselines_loglift30.eps} \\

    \includegraphics[width=54mm]{images/CH4_baselines_diversity_hellinger_False.eps} &   \includegraphics[width=54mm]{images/CH4_baselines_diversity_hellinger_True.eps} & \includegraphics[width=54mm]{images/CH4_baselines_SparsityPhiScore.eps} \\

    \includegraphics[width=54mm]{images/CH4_baselines_diversity_jensenshannon_False.eps} &   \includegraphics[width=54mm]{images/CH4_baselines_diversity_jensenshannon_True.eps} & \includegraphics[width=54mm]{images/CH4_baselines_SparsityThetaScore.eps} \\

\end{tabular}
\begin{minipage}[t]{15cm}
    \caption{Графики зависимости различных критериев качества тематических моделей для пяти моделей (TARTM, PLSA, LDA с 3 видами приоров). Каждой модели соответствуют три линии: среднее значение, минимум и максимум (по пяти случайным перезапускам).}
    \label{fig:ch4_base}
\end{minipage}

\end{figure}

\begin{figure}
\setlength\tabcolsep{0pt} % default value: 6pt
\begin{tabular}{ccc}
    \includegraphics[width=54mm]{images/CH4_vs_regularized_diversity_cosine_False.eps} &   \includegraphics[width=54mm]{images/CH4_vs_regularized_diversity_cosine_True.eps} & \includegraphics[width=54mm]{images/CH4_vs_regularized_jaccard_sim_30.eps} \\

    \includegraphics[width=54mm]{images/CH4_vs_regularized_diversity_euclidean_False.eps} &   \includegraphics[width=54mm]{images/CH4_vs_regularized_diversity_euclidean_True.eps} & \includegraphics[width=54mm]{images/CH4_vs_regularized_loglift30.eps} \\

 \includegraphics[width=54mm]{images/CH4_vs_regularized_diversity_hellinger_False.eps} &   \includegraphics[width=54mm]{images/CH4_vs_regularized_diversity_hellinger_True.eps} & \includegraphics[width=54mm]{images/CH4_vs_regularized_SparsityPhiScore.eps} \\   

    \includegraphics[width=54mm]{images/CH4_vs_regularized_diversity_jensenshannon_False.eps} &   \includegraphics[width=54mm]{images/CH4_vs_regularized_diversity_jensenshannon_True.eps} & \includegraphics[width=54mm]{images/CH4_vs_regularized_SparsityThetaScore.eps} \\

\end{tabular}
% \begin{minipage}[t]{15cm}
    \caption{Графики зависимости различных критериев качества тематических моделей для четырёх моделей (TARTM, и PLSA с тремя различными комбинациями регуляризаторов). Каждой модели соответствуют три линии: среднее значение, минимум и максимум (по пяти случайным перезапускам).}
\label{fig:ch4_vs_reg}
% \end{minipage}

\end{figure}

\begin{figure}
\setlength\tabcolsep{0pt} % default value: 6pt
\begin{tabular}{ccc}

    \includegraphics[width=54mm]{images/CH4_improved_diversity_cosine_False.eps} &   \includegraphics[width=54mm]{images/CH4_improved_diversity_cosine_True.eps} & \includegraphics[width=54mm]{images/CH4_improved_jaccard_sim_30.eps} \\

    \includegraphics[width=54mm]{images/CH4_improved_diversity_euclidean_False.eps} &   \includegraphics[width=54mm]{images/CH4_improved_diversity_euclidean_True.eps} & \includegraphics[width=54mm]{images/CH4_improved_loglift30.eps} \\

 \includegraphics[width=54mm]{images/CH4_improved_diversity_hellinger_False.eps} &   \includegraphics[width=54mm]{images/CH4_improved_diversity_hellinger_True.eps} & \includegraphics[width=54mm]{images/CH4_improved_SparsityPhiScore.eps} \\   

    \includegraphics[width=54mm]{images/CH4_improved_diversity_jensenshannon_False.eps} &   \includegraphics[width=54mm]{images/CH4_improved_diversity_jensenshannon_True.eps} & \includegraphics[width=54mm]{images/CH4_improved_SparsityThetaScore.eps} \\

\end{tabular}
    \caption{Графики зависимости различных критериев качества для пяти моделей (TARTM, TARTM с тремя различными комбинациями регуляризаторов и PLSA с комбинацией фоновых тем и декоррелирования). Каждой модели соответствуют три линии: среднее значение, минимум и максимум.}
    % \caption{какой-то короткий текст}
\label{fig:ch4_improved}
\end{figure}

\begin{figure}
\setlength\tabcolsep{0pt} % default value: 6pt
\begin{tabular}{ccc}
    \includegraphics[width=54mm]{images/CH4_baselines_PerplexityScore.eps} &   \includegraphics[width=54mm]{images/CH4_vs_regularized_PerplexityScore.eps} & \includegraphics[width=54mm]{images/CH4_improved_PerplexityScore.eps} \\
    (a)  & (b) & (c) \\[6pt]
\end{tabular}
    \caption{Сравнение перплексии различных моделей. (a) TARTM и PLSA/LDA; (b) TARTM и регуляризованные модели (c) TARTM и его регуляризованные модификации. Каждой модели соответствуют три линии: среднее значение, минимум и максимум (по пяти случайным перезапускам).}
\label{fig:Perple3x}
\end{figure}

\textbf{Сравнение с PLSA/LDA}. На рисунке \ref{fig:ch4_base} видно, что предлагаемый псевдорегуляризатор позволяет строить модели, темы которых более уникальны, чем темы классических моделей (среднее расстояние между темами и среднее расстояние до ближайшей темы у TARTM больше, чем у других моделей; коэффициент подобия Жаккара, напротив, меньше). Также можно заметить, что введение псевдорегуляризатора не слишком негативно сказалось на перплексии (конечная перплексия сравнима с перплексией \texttt{lda\_heuristic}). Мера качества LogLift показывает улучшение более чем в два раза.

Предлагаемая модель уступает лишь в разреженности матрицы $\Theta$, что вполне ожидаемо (действительно, матрица $\Theta$ уже не оптимизируется моделью в явном виде). Отметим при этом, что она показывает наилучший результат по разреженности матрицы $\Phi$.

\textbf{Сравнение с регуляризованными моделями}. Рисунок \ref{fig:ch4_vs_reg} демонстрирует сходную картину: рассматриваемый псевдорегуляризатор превосходит остальные модели по LogLift и по критериям качества, связанным с различием тем.

Две модели, в которых явно введён регуляризатор разреживания, показывают наилучший результат по разреженности матриц $\Theta$ и $\Phi$. Тем не менее TARTM имеет сравнимую разреженность матрицы $\Phi$ (но этот результат достигается за большее число итераций).

\textbf{Взаимодействие с дополнительными регуляризаторами}. Рисунок \ref{fig:ch4_improved} показывает, что формула \ref{eq:Mstep_noTheta} успешно комбинируется с другими регуляризаторами ARTM, за счёт чего можно улучшить критерии качества ещё больше. TARTM с традиционным набором регуляризаторов (сглаживание фоновых тем, разреживание предметных тем, декорреляция) выигрывает у аналогичного ARTM по разреженности.

% Это свидетельствует о том, что описанная здесь комбинация осмысленна.

% и проверить взаимодействие предложенной формулы с дополнительными регуляризаторами.

% что позволяет дополнительно .

\textbf{Сравнение по когерентности}. У построенных моделей была несколькими способами измерена когерентность.

Поточечная взаимная информация (Pointwise Mutual Information, PMI) и положительная поточечная взаимная информация (Positive Pointwise Mutual Information, PPMI) рассчитывались как $\log\frac{|D| N(w,v)}{N(w)N(v)}$. В этой формуле участвует $N(w,v)$~--- число документов, в~которых встречаются оба слова $w$~и~$v$,

$N(w)$~--- число документов, содержащих слово~$w$ ($N(w,v)$ и $N(w)$ замерены по той же коллекции, по которой обучалась модель). Разница между PMI и PPMI состоит в том, что для последнего отрицательные значения логарифма заменяются на нули. Когерентность модели определяется как средние $\mathrm{PMI}(w,v)$ и $\mathrm{PPMI}(w,v)$ по~всем темам и всем парам верхних слов в~каждой теме.

Для подсчёта UMass-когерентности использовался онлайн-сервис Palmetto. Palmetto позволяет оценивать когерентность множества слов при помощи статистики совстречаемости, собранной на английской <<Википедии>>. Когерентность множеств из 10 верхних слов каждой темы измерялась по отдельности. На основе полученной выборки из 20 (или 21) чисел были рассчитаны среднее, медиана, стандартное отклонение.


\begin{figure}
\begin{tabular}{lrrrrr}
\toprule
{} &  umass &   pmi &  ppmi &  median\_umass &  std\_umass \\
model\_type            &        &       &       &               &            \\
\midrule
heavily\_regularized   & -2.121 & 1.625 & 1.710 &        -1.964 &      1.021 \\
plsa                  & -1.850 & 1.483 & 1.517 &        -1.826 &      0.671 \\
thetaless             & -1.914 & 1.694 & 1.716 &        -1.709 &      0.831 \\
thetaless\_regularized & -2.195 & 1.729 & 1.788 &        -1.826 &      1.220 \\
\bottomrule
\end{tabular}
    \caption{Сравнение различных метрик когерентности PLSA и регуляризованных моделей}
    \label{tab:theta_coh}
\end{figure}

\begin{table}[t]
    \caption{Примеры верхних слов в двух сравнимых темах.  Слова общей лексики выделены жирным шрифтом.  TARTM выделяет слова общей лексики в~отдельные темы, в~отличие от модели~PLSA.}
    \label{fig:2topics}
    \small
    \begin{tabular}{ | p{7.5cm}| p{7.5cm} |}
    \hline
    TARTM &  PLSA
    \\ \hline	
game play team win player hit season lose fan league & \textbf{year} game \textbf{last} win play lose team hit player \textbf{guy}
    \\ \hline
\textbf{make} \textbf{point} \textbf{even} \textbf{case} \textbf{mean} \textbf{keep} \textbf{long} \textbf{actually} \textbf{consider} \textbf{every} & \textbf{make} \textbf{even} \textbf{may}  \textbf{case}  \textbf{consider}  fire  \textbf{less}  \textbf{mean}  \textbf{long}  force
    \\ \hline
    \end{tabular}
\end{table}

Приведённые в таблице \ref{tab:theta_coh} результаты (все приведённые числа --- усреднения по пяти перезапускам) показывают, что TARTM превосходит остальные модели в терминах документной совстречаемости верхних слов; дополнительная регуляризация незначительно улучшает это в дальнейшем. Отдельно следует рассмотреть значения UMass-когерентности: как видно из сравнения среднего, медианы и стандартного отклонения, регуляризация ухудшает <<среднее>> качество тем, но усиливает разрыв между хорошими и плохими темами. Это можно объяснить тем, что TARTM имеет тенденцию выделять частые, но неинформативные слова в отдельную тему; Рис \ref{fig:2topics} демонстрирует это на примере двух сравнимых тем.

\subsection{Интуитивное объяснение особенностей TARTM}

Основное объяснение полученных результатов следующее. PLSA и LDA предсказывают появление слов в документах как с помощью матрицы $\Phi$, так и с помощью матрицы $\Theta$, в то время как TARTM использует только матрицу $\Phi$. Это означает, что PLSA и LDA могут <<скорректировать>> недостатки матрицы $\Phi$ за счёт правильного подбора матрицы $\Theta$, а TARTM может <<исправлять>> эти недостатки, только меняя саму $\Phi$.

Приведём простой пример, иллюстрирующий данные рассуждения. Допустим, коллекция состоит из 4 слов и 3 документов: \texttt{herbs and spices}, \texttt{spices and medicine}, \texttt{herbs and medicine}. Таким образом, матрица $n_{dw}$ равна

\begin{center}
\begin{tabular}{l|llll}
$n_{dw}$   & and & herbs & spices & medicine \\ \hline
doc1       & 1   & 1     & 1      & 0        \\
doc2       & 1   & 0     & 1      & 1        \\
doc3       & 1   & 1     & 0      & 1      
\end{tabular}
\end{center}

Предположим, что мы хотим построить тематическую модель с четырьмя темами. Тематическая модель, наиболее простая из всех возможных, выделяет каждое слово в отдельную тему (иными словами, $\Phi = I$, а $\Theta = \frac{1}{3} n_{dw}$):

\begin{minipage}[t]{0.25\textwidth}
\[
\Phi_{tw} =
\begin{pmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1 \\
\end{pmatrix},
\]

\end{minipage}\begin{minipage}[t]{0.2\textwidth}

\[
\Theta_{td} = \frac{1}{3}
\begin{pmatrix}
    1 & 1 & 1 & 0 \\
    1 & 0 & 1 & 1 \\
    1 & 1 & 0 & 1 \\
\end{pmatrix}.
\]

\end{minipage}

Проведённый численный эксперимент показал, что TARTM успешно восстанавливает описанное выше решение в 998 из 1000 случаев. Тем не менее это решение не является единственным. Большинство найденных при помощи PLSA тематических моделей содержат коэффициенты, не кратные $\frac1{3}$. Ни один из 1000 случайных перезапусков PLSA не дал описанную выше естественную тематическую модель. В качестве примера приведём такую модель:

\[
\Phi_{tw} =
\begin{pmatrix}
    0.044 & 0.956 & 0 & 0 \\
    0.488 & 0 &  0 & 0.512 \\
    0.488 & 0 & 0.512 & 0 \\
    0.281 & 0 & 0.279 & 0.44 \\
\end{pmatrix},
\]

\[
\Theta_{td} =
\begin{pmatrix}
    0.349 & 0 & 0.651 & 0 \\
    0 & 0.008 & 0.244 & 0.748 \\
    0.348 & 0.652 & 0 & 0 \\
\end{pmatrix}.
\]

Несмотря на то что матрица $\Phi$ не разделяет искомые <<идеальные>> темы и является зашумлённой, комбинация $\Phi$ и $\Theta$ адекватно описывает $n_{dw}$ (нетрудно убедиться, что $\Phi \cdot \Theta \approx \frac{1}{3} n_{dw}$).\\

Недостатком данного примера является его искусственность: поскольку $|W|=|T|$, $|T|>|D|$, задача матричного разложения допускает точное (а не приближённое) решение. Можно привести и более правдоподобный пример, демонстрирующий то же явление. Рассмотрим следующую коллекцию из 7 слов и 6 документов:

\begin{verbatim}
medicine and spices
herbs and spices
herbs and spices and chicken
honey and spices
medicine and herbs
medicine and honey
\end{verbatim}

Как могла бы выглядеть <<хорошая>> тематическая модель из 4 тем, построенная на этой коллекции? Неинформативное слово \texttt{"and"} может либо лежать в какой-то единственной <<фоновой>> теме ($\exists t_0: \phi_{wt_0} > 0, \phi_{w\ast} = 0$ для $w=$\texttt{"and"}), либо распределиться между несколькими <<информативными>> темами. Первый вариант кажется более естественным. Из 1000 запусков PLSA и TARTM с разными начальными приближениями PLSA ни разу не выделил \texttt{"and"} в отдельную тему, в то время как TARTM сделал это в 365 случаях. При этом матрица $\Theta$, полученная в TARTM, содержала от 3 до 7 нулей, а в PLSA --- от 12 до 17. Это показывает как PLSA с помощью нулей в матрице $\Theta$ <<прячет>> недостатки, вызванные шумами в виде наличия \texttt{"and"} во всех темах.

\section{Заключение}

В данной главе предложен метод подбора коэффициентов сглаживания, коэффициентов разреживания и весов дополнительных модальностей при помощи математического аппарата \textit{относительных коэффициентов}. Эта техника облегчает построение тематических моделей с фоновыми темами и позволяет перенести имеющуюся стратегию обучения тематической модели на другие текстовые коллекции схожей структуры, поскольку фоновые темы любой существующей модели можно переформулировать в терминах относительного сглаживания и разреживания без каких-либо потерь.

Также была рассмотрена модификация оптимизационной задачи в тематическом моделировании, которая уменьшает количество оптимизируемых параметров. Эксперименты на реальных данных показывают, что предложенный подход действительно улучшает уникальность и когерентность получаемых тем.

Важным аспектом предлагаемого подхода является то, что задаваемый им алгоритм обучения тематической модели можно проинтерпретировать как введение нового псевдорегуляризатора. Экспериментально подтверждена совместимость этого псевдорегуляризатора с подходом ARTM: добавление в модель дополнительных регуляризаторов улучшает качество модели без неожиданных негативных эффектов.
