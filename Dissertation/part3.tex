\chapter{Внутритекстовая когерентность}


\section{Каркас вычисления когерентности}

Большинство подходов к оценке интерпретируемости укладываются в следующую схему:

1) Для каждой темы выбирается какой-то небольшой набор характеризующих её слов. Как правило, это список из 10 самых вероятных её слов, но встречаются и более сложные критерии выбора \cite{blei2009topic}\cite{ldavis2014}\cite{boydcare}; традиционно такие наборы называются \textit{топ-словами} данной темы.

2a) Этот набор демонстрируется человеку-эксперту, который каким-то образом оценивает качество темы

или

2b) Для каждого элемента этого набора собирается ряд статистик совстречаемостей, на основе которого вычисляются какие-то численные показатели

Эта схема впервые появилась во влиятельных работах Бли \cite{rtl} \cite{mrtl} и Мимно \cite{mimno2011}, а затем была существенно доработана исследовательским сообществом. Мы будем называть это семейство метрик \textit{основанным на топ-словах}. 

Главное достоинство этих метрик заключается в их простоте. Вместо того, чтобы изучать распределение вероятностей целиком, исследователю достаточно посмотреть на короткий список самых ``характерных'' слов.

Более того, благодаря работам \cite{mrtl}, \cite{mimno2011}, \cite{aletras2013evaluating}, \cite{roder2015exploring}, эта процедура может быть автоматизирована, что позволяет оптимизировать когерентность напрямую \cite{4keys}.

Однако, именно из этой простоты вытекает их фундаментальное ограничение. Список из 5-10 слов не может отражать распределение вероятностей целиком (ситуация усугубляется тем, что исследователи зачастую игнорруют вероятности топ-слов или даже их порядок в теме). 

Кроме того, по этому списку нельзя делать выводы о том, насколько хорошо тематическая модель описывает конкретную коллекцию документов. Распространена рекомендация базировать статистику совстречаемости на внешнем корпусе (например, Википедия); опять же, это может не отражать специфику конкретной коллекции (например, юридических текстов).

Мы утверждаем, что при помощи короткого списка топ-слов невозможно обосновать качество тематической модели (каким бы образом этот список не анализировался). Это относится и к оценкам, полученными из рук людей-экспертов, и к величинам, вычисленными на основе совстречаемостей слов. Иными словами, критический изъян кроется в пункте (1) вышеописанной схемы, в котором тема целиком огрубляется до конечного числа слов.

\section{Критика метрик, основанных на топ-токенах}
\input{images/astronomers}

To support our claim, we outline three main problems inherent in all top-token based measures. 

\subsection{Агностицизм относительно коллекции}

Автоматизированные метрики когерентности основываются на счётчках совстречаемости слов. Общая идея таких процедур состоит в следующем: 

1) подсчитывается, сколько раз определённые слова оказываются внутри одного и того же скользящего окна (контекста); иногда контекстом может выступать весь документ целиком.

2) полученное число сравнивается с частотой, предсказанной в предположении о том, что все совстречаемости случайны

Если случайное совпадение не может объяснить имеющиесся статистики совместной встречаемости слов, то тема называется \textit{когерентной} (буквально: связной, цельной, согласованной). Таким образом, когерентность определяется через позиции слов: тема называется когерентной, если её слова встречаются кластерами, а не разбросанны по документам случайно.

Обратите внимание на то, что мы делаем выводы о качестве темы в целом на основе поведения десяти её самых частых слов (подобные рассуждения уже критиковались в работе  \cite{dh_sea}). 

Неэквивалентность утверждений ``топ 10 слов темы когерентен'' и ``тема когерентна'' можно показать численно. Более подробно это будет подсчитано в секции \ref{sec:TODO}

% However, our preliminary estimations indicate that the real frequency tend to be dramatically lower (around $0.03\%$ for individual topic).

\subsection{Топ-токены и хвосты распределения}

\dscl{Этому аргументу не хватает конкретных чисел. Возможно, даже с числами будет не совсем убедительно, насколько это может быть вредно на самом деле; может быть надо просто брать не топ-10 слов, а отсекать по суммарному порогу}

Из определения топ-слов следует то, что они не отражают информацию, содержащейсся в хвостах распределения $\phi_{wt}$ (например, список топ-слов не изменится, если обнулить вероятности всех слов кроме 10 наиболее частых).

Это может приводить к ошибочным рассуждениям. Например, в работе~\cite{wang2016catching} была построена тематическая модель, описывающая твиты Дональда Трампа. На основании пропорций тем, полученных из этой модели, была создана модель, предсказывающая число ``лайков''. Авторы приходят к выводу, что самой ``любимой'' темой читателей Трампа является тема \texttt{Демократы}. Здесь делается неявное допущение о том, что тема эквивалентна понятию, объединяющему множество её топ-слов. Похожее допущение принимается и при изучении графика ``вероятность темы в момент времени'', как в работе \cite{monsters_men, dispatch}.

Для того, чтобы сделать это рассуждение более строгим, мы измерим следующее: 1) как изменится распределение $\theta_{td}$, если обнулить хвосты распределения (то есть: отражают ли пропорции $\theta_{td}$ частоты топ-слов темы); 2) какова вероятность встретить топ-слово темы $t$ в теме $t'$ и как это зависит от регуляризации (то есть: насколько можно ``спрятать'' часть тематических слов внутри других тем).

% \subsection{Когерентность как показатель улучшения}

% как когерентность должна выглядеть на графиках "когерентность vs номер итерации". Там должны быть резкие скачки и периоды "стабильности". Моя математическая интуиция тоже говорит мне о том, что когерентность должна меняться только в те моменты, когда какое-то слово вошло в топ 10 или вышло из него, и меняться она при этом должна резкими скачками.

% token ordering

% hence it changes only when some token leaves the top10 list to be replaced by another token.

% when they switch places

% The issue is mitigated somewhat by averaging. 
% If we average within the large number of topics
% Even better, 
% you can restart the inference of topic model several times from different initializations.

% However, this is flawed when researcher is interested in a single topic.

\subsection{Когерентность и стоп-слова}

В работе \cite{fan2019assessing} приводится критика когерентности, основанная на рассуждениях другого характера.

\todo{Про то что множество топ-слов когерентнее чем специфиная тема}

Реальный пример, обнаруженный на практике при построении тематической модели над корпусом 20NG:

law gun state government right weapon crime bill firearm control
-2.258461030291851
uci: 0.7389071029434029

one would get go say think make time know like
-0.9392290418896391
uci: 0.7822380792245011



\section{Эксперимент: никаких оценок без представительства}

Пусть $Q$ --- некое множество слов. Эти слова встречаются в неком документе $d$, который можно считать списком словопозиций: $d = [w_1, w_2, w_3, \dots w_{n_1}]$.

Назовём словопозицию $i, ~w_i \in Q$ \textit{представленной}, если существует какое-то контекстное окно $[w_i, w_{i+1}, w_{i+2}, \dots w_{i+k-1}, w_{i+k}]$, содержащее эту позицию вместе с позицией какого-то другого слова $v, v \in Q$. Таким образом, словопозиция является \textit{представленной} если и только если у неё ненулевое вклад в счётчики совстречаемости множества $Q$.

Теперь можно сформулировать следующий естественный вопрос: если совстречаемости посчитаны на основе 10 топ-слов одной темы (или сразу нескольких тем), то какая доля словопозиций коллекции \textit{представлена} в этих статистиках?

Ясно, что эта величина не может быть больше чем общая частота топ-слов в коллекции (эта частота обычно лежит в промежутке $1\%$ - $5\%$).

\dscl{тут стоит расширить эксперименты, посмотреть больше моделей (в том числе и с разной регуляризацией)}

Мы измерим представительность двух тематических моделей на двух разных коллекциях документов. Первая модель построена на коллекции статей сайта ``ПостНаука'' и состоит из 19 предметных и одной фоновой темы. Вторая модель является наиболее интерпретируемой (согласно людям-экспертам, оценивавшим 10 топ-слов тем) из 9 моделей, рассмотренных в работе~\cite{rtl}. Эта модель построена на подвыборке статей английской Википедии и состоит из 50 тем.

\begin{table}[ht]
\begin{tabular}{lll}
         & ПостНаука & Википедия \\
Минимум  & 0.0159\%  & 0.0065\%  \\
Медиана  & 0.0483\%  & 0.0293\%  \\
Среднее  & 0.0619\%  & 0.0356\%  \\
Максимум & 0.2764\%  & 0.1149\%  \\
Суммарно & 1.2027\%  & 1.6585\% 
\end{tabular}
    \caption{
      Доля коллекции, имеющая ненулевой вклад в счётчики совстречаемости 10 топ-слов. Статистики посчитаны по каждой теме отдельно; строка ``суммарно'' показывает представительность объединённого множества топ-слов всех тем.
    }
    \label{table:represented}
\end{table}

Как можно видеть в таблице \ref{table:represented}, топ-слова покрывают исчезающе малую часть коллекции (менее 2\% корпуса). Когерентность отдельно взятой темы, в большинстве случаев, учитывает менее тысячной доли всего корпуса текста.


\section{Предлагаемая мера: внутритекстовая когрентность}

Как было замечено ранее, традиционные метрики когерентности неявно основываются на допущении о том, что слова ``хороших'' тем часто встречаются рядом. Это похоже на лингвистическую концепцию \textit{когезии} \cite{halliday1976cohesion}: предложения естественного языка подчиненны линейной внутренней структуре. Эта горизонтальная структура организуется 
различными синтаксическими и лексическими средствами: союзами, повторами, словами-заместителями, согласованием временных и иных форм \cite{kazachenko2009}.

В данной работе делается предположение, что тексты естественного языка состоят из связных фрагментов, каждый из которых содержит малое число скрытых тем. Из этого следует, что интерпретируемость темы должна оцениваться не как согласованность наиболее частых слов темы, но как согласованность слов темы внутри связных фрагментов текста. Нужно заметить, что частая совместная встречаемость самых вероятных слов тоже косвенно указывает на то, что тема встречается в текстовой коллекции как связный фрагмент текста.

При помощи этого рассуждения можно построить семейство автоматизированных мер интерпретируемости. Каждая из них будет измерять, насколько ``быстро'' или ``сильно'' меняется тематический профиль соседних слов. 

Таким образом, мы меняем порядок вычислений когерентности. Традиционные меры когерентности сначала выделяют какое-то множество слов по их $\phi_{wt}$ и затем анализируют, каким образом эти слова встречаются в тексте (\emph{от темы к тексту}). В предлагаемом же методе сначала выделяются все соседние слова текста, распределение $\phi_{wt}$ которых затем анализируется (\emph{от текста к теме}).

Первый предлагаемый способ~---~SemantiC (Semantic Closeness)~---~считает семантическую близость близко стоящих в тексте слов как векторов с компонентами $p(t \mid w)$
  \begin{equation}
    \text{SemantiC}_{L2} \Bigm|_{t} = -\biggl\langle\bigl[\rho(w_i, w_j) \leq p_1\bigr] \|\bsym{w_i}-\bsym{w_j}\|_2\biggr\rangle
    \label{coh:semantic}
  \end{equation}
  где $\rho(w_i, w_j) \hm= j \hm- i$~---~расстояние между словами по тексту~---~,
  $p_1$~---~окно, внутри которого слова считаются близкими по тексту.   Знак минус взят перед нормой с тем, чтобы улучшение когерентности означалось её возрастанием.
  Помимо $p_1$, параметром метода является также и способ оценки близости между векторами
  $\bsym{w_i}, \bsym{w_j}$. Дополнительно рассмотрим меру на основе косинусной близости:
  \begin{equation}
    \text{SemantiC}_{Cos} \Bigm|_{t} = \biggl\langle\bigl[\rho(w_i, w_j) \leq p_1\bigr] \cos(\bsym{w_i},\bsym{w_j})\biggr\rangle
    \label{coh:semantic_cos}
  \end{equation}
и меру, отражающую дисперсию вероятностей $p(t\mid w)$, относящихся к заданной теме:
    \begin{equation}
    \text{SemantiC}_{Var} \Bigm|_{t} = -\mathbb{Var}(\bsym{w_i}(t),\bsym{w_{i+1}}(t),\dots,,\bsym{w_{i+window}}(t))
    \label{coh:semantic_var}
  \end{equation}

  
\begin{figure}[ht]
    \caption{Зависимость максимальной из длин, которые выдавал метод TopLen при работе с разными моделями, от длины сегмента (документ состоит из 5 тем).}
    \label{plot:lens-sgm}
    \includegraphics[width=\linewidth]{images/coherence/lens-sgm}
  \end{figure}
 
Следующий способ~---~TopLen (Topic Length)~---~считает среднюю продолжительность темы внутри текста. Мера подсчитывает количество идущих подряд слов, относящихся к заданной теме. В вычислениях также участвует параметр `порог'', уменьшающий чувствительность метрики к словам общей лексики. Чуждые текущей теме слова тоже могут быть учтены, но каждое из них даёт негативный штраф. Когда величина суммарно набранного штрафа пересекает заданный порог, процесс завершается; количество отмеченных слов и будет значением искомой метрики качества.

Вспомогательная функция $\text{score}(w_j, t)$ возвращает разность между компонентой вектора $\bsym{w_j}$, отвечающей теме $t$, и максимальной компонентой из оставшихся.
  
  \begin{lrbox}{\algbox}
  \begin{minipage}{0.9\linewidth}
  \vspace{-1.5cm}
  \begin{algorithm}[H]
  \begin{algorithmic}[1]
  \Function{score}{$w_j, t$}
    %\State $\idxmax(\bsym{w_j}) = \underset{\substack{1 \leq \tau \leq |T|\\\tau \not= t}}{\operatorname{arg\,max}}\, \bsym{w_j}[\tau]$
    \State $w_j$ is scored
    \State \Return $\bsym{w_j}[t] - \bsym{w_j}[\underset{\substack{1 \leq \tau \leq |T|\\\tau \not= t}}{\operatorname{arg\,max}}\, \bsym{w_j}[\tau]]$
  \EndFunction
  \end{algorithmic}
  
  \begin{algorithmic}[1]
    \For {$d \in D$}
      \For {$w_i \in W_d$}
        \If {$w_i \in W_t$ and $w_i$ is not scored}
          \State $\series \gets \max\Bigl\{n : \window + \sum\limits_{j=i}^{i+n}\text{score}(w_j, t) \geq 0\Bigr\}$
        \EndIf
      \EndFor
    \EndFor
    \State $\text{TopLen}\Bigm|_{t}\, \gets \biggl\langle\series\biggr\rangle$
  \end{algorithmic}
  \caption{TopLen}
  \end{algorithm}
  \end{minipage}
  \end{lrbox}
  
  \vspace{0.25cm}
  \begin{flalign}
    &\usebox{\algbox}&
    \label{coh:toplen}
  \end{flalign}
  \todo[inline]{Кажется, что тут либо надо подробно обсудить график (возникает не совсем верное впечатление, что алгоритм находит сегменты длиной 9, когда есть сегменты длиной 300), либо убрать этот кусок вовсе}
  На графике \ref{plot:lens-sgm} видна прямая связь между длиной сегмента датасета и соответствующим значением TopLen. Это подтверждает обоснованность интерпретации TopLen как средней длины темы.
  
  Последний предлагаемый способ~---~FoCon (Focus Consistency)~---~оценивает, как сильно в целом по всему тексту отличаются по тематике смежные слова. Суммируются попарные разности между максимальными вероятностями в каждом векторе $p(t\mid w)$.
  \begin{equation}
    \FoCon \Bigm|_{t} = \sum\limits_{d \in D}\sum\limits_{\substack{w_i, w_j \in W_d\\j-i=1}}
      \bigl|\bsym{w_i}[t_1] - \bsym{w_j}[t_1]\bigr| + \bigl|\bsym{w_i}[t_2] - \bsym{w_j}[t_2]\bigr|,\ 
      \left\{\begin{aligned}
        &t_1 = \argmax\limits_t \bsym{w_i[t]}\\
        &t_2 = \argmax\limits_t \bsym{w_j[t]}
      \end{aligned}\right.
    \label{coh:focon}
  \end{equation}

Метрик FoCon также использует дополнительный механизм, работающий с ``чуждыми'' словами. Метрика вычисляется за два прохода по коллекции. Во время первого прохода для каждой темы $t$ подсчитывается количество словопозиций, где $p(w|t) >= p(w|t')$ и определяется фоновая тема --- тема, имеющая больше всего приписанных к ней словопозиций. Во время второго прохода по формуле (3) вычисляется само значение метрики; при этом два слова считаются соседними, если количество принадлежащих фоновой теме слов между ними не больше заданного порога. 
\section{Постановка задачи}

Оценка интерпретируемости --- очень трудоёмкое мероприятие, даже для процедур, основанных на топ-словах. 
В данной работе эта проблема усугубляется. С одной стороны, мы желаем построить меру качества, учитывающую $\Phi$, $\Theta$ и коллекцию документов целиком. С другой стороны, валидация такой метрики требует сравнения её с человеческими оценками интерпретируемости (что означает необходимость разметки огромного массива данных).

Мы предлагаем способ обойтись без этого затруднительного мероприятия: вместо того, чтобы размечать данные вручную, можно сгенерировать полусинтетический корпус с известной разметкой. Структура коллекции ПостНаука играет в этом важную роль: темы статей настолько обширны и различны, что большинство документов являются \textit{монотематическими}: каждое слово такого документа связано не более чем с одной предметной темой (иными словами, монотематические документы --- это документы, все слова которых являются либо фоновыми, либо относятся к определённой предметной теме). Среди 3446 оригинальных статей исходной коллекции 2118 являются монотематическими. 

Полусинтетическая коллекция будет собрана из фрагментов таких монотематических документов. Исходные тексты разбиваются на сегменты, которые перемешиваются и объединяются в новые документы (см. листинг \ref{coh:dataset_gen}).

% Идея состоит в том, что большие монотематические документы можно разрезать на маленькие монотематические сегменты, которые затем будут случайно сшиты вместе.

  В результате этого процесса получается коллекция данных с известным общим числом тем, распределением тем в документах и даже известными метками тем для слов. Заметим, что любая тематическая модель неявно классифицирует слова заданного документа по темам. В самом деле, любому слову $w$ из документа $d$ соответствует величина $p_{tdw} = p(t \mid d, w \propto \phi_{wt}\theta_{td}$, задаваемая тематической моделью. Таким образом, полусинтетическая коллекция позволяет определить эталонную меру качества произвольной тематической модели, измеряя соответсвие между $p_{tdw}$ и ''эталонной'' меткой слова $w$. Назовём эту меру \textit{качеством сегментации} данной тематической модели. 
  
  Параметрами при создании нового датасета являются количество тем в документе, количество слов в сегменте, количество слов в документе.  Коллекция пересоздаётся несколько раз при разных значениях указанных выше параметров, с тем чтобы можно было изучить, как от них зависит точность тематизации слов внутри сегментов.


  
    % Пусть $\C = \{\Newman, \Mimno, \Cosine, \SemantiC, \TopLen, \FoCon\}$~---~множество исследуемых способов оценки когерентности тематической модели, $\C \ni \coh(\bsym p)$~---~функция из этого множества, которая по коллекции текстов и характеристикам модели даёт оценку её когерентности, $\bsym p$~---~набор параметров функции (размерность $\bsym p$ своя для каждой функции).
  
  Качество способа оценки когерентности $\coh \in \C$ с параметрами $\bsym{p}$ оценивается как коэффицент корреляции Спирмана $R(\coh(\bsym{p}), \segm)$ между значением когерентности и качеством сегментации. В дальнейшем бы будем говорить просто о $R(\coh, \segm)$, имея в виду $\coh(\bsym{p}^*)$, где $\bsym{p}^*$ выбран таким образом, чтобы максимизировать $R(\coh(\bsym{p}^*), \segm)$. 
  
  В данной работе мы ставим задачу показать, что в серии моделей с улучшающейся интерпретируемостью внутритекстовые меры когерентности монотонно возрастают в то время, как способы оценки когерентности по топ-словам нет.
  
  Для этого исследуется множество
  \begin{equation}
  \begin{split}
  \Bigl\{
    \underset{\coh \in \C}{\operatorname{arg\,max}}\, \R(\coh|_{\bsym{p}^*}, \segm) \Bigm|\,
      &\segm \in \Segm,{}\\
    &\bsym{p}^*= \underset{\bsym{p}}{\operatorname{arg\,max}}\, \R(\coh(\bsym{p}), \segm)
  \Bigr\}
  \end{split}
  \end{equation}


\section{Вычислительный эксперимент}

\dscl{Мне кажется, что тут не хватает двух экспериментов. 1) Представительность нужно посмотреть с другим размером множества топ-токенов (не 10, а 20, 30, 50, 100; отсечение по порогу а не по константе). 2) Каким-то обрзом показать, что новые метрики более представительны, в том плане что больше словопозиций вносят свой вклад}



  Качество сегментации текста тематической моделью оценивалось двумя способами
  \begin{itemize}
  \item
    $\soft$: для каждой темы $t$ считается сумма $p(t \mid d, w)$ на всех парах $(d, w),\ d \hm\in D,\ w \hm\in W_d$, итоговый результат~---~сумма таких сумм по всем темам
  \item
    $\strict$: для каждой темы $t$ считается число совпадений темы, предсказанной моделью для данного слова в данном документе, с темой того сегмента, к которому относится слово (при условии, что сегмент относится к теме $t$)\par
    $\Bigl[\argmax\limits_{\tau} p(\tau \mid d, w) \hm= t\Bigr]$.
  \end{itemize}
  Тогда $\Segm = \{\soft, \strict\}$. График \ref{plot:segm_quality-iteration} показывает, как ведёт себя качество мягкой сегментации при увеличении внутренней меры качества тематической модели.

  Для того, чтобы вычислить указанные выше величины, необходимо знать соответствие между темами, выданными моделью, и темами исходного датасета статей "`ПостНауки"'. Для этого использовался венгерский алгоритм, \todo{Ссылка?} 
  который для каждого качества сегментации $\soft$ и $\strict$ выдавал наиболее удачное соответствие тем модельная-исходная.


  \subsection{Эксперимент 3}

For this purpose, we generated a number of different $\Phi$ matrices as a weighted
combination of $\Phi_{good}$ topic model of PostNauka dataset, discussed above) and $\Phi_{bad}$
(a set of random columns taken from Dirichlet ($0.01^{|W|}∣$) distribution):

\[
m(\alpha) = \alpha \Phi_{bad} + (1-\alpha)\Phi_{good}
\]

Для каждого $\alpha$ вычислялись качество сегментации и все изучаемые метрики когерентности. Отсюда была получена выборка $\{⟨soft(m), strict(m), c_1(m), c_2(m), \dots, c_n(m) \mid m \in M, c_i
\in Coh, 1 ≤ i ≤ |\Coh|\}$. Были проведены четыре серии экспериментов с различными матрицами $\Phi_{bad}$.

Все эксперименты с данными были проведены с помощью библиотеки BigARTM
  \cite{vokov2015}.
 
\subsection{Результаты}
Предсталвены три метода оценки интерпретируемости тематических моделей: SemantiC, TopLen
и FoCon. В отличии от традиционных оценок когерентности, предложенные методы пытаются учесть все словопозиции коллекции.

Для того, чтобы сравнить предложенные меры качества с тремя традиционными метриками:  Newman, Mimno и Cosine, было проведено три эксперимента на полусинтетическом датасете, составленном из сегментов различных тем. 

В экспериментах анализировались корреляции между значениями когерентностей и качеством сегментации текстов, а также устойчивость метрик относительно начальных приближений матрицы $\Phi$.

Два из предложенных методов --- SemantiC и TopLen --- и существующяя метрика UMass-когерентность показывают высокие корреляции с качеством сегментации. 

\subsection{Подписи к графикам}
Рисунок показывает фрагмент одного из сгенерированных документов, который состоит из двух соседних сегментов различных тем длиной 50 слов. Слова сегментов обработаны ``хорошей'' моделью и ``плохой'' моделью (которые были описаны ранее). Нераскрашенные слова были отнесены к какой-то теме, отличной от двух ``главных'' тем. Также приведены численные величины различных когерентностей и значения, характеризующие качество сегментации. Полужирным отмечены ситуации, в которых значение когерентности возрастает при улучшении качества модели.

% SQ (S) --- мягкая segmentation quality, SQ (H)—strict segmentation quality, N—Newman, M—Mimno, SC—SemantiC, TL—TopLen, FC—FoCon. 

На графике показана зависимость мер качества от степени деградации матрицы $\Phi$. Согласованность качества сегментации и перплексии говорит о том, что качество сегментации действительно характеризует ``хорошесть'' тематической модели.