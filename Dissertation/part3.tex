\chapter{Внутритекстовая когерентность}
\label{chap:coh}

Данная глава продолжает рассматривать основанные на верхних токенах
меры качества. Будет показана общая проблема всех подходов к оценке интерпретируемости, основанных на верхних токенах.

Также в этой главе будут предложены несколько мер качества, основанных на идее \textit{внутритекстовой когерентности}, и анализирующих распределение $\phi_{wt}$ всех соседних слов текста.

Предполагается, что этот метод будет отражать коллекцию в большей степени, нежели традиционные меры когерентности. Эксперимент на полусинтетической коллекции показывает, что предлагаемый подход действительно отличается большей чувствительностью.

\section{Каркас вычисления когерентности}

Большинство описанных в разделе \ref{sec:toptokens} предыдущей главы подходов к оценке интерпретируемости укладываются в следующую схему:

\begin{enumerate}
    \item Для каждой темы выбирается небольшой набор характеризующих её токенов. Как правило, это список из 10 самых вероятных её токенов, но встречаются и более сложные критерии выбора \cite{Blei_lafferty,ldavis2014,boydcare,frex}; традиционно такие наборы называются \textit{верхними токенами} (top-tokens) данной темы.

    \item{Этот набор анализируется одним из двух способов:
    \begin{itemize}	
        \item Проведя визуальный осмотр этих токенов, эксперт каким-либо образом оценивает качество темы
        \item Для каждого элемента этого набора собирается ряд статистик совстречаемостей, на основе которого вычисляются различные численные показатели
    \end{itemize}
    }
\end{enumerate}

Эта схема впервые появилась во влиятельных работах \cite{rtl,mrtl,newman2010automatic,mimno2011}, а затем была существенно доработана исследовательским сообществом. Мы будем называть данное семейство метрик \textit{основанным на верхних токенах}.

Главное достоинство этих метрик заключается в их простоте. Вместо того чтобы изучать распределение вероятностей целиком, исследователю достаточно посмотреть на короткий список самых <<характерных>> слов.

Более того, благодаря работам \cite{mrtl,mimno2011,aletras2013evaluating,roder2015exploring}, эта процедура может быть автоматизирована, что позволяет оптимизировать когерентность напрямую \cite{4keys}.

Однако именно из этой простоты вытекает их фундаментальное ограничение. Список из 5-10 слов не может отражать распределение вероятностей целиком (ситуация усугубляется тем, что исследователи часто игнорируют вероятности верхних слов или даже их относительный порядок в теме).

Кроме того, по этому списку нельзя делать выводы о том, насколько хорошо тематическая модель описывает конкретную коллекцию документов. Распространена рекомендация базировать статистику совстречаемости на внешнем корпусе (например, <<Википедия>>); опять же, это может не отражать специфику конкретной коллекции (например, юридических текстов).

Мы утверждаем, что при помощи короткого списка верхних слов невозможно обосновать качество тематической модели (каким бы образом этот список ни анализировался). Это относится и к оценкам, полученным из рук людей-экспертов, и к величинам, вычисленным на основе совстречаемостей слов. Иными словами, критический изъян кроется в пункте (1) вышеописанной схемы, в котором тема целиком огрубляется до конечного числа слов.

\section{Критика метрик, основанных на верхних токенах}
\input{images/astronomers}

Для того чтобы обосновать это заявление, мы опишем ряд проблем, присущих всем критериям качества, основанным на верхних токенах.

\subsection{Учёт специфики коллекции} \label{sec:corpus_agnostic}

Автоматизированные метрики когерентности основываются на счётчиках совстречаемости слов. Общая идея таких процедур состоит в следующем:

\begin{enumerate}
    \item Подсчитывается, сколько раз определённые слова оказываются внутри одного и того же скользящего окна (контекста); иногда контекстом может выступать весь документ целиком.
    \item Полученное число сравнивается с частотой, предсказанной в предположении о том, что все совстречаемости случайны.
\end{enumerate}

Если случайное совпадение не может объяснить имеющиеся статистики совместной встречаемости слов, то тема называется \textit{когерентной} (буквально: связной, цельной, согласованной). Таким образом, когерентность определяется через позиции слов: тема называется когерентной, если её слова встречаются кластерами, а не разбросаны по документам случайно.

Ориентирующийся на когерентность исследователь неявно рассуждает таким образом:
\begin{enumerate}
    \item Верхние токены темы имеют высокую когерентность;
    \item Верхние токены темы встречаются в документах согласованным образом;
    \item Все токены темы встречаются в документах согласованным образом;
    \item Тема является хорошей.
\end{enumerate}

Переход от (1) к (2) может быть проблематичным, если статистика совстречаемостей подсчитывается на основании внешнего корпуса (например, <<Википедии>>), а лексика в исследуемой коллекции текстов используется в специфичных значениях или употребляется нетипичным образом.

Переход от (2) к (3) некорректен по другой причине: происходит вывод о качестве темы в целом на основе поведения десяти её самых частых слов (подобные рассуждения уже критиковались в работе  \cite{dh_sea}). Неэквивалентность утверждений <<верхние 10 слов темы когерентны>> и <<вся тема когерентна>> можно показать численно. Более подробно это будет подсчитано в секции \ref{sec:represented}

% However, our preliminary estimations indicate that the real frequency tend to be dramatically lower (around $0.03\%$ for individual topic).

% \subsection{Топ-токены и хвосты распределения}

% \dscl{Этому аргументу не хватает конкретных чисел. Возможно, даже с числами будет не совсем убедительно, насколько это может быть вредно на самом деле; может быть надо просто брать не топ-10 слов, а отсекать по суммарному порогу}

% Из определения топ-слов следует, что они не отражают информацию, содержащуюся в хвостах распределения $\phi_{wt}$ (например, список топ-слов не изменится, если обнулить вероятности всех слов, кроме 10 наиболее частых).

% Это может приводить к ошибочным рассуждениям. Например, в уже упомянутой работе~\cite{wang2016catching} делается вывод о том, что самой <<любимой>> темой читателей Трампа является тема \texttt{Демократы}. Здесь делается неявное допущение о том, что тема эквивалентна понятию, объединяющему множество её топ-слов. Похожее допущение принимается и при изучении графика <<вероятность темы в момент времени>>, как в работе \cite{monsters_men, dispatch}.

% Для того чтобы сделать это рассуждение более строгим, мы измерим следующее: 1) как изменится распределение $\theta_{td}$, если обнулить хвосты распределения (то есть: отражают ли пропорции $\theta_{td}$ частоты топ-слов темы); 2) какова вероятность встретить топ-слово темы $t$ в теме $t'$ и как это зависит от регуляризации (то есть: насколько можно <<спрятать>> часть тематических слов внутри других тем).

% \subsection{Когерентность как показатель улучшения}

% как когерентность должна выглядеть на графиках "когерентность vs номер итерации". Там должны быть резкие скачки и периоды "стабильности". Математическая интуиция говорит о том, что когерентность должна меняться только в те моменты, когда какое-либо слово вошло в топ-10 или вышло из него, и меняться она при этом должна резкими скачками.

% token ordering

% hence it changes only when some token leaves the top10 list to be replaced by another token.

% when they switch places

% The issue is mitigated somewhat by averaging.
% If we average within the large number of topics
% Even better,
% you can restart the inference of topic model several times from different initializations.

% However, this is flawed when researcher is interested in a single topic.

\subsection{Когерентность и стоп-слова}

В работе \cite{fan2019assessing} приводится критика когерентности, основанная на рассуждениях другого характера. Сначала отмечается, что стоп-слова могут быть каноничными (<<the>> и <<and>> для английского языка) или специфическими для данной области (<<ребёнок>> и <<сын>> для корпуса педагогической направленности).

Работа со стоп-словами --- важный шаг, которому редко уделяется должное внимание. В частности, распространена практика итерационного построения LDA с обновлением списка стоп-слов (в который попадают неинформативные верхние слова из модели, построенной ранее). В работе \cite{boydcare} упоминается сходная рекомендация. В пример приводится тема, верхние токены которой состоят из римских чисел: vii, viii, xiv, xiii, xii и т.д. К сожалению, этот подход снижает объективность и воспроизводимость результатов.

В связи с этим в работе \cite{fan2019assessing} подчёркивается и экспериментально подтверждается проблема мер когерентности в условиях наличия стоп-слов. Меры когерентности, основанные на документной совстречаемости, <<поощряют>> присутствие в теме частых неинформативных токенов, поскольку такие токены часто встречаются в одном и том же документе. В результате корреляция
этих показателей качества с экспертными оценками интерпретируемости существенно снижается, когда в верхних словах темы имеются стоп-слова. Кроме того, когерентность не позволяет проконтролировать различность тем.

Описываемое явление действительно встречалось в нашей практике при построении тематической модели над корпусом 20NG (см. таблицу \ref{fig:law_gun_would}).

\begin{table}[ht]
    \caption{10 верхних слов в двух темах и оценки их качества при помощи UMass-когерентности и UCI-когерентности. Первая тема более интерпретируема, при этом численные оценки отмечают её низкую когерентность.}
    \label{fig:law_gun_would}
    \small
    \begin{tabular}{ | p{7.5cm}| p{3.5cm} |p{3.5cm} |}
    \hline
    Верхние слова &  UMass & UCI
    \\ \hline	
law gun state government right weapon crime bill firearm control &
-2.2584 & 0.7389
    \\ \hline
one would get go say think make time know like & \textbf{-0.9392} & \textbf{0.7822}
    \\ \hline
    \end{tabular}
\end{table}

\section{Эксперимент: никаких оценок без представительства}
\label{sec:represented}


\begin{figure}
    %\begin{tabular}{p{7.5cm}p{3.5cm}}
        \includegraphics[width=0.75\textwidth]{doc11358_topic0.png} %&
        % \includegraphics[width=0.25\textwidth]{legend.eps} \\
    %\end{tabular}
    \caption{Демонстрация доли текста, покрытой верхними словами, на примере одного документа. Словопозиции обозначены серо-синим цветом, словопозиции верхних слов показаны красным цветом, зелёным цветом показаны словопозиции, имеющие ненулевой вклад в расчёт когерентности (т.е. попадающие в скользящее окно вместе с другим верхним словом).}
\label{fig:ch3_doc_compound}
\end{figure}


Пусть $Q$ --- некое множество слов. Эти слова встречаются в некоем документе $d$, который можно считать списком словопозиций: $d = [w_1, w_2, w_3, \dots w_{n_1}]$.

Назовём словопозицию $i, ~w_i \in Q$ \textit{представленной}, если существует какое-либо контекстное окно $[w_i, w_{i+1}, w_{i+2}, \dots w_{i+k-1}, w_{i+k}]$, содержащее эту позицию вместе с позицией какого-либо другого слова $v, v \in Q$. Таким образом, словопозиция является \textit{представленной} если и только если у неё ненулевой вклад в счётчики совстречаемости множества $Q$.

Теперь можно сформулировать следующий естественный вопрос: если совстречаемости посчитаны на основе 10 топ-слов одной темы (или сразу нескольких тем), то какая доля словопозиций коллекции \textit{представлена} в этих статистиках?

%  Ясно, что эта величина не может быть больше, чем общая частота топ-слов в коллекции (эта частота обычно лежит в промежутке $1\%$ - $5\%$).

% \dscl{тут стоит расширить эксперименты, посмотреть больше моделей (в том числе и с разной регуляризацией)}

% \dscl{Мне кажется, что тут не хватает двух экспериментов. 1) Представительность нужно посмотреть с другим размером множества топ-токенов (не 10, а 20, 30, 50, 100; отсечение по порогу, а не по константе).}

Мы измерим представительность двух тематических моделей на двух разных коллекциях документов. Первая модель построена на коллекции статей сайта <<ПостНаука>> и состоит из 19 предметных и одной фоновой темы \cite{irina}. Вторая модель является наиболее интерпретируемой (согласно экспертам, оценивавшим 10 верхних слов тем) из 9 моделей, рассмотренных в работе~\cite{rtl}. Эта модель построена на подвыборке статей английской <<Википедии>> и состоит из 50 тем.

\begin{table}[ht]
\begin{tabular}{|l|l|l|} \hline
         & ПостНаука & Википедия \\ \hline
Минимум  & 0.0159\%  & 0.0065\%  \\ \hline
Медиана  & 0.0483\%  & 0.0293\%  \\ \hline
Среднее  & 0.0619\%  & 0.0356\%  \\ \hline
Максимум & 0.2764\%  & 0.1149\%  \\ \hline
Суммарно & 1.2027\%  & 1.6585\%  \\ \hline
\end{tabular}
    \caption{
      Доля коллекции, имеющая ненулевой вклад в счётчики совстречаемости 10 верхних слов. Статистики посчитаны по каждой теме отдельно; строка <<суммарно>> показывает представительность объединённого множества верхних слов всех тем.
    }
    \label{table:represented}
\end{table}

Как можно видеть в таблице \ref{table:represented}, верхние слова покрывают исчезающе малую часть коллекции (менее 2\% корпуса). Когерентность отдельно взятой темы в большинстве случаев учитывает менее тысячной доли всего корпуса текста.

\section{Предлагаемая мера: внутритекстовая когрентность}

Как было замечено ранее, традиционные метрики когерентности неявно основываются на допущении о том, что слова <<хороших>> тем часто встречаются рядом. Это похоже на лингвистическую концепцию \textit{когезии} \cite{halliday1976cohesion}: предложения естественного языка подчинены линейной внутренней структуре. Эта горизонтальная структура организуется
различными синтаксическими и лексическими средствами: союзами, повторами, словами-заместителями, согласованием временных и иных форм \cite{kazachenko2009}.

В данной работе делается предположение, что тексты естественного языка состоят из связных фрагментов, каждый из которых содержит малое число скрытых тем. Из этого следует, что интерпретируемость темы должна оцениваться не как согласованность наиболее частых слов темы, но как согласованность слов темы внутри связных фрагментов текста. Нужно заметить, что частая совместная встречаемость самых вероятных слов тоже косвенно указывает на то, что тема встречается в текстовой коллекции как связный фрагмент текста (см. \ref{sec:corpus_agnostic}).

При помощи этого рассуждения можно построить семейство автоматизированных мер интерпретируемости. Каждая из них будет измерять, насколько <<быстро>> или <<сильно>> меняется тематический профиль соседних слов.

Таким образом, мы меняем порядок вычислений когерентности. Традиционные меры когерентности сначала выделяют какое-то множество слов по их $\phi_{wt}$ и затем анализируют, каким образом эти слова встречаются в тексте. В предлагаемом же методе сначала выделяются все соседние слова текста, распределение $\phi_{wt}$ которых затем анализируется.

Можно предложить несколько принципиально разных подходов к формализации этой идеи.

\begin{figure}
  \small
  \begin{tabularx}{1.0\textwidth}{l| *{3}{Y}|*{3}{Y}|}
    & \multicolumn{3}{c|}{Низкая дисперсия}
    & \multicolumn{3}{c|}{Высокая дисперсия}\\
    \cline{2-7}
    & русский & поэт & Пушкин & Толстой & Рассел & Эйлер \\
    \begin{tabular}[c]{@{}l@{}}$\begin{smallmatrix}\\ \textcolor{my-red}{Literature} \\\\
    \textcolor{green}{Philosophy} \\\\
    \textcolor{blue}{Mathematics} \\\\
    \end{smallmatrix}$\end{tabular}  &
    \begin{tabular}[c]{@{}l@{}}
      $\smalltopicvector{my-red!50}{green!25}{blue!25}$
    \end{tabular} &
    \begin{tabular}[c]{@{}l@{}}
      $\smalltopicvector{my-red!90}{green!10}{blue!10}$
    \end{tabular} &
    \begin{tabular}[c]{@{}l@{}}
      $\smalltopicvector{my-red!90}{green!10}{blue!10}$
    \end{tabular} &
    \begin{tabular}[c]{@{}l@{}}
      $\smalltopicvector{my-red!90}{green!30}{blue!10}$
    \end{tabular} &
    \begin{tabular}[c]{@{}l@{}}
      $\smalltopicvector{my-red!10}{green!50}{blue!50}$
    \end{tabular} &
    \begin{tabular}[c]{@{}l@{}}
      $\smalltopicvector{my-red!10}{green!10}{blue!90}$
    \end{tabular}
  \end{tabularx}
    \caption{Интуитивный смысл меры когерентности, основанной на измерении различности тематик соседних слов. Тематические вектора слева более похожи, чем тематические вектора справа.}
    \label{fig:intracohs_pic1}
\end{figure}
\textbf{Средний разброс в окне.} Семантическую близость расположенных рядом слов можно выразить как дисперсию векторов с компонентами $p(t \cond w)$ (см. рис. \ref{fig:intracohs_pic1}). Данная величина подсчитывается в каждом скользящем окне длиной в $p_1$ слов. Другая естественная вариация --- среднее попарное расстояние векторов $p(t \cond w)$ в скользящем окне. Расстояние можно подсчитывать несколькими способами (например, как евклидово расстояние или как косинусную близость).

\begin{figure}
    \noindent
    $\mbox{Группе}\ \underbrace{\mbox{\textcolor{my-pink}{астрономов}}\ \mbox{удалось}}_{l_1=2}\ \mbox{обнаружить}\ \underbrace{\mbox{\textcolor{my-pink}{звезду}}, \mbox{обращающуюся}}_{l_2=2}$\\
    $\mbox{вокруг}\ \underbrace{\mbox{\textcolor{my-red}{чёрной}}\ \mbox{\textcolor{my-red}{дыры}}\ \mbox{на}\ \mbox{рекордно}\ \mbox{близком}}_{l_3=4}\ \mbox{расстоянии}.$
    \caption{Принцип вычисления длин сегментов для темы $\mbox{<<Чёрные дыры>>}$}
    \label{fig:intracohs_pic3}
\end{figure}

\textbf{Длина сегмента.} Другим показателем тематической однородности может быть средняя продолжительность темы внутри текста. Будем считать, что слово $w$ относится к теме $t$, если разность между компонентой вектора $p(t \cond w)$, отвечающей теме $t$, и максимальной компонентой из оставшихся больше заданного порога (здесь в качестве порога был использован $0$).

На практике оказывается нужен дополнительный механизм, уменьшающий чувствительность меры качества к словам общей лексики. Для этого можно <<разрешить>> ей учитывать чуждые текущей теме слова с небольшим негативным штрафом. Когда величина суммарно набранного штрафа пересекает заданный порог, сегмент отмечается как закончившийся; количество отмеченных слов и будет значением искомой метрики качества (см. рис. \ref{fig:intracohs_pic3}).

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth, height=0.2\textheight]{astronomers_focon.eps} % .eps image is wrong scaled
    \caption{Интуитивный смысл меры когерентности, основанной на подсчёте скачков тематики. Для каждой пары соседних слов $w_1, w_2$ критерий качества суммирует разности $|\phi_{w_1 t_1} - \phi_{w_2 t_1}| + |\phi_{w_1 t_2} - \phi_{w_2 t_2}|$, где $t_1$~--- наиболее вероятная тема слова $w_1$, а $t_2$~--- наиболее вероятная тема слова $w_2$.}
    \label{fig:intracohs_pic2}
\end{figure}

\textbf{Скачки тематики.} Третий подход к измерению внутритекстовой когерентности --- оценивать, как сильно в целом по всему тексту отличаются <<главные>> тематики смежных слов. Суммируются попарные разности между максимальными вероятностями в каждом векторе $p(t\cond w)$. Подразумевается, что наиболее вероятные темы у соседних слов могут различаться, но тема не должна <<затухать>> слишком быстро (см. рис. \ref{fig:intracohs_pic2}).

Этот подход также требует механизма работы с <<чуждыми>> словами: два слова считаются соседними, если количество принадлежащих фоновой теме слов между ними не больше заданного порога (фоновой считается тема, имеющая больше всего приписанных к ней словопозиций в данном документе).

\section{Постановка задачи}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/segm_1.png}
    \caption{На графике показана зависимость мер качества от степени деградации матрицы $\Phi$. Согласованность качества сегментации и перплексии говорит о том, что качество сегментации действительно характеризует <<хорошесть>> тематической модели.}
    \label{plot:segm_quality-iteration}
\end{figure}

Оценка интерпретируемости --- очень трудоёмкое мероприятие, даже для процедур, основанных на верхних словах.
В данной работе эта проблема усугубляется. С одной стороны, мы желаем построить меру качества, учитывающую $\Phi$, $\Theta$ и коллекцию документов целиком. С другой стороны, валидация такой метрики требует сравнения её с человеческими оценками интерпретируемости (что означает необходимость разметки огромного массива данных).

Мы предлагаем способ обойтись без этого затруднительного мероприятия: вместо того, чтобы размечать данные вручную, можно сгенерировать полусинтетический корпус с известной разметкой. Структура коллекции <<ПостНаука>> играет в этом важную роль: темы статей настолько обширны и различны, что большинство документов являются \textit{монотематическими}: каждое слово такого документа связано не более чем с одной предметной темой (иными словами, монотематические документы --- это документы, все слова которых являются либо фоновыми, либо относятся к определённой предметной теме). Среди 3446 оригинальных статей исходной коллекции 2118 являются монотематическими.

Полусинтетическая коллекция будет собрана из фрагментов таких монотематических документов. Исходные тексты разбиваются на сегменты, которые перемешиваются и объединяются в новые документы.

% Идея состоит в том, что большие монотематические документы можно разрезать на маленькие монотематические сегменты, которые затем будут случайно сшиты вместе.

В результате этого процесса получается коллекция данных с известным общим числом тем, распределением тем в документах и даже известными метками тем для слов. Отметим, что любая тематическая модель неявно классифицирует слова заданного документа по темам. Действительно, любому слову $w$ из документа $d$ соответствует величина $p_{tdw} = p(t \cond d, w) \propto \phi_{wt}\theta_{td}$, задаваемая тематической моделью. Таким образом, полусинтетическая коллекция позволяет определить эталонную меру качества произвольной тематической модели, измеряя соответствие между $p_{tdw}$ и <<эталонной>> меткой слова $w$. Назовём эту меру \textit{качеством сегментации} данной тематической модели.

Качество сегментации текста тематической моделью оценивается <<мягким>> образом: для каждой темы $t$ считается сумма $p(t \cond d, w)$ на всех парах $(d,~w),\ d~\in~D, w~\in~W_d$, итоговый результат~---~сумма таких сумм по всем темам. Для того чтобы вычислить указанные выше величины, необходимо знать соответствие между темами, выданными моделью, и темами исходного датасета статей <<ПостНауки>>. Для этого использовался венгерский алгоритм \cite{kuhn1955hungarian}, выдающий наиболее удачное соответствие тем модельная-исходная.

Искомый процесс оценки различных мер качества будет построен следующим образом. Пусть дано множество различных тематических моделей. Для каждой модели можно вычислить её качество сегментации и различные рассматриваемые меры качества (как традиционные меры когерентности, так и предлагаемые меры внутритекстовой когерентности). Таким образом, каждой модели $m_i$ можно сопоставить набор численных значений $f_j(m_i)$ --- замеры её качества при помощи различных методов $f_j$. Величина, которая будет показывать качество какого-либо из методов --- коэффицент корреляции Спирмана между $f_j(m_i)$ и качеством сегментации $m_i$ по всем возможным $m_i$.

Рассматриваются шесть мер качества: 
\begin{itemize}
    \item \texttt{Newman} (также называемый UCI-когерентностью) вычисляется как средний попарный PMI среди всех верхних токенов. Статистика совстречаемостей вычисляется на данном полусинтетическом датасете.
    \item \texttt{Mimno} (также называемый UMass-когерентностью) вычисляется как средний логарифм отношения $p(w_i, w_j)$ к $p(w_j)$, взятый по всем парам слов $(w_i,~w_j)$, в которых $w_j$ более вероятно в теме $t$, нежели слово $w_i$ (т.е. условной вероятности менее вероятного слова с учётом более вероятного). Статистика совстречаемостей вычисляется на данном полусинтетическом датасете. 
    \item  \texttt{SemantiC\_L2}~---~предлагаемый критерий качества, основанный на попарном евклидовом расстоянии между тематическими векторами.
    \item  \texttt{SemantiC\_Var}~---~предлагаемый критерий качества, основанный на дисперсии тематических векторов в окне.
    \item  \texttt{TopLen}~---~предлагаемый критерий качества, основанный на средней длине тематических сегментов в тексте.
    \item  \texttt{FoCon}~---~предлагаемый критерий качества, учитывающий скачки тематик соседних слов.
\end{itemize}

В данной работе мы ставим задачу показать, что в серии моделей с улучшающейся интерпретируемостью внутритекстовые меры когерентности монотонно возрастают, в то время как способы оценки когерентности по верхним словам этого не делают.

\section{Вычислительный эксперимент}

\begin{figure}[hb]

    \centering
    \includegraphics[width=\textwidth]{combine_bad.jpg}

  %\vspace{-0.5cm}
    \scriptsize
    \centering
    \begin{tabular}{rrrrrrrr}
      Сегментация & UCI & UMass & SemantiC L2 & SemantiC Var & TopLen & FoCon\\
      \midrule
      \rowcolor{my-blue-light}
      11000 & -4.8 & -3.1 & -13 & -37000 & 2.9 & -140000\\
      \textbf{38000} & \textbf{-3.7} & \textbf{-2.7} & \textbf{-3.7} & \textbf{-8100} & \textbf{3.5} & \textbf{-54000}
    \end{tabular}
  %\vspace{-0.5cm}

    \centering
    \includegraphics[width=\textwidth]{combine_good.jpg}

    \scriptsize
    \centering
    \begin{tabular}{rrrrrrrr}
      Сегментация & UCI & UMass & SemantiC L2 & SemantiC Var & TopLen & FoCon\\
      \midrule
      11000 & -4.8 & -3.1 & -13 & -37000 & 2.9 & -140000\\
      \rowcolor{my-blue-light}
      \textbf{38000} & \textbf{-3.7} & \textbf{-2.7} & \textbf{-3.7} & \textbf{-8100} & \textbf{3.5} & \textbf{-54000}
    \end{tabular}
  %\vspace{-0.5cm}

    \caption{Рисунок показывает фрагмент одного из сгенерированных документов, который состоит из двух соседних сегментов различных тем длиной 50 слов. Слова сегментов обработаны описанными ранее <<хорошей>> моделью и <<плохой>> моделью. Нераскрашенные слова были отнесены к какой-либо теме, отличной от двух <<главных>>. Также приведены численные величины различных когерентностей и значения, характеризующие качество сегментации. Полужирным отмечены ситуации, в которых значение когерентности возрастает при улучшении качества модели.}
    \label{fig:segm_good_bad}
\end{figure}




График \ref{plot:segm_quality-iteration} показывает, как ведёт себя качество мягкой сегментации при увеличении внутренней меры качества тематической модели.

Выборка тематических моделей, по которой рассчитывалась корреляция Спирмана, представлет собой параметризованное параметром $\alpha$ семейство. Матрица $\Phi$ модели из этого семейства является взвешенной комбинацией

\[
m(\alpha) = \alpha \Phi_{bad} + (1-\alpha)\Phi_{good},
\]

где $\Phi_{good}$ --- матрица $\Phi$ описанной выше модели корпуса <<ПостНауки>>, а $\Phi_{bad}$ --- набор случайных столбцов, порождённых распределением Дирихле ($0.01^{|W|}$). Были проведены четыре серии экспериментов с различными матрицами $\Phi_{bad}$. На рисунке \ref{fig:segm_good_bad} показано различие качества этих моделей на примере сегментации одного из документов.

\subsection{Результаты}
Представлены три метода оценки интерпретируемости тематических моделей: SemantiC, TopLen и FoCon. В отличие от традиционных оценок когерентности, предложенные методы пытаются учесть все словопозиции коллекции.

Для того чтобы сравнить предложенные меры качества с двумя традиционными метриками (Newman и Mimno), был проведён эксперимент на полусинтетическом датасете, составленном из сегментов различных тем.

В экспериментах анализировались корреляции между значениями когерентностей и качеством сегментации текстов. Два из предложенных методов --- SemantiC и TopLen --- и существующая метрика UMass-когерентность показывают высокие корреляции с качеством сегментации.

\begin{figure}
\begin{tabular}{cc}
    \includegraphics[width=70mm]{images/segm_mimno.png} &   \includegraphics[width=70mm]{images/segm_newman.png} \\
 
    \includegraphics[width=70mm]{images/segm_l2.png} &   \includegraphics[width=70mm]{images/segm_var.png} \\
 \includegraphics[width=70mm]{images/segm_toplen.png} &   \includegraphics[width=70mm]{images/segm_focon.png} \\
\end{tabular}
    \caption{Сравнение различных мер когерентности и качества сегментации, нарисованное как функция от степени деградации тематической модели $\alpha$. }
\label{fig:ch3_corr}
\end{figure}




% SQ (S) --- мягкая segmentation quality, SQ (H)—strict segmentation quality, N—Newman, M—Mimno, SC—SemantiC, TL—TopLen, FC—FoCon.
 
