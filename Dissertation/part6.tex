\chapter{Применение регуляризации для кластеризации интентов}

В данной главе предлагается иерархическая мультимодальная регуляризованная тематическая модель, играющая роль первого приближения к структуре коллекции. Главное достижение, описанное в этой главе, --- построение двухуровневой иерархической тематической модели, использующей различные признаки на первом и втором уровнях. Насколько нам известно, ранее в литературе подобный подход не изучался.  Вводится специфичная для задачи мера качества, измеряющая качество иерархических связей между темами и обнаруженными интентами.

Иерархическая структура увеличивает интерпретируемость предлагаемой кластеризации. Точнее говоря, от тем первого уровня требуется, чтобы они описывали субъект диалога, а темы второго уровня должны описывать действие, в котором заинтересован пользователь. Информация о частях речи слов используется для того, чтобы достичь этой цели.

\section{Предобработка} \label{nlp_methods}

Для двух описанных далее коллекций использовалась одна и та же процедура предобработки. Она состоит из большого числа частей (поэтому важно, чтобы каждая из них была сравнительно быстрой), описанных ниже.

\textbf{Первичная обработка данных}: очистка от ненужной информации. В её состав входит:

\begin{itemize}
    \item  \textbf{Фильтрация документов}. Удаление длинных документов, часто встречающихся в качестве сообщений (например, автоматические сообщения — ``Создано новое обращение \#123'').
    \item \textbf{Выделение/удаление простых сущностей}. Выделение URL-адресов, html-тегов, адресов электронной почты. Каждая сущность заменяется на свой тег: например, все гиперссылки заменяются на <URL>. Для каждой из сущностей задан набор регулярных выражений.
\item \textbf{Выделение/удаление географических локаций}. Выделение городов и регионов. Выделение происходит с помощью поиска по некоторому списку географических локаций.
\item \textbf{Посимвольная нормализация}. Удаление повторяющихся знаков пунктуации. Замена нескольких повторяющихся знаков пунктуации на один знак. Замена ё на е. Удаление повторяющихся пробелов.
\item \textbf{Исправление раскладки}. Если в тексте содержится несколько слов не из словаря, которые при смене раскладки будут оказываться в словаре, раскладка всего текста изменится.
\item \textbf{Исправление опечаток}. Опечатки исправляются двухэтапно. Для каждого слова в предложении генерируются кандидаты на исправление, затем из всех кандидатов выбирается лучший с помощью языковой модели. Теоретически алгоритм способен исправлять ошибки: пропущено две буквы, добавлено две лишних буквы, два раза две буквы поменялись местами, слились два соседних слов в одно. Далее процесс исправления опечаток будет рассмотрен более подробно.
\end{itemize}

Нормализация данных:

\begin{itemize}
\item \textbf{Токенизация}. Токенизация документов (разделение на отдельные токены) с помощью библиотеки spacy. Сначала документ разделяется на токены по любым символам, которые не являются буквой или цифрой. Затем, согласно специальным шаблонам, некоторые группы токенов соединяются вместе.
\item \textbf{Выделение/удаление имён/фамилий}. Выделение имён и фамилий. Производится с помощью библиотеки pymorphy2.
\item \textbf{Лемматизация}. Лемматизация документов (приведение всех слов в начальную форму) с помощью библиотеки pymorphy2.
\item \textbf{Учёт частицы не}. Склеивание не со следующим словом, если слово является глаголом (таким образом, тематическая модель может отличить ``получается'' от ``не получается'').
\item \textbf{Выделение информативных $n-$грам}. Выделение $n-$грам (коллокаций или устойчивых словосочетаний) с помощью алгоритма TopMine. Далее этот этап будет рассмотрен более подробно.
\item \textbf{Удаление стоп-слов}. Удаление стоп-слов по заранее заданному списку слов и списку регулярных выражений.
\item \textbf{Разделение данных на группы}. Разделение слов и словосочетаний на две группы: “тематичные/предметные” и “функциональные”. Процедура и смысл этого разделения будут описаны ниже.
\end{itemize}

\subsection{Объединение и фильтрация реплик}

Дополнительно происходит фильтрация некоторых реплик, определённых как “мусорные”. Обычно это дословный copy-paste текста какого-либо нормативного документа или содержимого веб-страницы (в том числе и предыдущего диалога).

Обучение кластеризации на таких объектах снижает итоговое качество модели. Кроме того, становится неудобно изучать выдачу топ-документов тем. Используются три критерия <<мусорности>> реплики (реплика удаляется, если справедлив хотя бы один из них).

Первым критерием является \textbf{многострочность}. Главными признаками этого критерия считается достаточная длина реплики (больше 6 строк или 10 слов) при малом количестве знаков препинания (каждая строка в среднем содержит менее 2/3 знака).

Данный критерий выделяет содержимое веб-страницы, скопированное в чат. В этом случае перенос строки означает не конец абзаца, а конец структурного элемента страницы; соответственно, отсутствует завершающий предложение знак препинания.

Вторым критерием является \textbf{пропорция одинаковых слов}. Для его проверки сравнивается длина реплики в словах и количество уникальных слов в реплике. Реплика считается мусорной, если каждое уникальное слово встречается в ней больше $2.5$ раз (в среднем). Это правило применяется только к репликам, состоящим более чем из 10 слов.

Критерий позволяет найти реплики, в которых одна и та же фраза повторяется несколько раз.

Третий критерий основан на \textbf{скорости убывания частот слов}. Частотность естественной речи обычно подчиняется закону Ципфа или похожим степенным распределениям; самое частое в тексте слово должно быть в несколько раз более распространённым, чем следующее по частоте.

Для проверки этого критерия все слова сортируются по их числу вхождений в реплику, затем на основании частот трёх самых распространённых слов $w_1~,w_2~,w_3$ вычисляется скорость убывания частоты слов ($\frac{n_{d{w_1}}}{n_{dw_2}}$ и $\frac{n_{d{w_2}}}{n_{dw_3}}$). Реплики, состоящие из одной строки, считаются мусорными, если оба этих отношения не превосходят $1$. Для многострочных реплик оба этих отношения должны быть не меньше $2$.

Расчёт частот идёт без учёта предлогов, местоимений, союзов и цифр. Это правило выделяет реплики, содержащие многократное повторение одной и той же фразы с небольшими изменениями или непохожие на естественную речь. Примером последнего служат тексты песен или текст отчёта об ошибке (error traceback).

После удаления мусорных реплик происходит объединение всех сообщений диалога в один документ. Реплики оператора не учитываются в этом объединении. Таким образом, один документ коллекции --- это конкатенация всех реплик пользователя из одного диалога.

\subsection{N-grams extracting}

\par Распространённая техника, улучшающая интерпретируемость тематических моделей --- использование информативных нграм (или коллокаций) в качестве дополнительной модальности. Для того чтобы извлечь информативные нграмы, был использован алгоритм TopMine \cite{topmine}, основанный на статистике совстречаемостей слов.

Для нашей задачи имело смысл внести ряд правок в алгоритм TopMine.

Во-первых, исходный алгоритм вычисляет статистики совстречаемости для упорядоченных кортежей слов $(w_1, w_2, \dots, w_k)$. В частности, алгоритм различает последовательности $(w_1, w_2)$ и $(w_2, w_1)$. Эта особенность делает алгоритм TopMine плохо приспособленными для обработки текстов на языках с нестрогим порядком слов (таких как русский). По этой причине логика подсчёта статистик совстречаемостей алгоритма была модифицирована таким образом, чтобы в ней использовались мультимножества слов вместо последовательностей слов.

Во-вторых, TopMine не выделяет пересекающиеся коллокации. Это приводит к тому, что похожие предложения (``записать ребёнка в детский сад'' и ``записать ребёнка в детский садик'') могут вовсе не содержать общих коллокаций. Примером служат предложение «получение паспорта РФ» (выделится коллокация \texttt{получение\_паспорт\_РФ}) и предложение «паспорт РФ был утерян» (выделится коллокация \texttt{паспорт\_РФ}). Это следует из процесса поиска коллокаций алгоритмом: на каждом шаге обработки соседние коллокации-кандидаты сливаются, если их объединение удовлетворяет критерию информативности. Для того чтобы устранить вышеописанную проблему, достаточно изменить процесс итеративного слияния фраз так, чтобы при успешном слиянии коллокаций они не удалялись из множества кандидатов. Данная модификация увеличивает расход памяти алгоритма, однако делает процесс поиска не ``жадным''.

\subsection{Named entity recognition}

\par В текстах диалогов содержится большое число упоминаний имён участников, названий компаний/продуктов и названий улиц/городов. Аккуратный учёт такого рода сущностей даст прирост качества моделей, а также сделает списки верхних слов более интерпретируемыми (к информативным словам не будут примешиваться имена операторов-собеседников).

Поэтому в рамках предобработки коллекции все обнаруженные именованные сущности заменялись на теги $\langle \mathrm{PERSON} \rangle$.

Рассматривалось несколько подходов к выделению именованных сущностей: с помощью правил/словарей и с помощью использования RNN+CRF модели.

Нейронная сеть из работы \cite{burtsev}, предобученная на датасете PERSONS-1000 (\cite{persona}), давала лучшее качество выделения именованных сущностей. Однако соображениями вычислительной эффективности был продиктован выбор модуля \texttt{pymorphy2} (основанного на правилах) для окончательной версии конвейера предобработки.

\subsection{Коррекция ошибок}

\par Ошибки и опечатки --- частое явление в дилоговых корпусах. Для того чтобы сделать текстовую коллекцию более подходящей для тематического моделирования, необходимо использовать алгоритм коррекции ошибок. В данной работе используется Jamspell\footnote{\href{https://github.com/bakwc/JamSpell}{Jamspell github}} по причине высокой скорости его работы.

\par Был проведён ряд модификаций для того, чтобы адаптировать модель Jamspell к нашему случаю. Во-первых, языковая модель, согласно которой выбирается лучший кандидат для исправления, была дообучена на кластеризуемой коллекции. Эта модификация нужна для учёта специфики коллекции; в противном случае специфичные для коллекции слова (такие как ``гибдд'' или ``адсл'') считались бы незнакомыми и в них исправлялись бы несуществующие опечатки.

Во-вторых, было расширено множество кандидатов для исправления. Согласно статистике поиска Яндекса\footnote{Сложности русского языка: \texttt{https://yandex.ru/company/researches/2016/ya\_spelling}}, одной из наиболее распространённых категорий ошибок является слитно-раздельное написание слов; в текстах коллекции также часто встречаются ``слипшиесся'' слова (<<\textbf{очередьк} врачу>>). Поэтому все возможные разбиения слова на два также рассматривались как кандидаты на исправление.

\section{Построение модели}

Для кластеризации полученных объектов строится тематическая модель. \par Темы тематической модели, которую мы стремимся построить, должны соответствовать пользовательским интентам. В данной работе используется следующее операциональное определение интента: два диалога (представленные репликами пользователя) считаются \textit{имеющими одинаковый интент}, если оба пользователя были бы удовлетворены практически одинаковой реакцией оператора колл-центра. Это определение, несмотря на свои недостатки, позволяет подчеркнуть ряд важных практических проблем:

\begin{itemize}
\item Простого подхода, основанного на гипотезе ``мешка слов'', недостаточно. Сравните: ``Я хочу заблокировать свою учётную запись, что мне делать?`` and ``Моя учётная запись заблокирована, что мне делать?``.

\item В некоторых случаях замена одного слова изменяет интент реплики, а в других --- нет. В контексте имеющейся коллекции ``я хочу записаться на приём к кардиологу`` и ``я хочу записаться на приём к неврологу`` считаются одинаковыми интентами, поскольку требующиеся от пользователя/оператора действия практически не отличаются. Однако ``уплата госпошлины за паспорт`` и ``уплата госпошлины за автомобиль`` существенно различаются.

\end{itemize}

Для того чтобы получить больше контроля над выделением интентов, предлагается двухуровневая иерархическая тематическая модель. Первый уровень предназначен для грубого определения похожести, в то время как второй уровень учитывает менее очевидные, но важные признаки.

Иерархическая АРТМ модель представляет собой набор различных АРТМ моделей для каждого уровня, связанных друг с другом. Первый уровень иерархии может быть любой тематической моделью. Второй уровень строится с использованием специального регуляризатора из работы \cite{chirkova2016additive}, который стремится сделать модель такой, чтобы темы первого уровня выражались через взвешенную сумму тем второго уровня.

Выход иерархической модели — распределение тем первого и второго уровня по словам и документам, а также матрица зависимостей тем второго уровня от тем первого уровня.

Для обеспечения того, чтобы каждая родительская тема была связана лишь с небольшим числом дочерних тем, можно применить различные методы. Обычно используется регуляризатор, разреживающий междууровневые связи \cite{chirkova2016additive} ; также в литературе была предложена процедура прямого удаления ``плохих'' рёбер (в соответствии с мерой качества EmbedSim \cite{belyy}).

\subsection{Построение уровней иерархии} \label{hierarchy_distinct}

Для того чтобы построенные моделью темы второго уровня существенно отличались от тем первого уровня, мы модифицировали признаковое пространство для модели второго уровня.

В контексте поставленной задачи было предпринято разделение признаков по их функциональному назначению. Из всех токенов (слов и нграм) были выделены две группы на основании их частеречного состава: ``тематическая'' и ``функциональная''. ``Функциональная'' группа состоит из одиночных глаголов и нграм, содержащих хотя бы один глагол. ``Тематическая'' группа состоит из одиночных существительных, одиночных прилагательных и нграмм, включающих в себя хотя бы одно существительное и не имеющих в своём составе глаголов.

Таким образом, предлагаемая тематическая модель имеет пять модальностей: \texttt{@lemmatized} (просто слова), \texttt{@verb\_lemmatized} (слова-глаголы), \texttt{@noun\_lemmatized} (слова-существительные и слова-прилагательные), \texttt{@theme\_ngramms} (нграмы с существительными и без глаголов), \texttt{@verb\_ngramms} (нграмы с глаголами).

% Inspired by multi--level \cite{Tang2018SubgoalDF} and multi--syntactic \cite{Gupta2018SemanticPF} phrases annotation, among with hierarchical partition, our approach is essential for client goal and subgoals extraction.

\par Первый уровень иерархии предназначен для определения субъекта диалога (то есть сущностей, о которых идёт речь). По этим соображениям вес тематической группы токенов должен быть существенно выше, чем вес функциональной группы. Цель второго уровня иерархии --- нахождение интента клиента, касающегосся определённых сущностей (то есть действий, которые пытается предпринять клиент). На этом уровне функциональные токены должны иметь большее влияние, нежели токены тематической группы. Токены модальностей, напрямую не относящихся к данным двум группам, используются на всех уровнях; они выступают в роли связи между двумя разными слоями иерархии.

Сначала строится первый уровень тематической модели, затем второй, так, чтобы любая тема первого уровня являлась линейной комбинацией тем второго уровня. Для каждого из уровней используется свой набор коэффициентов важности категорий признаков (модальностей).
Первый уровень соответствует разделению документов на “темы”, поэтому для построения уровня больший вес имеют “тематичные” признаки: слова-существительные и слова-прилагательные, словосочетания, не содержащие глаголов.
Второй уровень соответствует разделению документов на “интенты”, поэтому для построения второго уровня больший вес по сравнению с первым уровнем имеют признаки: слова-глаголы, словосочетания, содержащие глаголы.
На обоих уровнях для построения кластеризации с небольшим весом используются словосочетания из реплик операторов. Для того чтобы модель была устойчива к разным коллекциям, используются не абсолютные веса для задания значимости категории признаков, а относительные (вес — число от 0 до 1, равный вес у разных категорий означает одинаковую значимость этих категорий для модели).
% На каждом уровне модели используется некоторое количество “фоновых” тем. Эти темы являются техническими, их цель — собирать внутри себя слова фоновой лексики, не влияющие на кластеризацию. К этим темам нельзя получить доступ через систему запросов.
% На каждом уровне модели используется свой набор регуляризаторов со своими коэффициентами регуляризации. Все коэффициенты, за исключением коэффициента регуляризатора декоррелирования (отвечающего за повышение отличия разных тем друг от друга), настраиваются автоматически.

\subsection{Обработка результатов моделирования}

Выход иерархической модели не обязан по построению иметь древовидную структуру (хотя и стремится к ней). Чтобы выход иерархии был древовидным, используются дополнительные преобразования итоговой структуры: для каждой темы второго уровня выбирается единственная тема первого уровня.

Каждую полученную тему можно представить наборами репрезентативных документов и репрезентативных токенов. Над набором документов проводится дополнительная фильтрация: удаляются документы, в которых почти не встречается самых репрезентативных нграмм, удаляются тексты с непропорционально большим количеством заглавных букв.

\section{Experiments}

В экспериментах были использованы две коллекции диалогов из русских колл-центров (примерно по 90 тысяч диалогов в каждой, средняя длина диалога --- шесть реплик). Первый корпус состоит из диалогов клиентов с представителями различными государственных организаций. Второй корпус представляет собой логи технической поддержки провайдера. Оба датасета непубличны.

\subsection{Оценка качества}

Существует ряд подходов для оценки качества тематических моделей, в частности их интерпретируемости. Традиционная процедура включает в себя анализ списка верхних слов каждой из тем людьми. Однако данный подход имеет фундаментальные недостатки, о которых было рассказано ранее \ref{chap:coh}. В рамках этой задачи был использован другой подход.
Для каждого датасета были сгенерированы пары $(d_1, d_2)$ (где $d_i$ --- диалог), которые впоследствии были размечены тремя экспертами. Для измерения качества модели полученные таким образом метки сравнивались с метками, полученными из модели.
Приведённый ниже список резюмирует описанный подход к оценке качества и регламент разметки, предложенный экспертам:
\begin{itemize}
    \item \texttt{0}: между $d_1$ и $d_2$ нет ничего общего. В терминах тематической модели такие диалоги должны относиться к разным темам первого уровня.
    \item \texttt{1}: Оба диалога связаны с одним и тем же объектом, но между ними имеются существенные различия. Такие диалоги должны попадать в одну и ту же тему первого уровня, но относиться к двум разным темам второго уровня.
    \item \texttt{2}: Оба диалога $d_1$ и $d_2$ имеют один и тот же интент. Такие диалоги должны относиться к одинаковым темам первого и второго уровня.
    \item \texttt{?}: Невозможно определить интент по крайней мере в одном из рассматриваемых диалогов.
\end{itemize}

Оценка качества моделей производится через измерения точности по этим размеченным парам. Таким образом, отбор моделей происходит по принципам, согласующимся с соображениями, изложенными в \ref{multilevel}.

Разметка разделена на три набора пар: ``1-small`` и ``2-small`` относятся к первой коллекции и состоят из $\sim\!12K$ и $\sim1.5K$ объектов соответственно, а набор ``2-small'' относится ко второй коллекции и состоит из $\sim\!1.5K$ пар. Гиперпараметры модели подбираются исходя из точности, измеренной на  наборе ``1-big``. Два оставшихся набора (``1-small`` и ``2-small``) используются для контроля переобучения. Стоит отметить, что хорошее качество, продемонстрированное на датасете 2-small свидетельствует о том, что стратегия построения модели обобщается на коллекции, отличной от изначальной.



\subsection{Базовые модели}

\par Простейший способ построения модели кластеризации на коллекции текстовых документов состоит из двух этапов. Сначала каждый документ каким-либо образом отображается в вектор действительных чисел. Затем к полученным векторам применяется один из стандартных алгоритмов кластеризации.


\par Существует много способов построить по документу его векторное представление. Простейший из них --- просто кодировка каждого документа вектором длины $W$ (полученный вектор будет равен сумме прямо закодированных (one-hot encoding) векторов для всех составляющих документ слов). Несложная, но действенная его модификация --- замена счётчиков встречаемостей на tf-idf.

Логистическая регрессия, работающая с tf-idf представлениями, показывает достаточно хорошее качество в задачах классификации. Этот алгоритм --- достойная точка отсчёта даже в контексте глубинного обучения \cite{park2019adc}. При этом часто tf-idf представление оказывается неприменимым для задачи кластеризации в силу проклятья размерности. Методы снижения размерности могут помочь решить эту проблему; к их числу относятся PCA и Uniform Manifold Approximation and Projection (UMAP, \cite{mcinnes2018umap}).

\par Другой распространённый подход основывается на векторных представлениях слов \cite{embeddings_in_tm}. Сначала каждому слову ставится в соответствие его векторное представление; представление документа получается через усреднение представлений отдельных слов.

Наиболее распространённые представления слов относятся к семейству word2vec \cite{word2vec}: CBOW, Skip-gram и их модификации (\cite{mikolov2013efficient}).

Подобные модели необходимо обучать на большой коллекции документов, такой как «Википедия».

% в последнее время набирают популярность BERT?

При использовании векторных представлений слов для кластеризации часто применяется снижение размерности \cite{park2019adc} (как и в предыдущем подходе).

Векторное представление документа может быть получено несколькими способами, использующими различные схемы усреднения представлений слов. Помимо арифметического усреднения, в котором вклад каждого слова одинаков, используется усреднение с idf-весами, благодаря которым редкие слова оказывают большее влияние, чем частые.

Использование этих и подобных техник вкупе с тонкой настройкой их параметров позволяет повысить качество кластеризации.

Следующая процедура была использована в качестве первой базовой модели. Сначала тексты были преобразованы в вектора действительных чисел при помощи предобученных представлений либо значений tf-idf. Затем данный набор векторов был прокластеризован при помощи алгоритма K-средних (K-Means). Наконец, каждый полученный кластер разбивается на несколько более мелких (каждый кластер при этом считается отдельной коллекцией). В результате получаются кластера как первого, так и второго уровня.

В роли второй базовой модели выступали иерархические тематические модели: иерархическая тематическая модель без дополнительной регуляризации и иерархическая тематическая модель со сглаживанием распределений  $\Phi$ и $\Theta$ на обоих уровнях.

Количество тем/кластеров на обоих уровнях для базовых моделей настраивались для получения наилучшего качества. Для подхода, основанного на K-Means, дополнительно настраивалась размерность представлений. Как демонстрирует таблица  \ref{baselines}, на двух из трёх размеченных наборах пар регуляризованная тематическая модель превосходит основанный на K-Means подход.

\begin{table}[!h]
    \centering
\begin{tabular}{p{2.7cm}|c|c|c}
    \hline
    & 1-big           & 1-small            & 2-small            \\ \hline
    hKmeans (tf-idf)      & 0.568          & 0.593          & \textbf{0.649} \\
    hKmeans (emb.) & 0.615          & 0.638          & 0.641          \\
    hPLSA         & 0.603          & 0.675          & 0.633          \\
    hARTM         & \textbf{0.636} & \textbf{0.683} & 0.631          \\  \hline
\end{tabular}
    \caption{Baselines accuracy}
    \label{baselines}
\end{table}


\subsection{Качество предложенной модели}

Построение модели происходит последовательно, будут настраиваться как параметры модели, так и этапы конвейера предобработки (описанные в \ref{nlp_methods}). Поиск происходит в следующем порядке:

\begin{enumerate}
    \item Добавление модальности $n-$грам, настройка алгоритма их извлечения, подбор весов модальностей и числа тем;
    \item Добавление регуляризатора сглаживания $p_{tdw}$ на первом уровне иерархической модели, подбор коэффициента регуляризации и числа тем; 
    \item Замена имён на теги, выбор между словарными и нейросетевыми подходами к определению именованных сущностей; 
    \item Исправление опечаток, выбор между стандартным и модифицированным алгоритмом исправления опечаток.  
\end{enumerate}

Отправная точка поиска --- модель hPLSA.

\begin{table}[!h] 
    \centering 
\begin{tabular}{p{2.7cm}|c|c|c} 
    \hline 
    & 1-big            & 1-small            & 2-small            \\ \hline 
    hPLSA         & 0.603          & 0.675          & 0.633          \\ 
\hline \hline 
    + n-grams стандарт & 0.612 & 0.634 & 0.633 \\ 
    + n-grams модификация & \textbf{0.635} & \textbf{0.674} & \textbf{0.655} \\ 
\hline \hline 
    + сглаживание ptdw & \textbf{0.64} & \textbf{0.678} & \textbf{0.66} \\ 
\hline \hline 
    + NER по словарю & 0.634         & 0.661         & 0.635 \\ 
    + NER через нейросети & \textbf{0.64} & \textbf{0.68} & \textbf{0.662} \\ 
\hline \hline 
    + Jamspell  & {0.635} & {0.674} &  {0.655} \\ 
    + модификация Jamspell & \textbf{0.657} & \textbf{0.686} & \textbf{0.663} \\  \hline 
\end{tabular} 
    \caption{Изменение качества в результате последовательных улучшений} 
    \label{nlp_techniques} 
\end{table}

Как видно из таблицы \ref{nlp_techniques}, в этой задаче модифицированный метод извлечения n-грам даёт выигрыш в качестве по сравнению с традиционным алгоритмом TopMine. Поиск и замена именованных сущностей не приводит к заметному улучшению качества (что объясняется относительной их редкостью: из верхних токенов hPLSA лишь $3\%$ являются именами. Процедура замены имён на теги понижает это соотношение до $0.3\%$). Исправление опечаток улучшает качество на всех трёх проверочных выборках (следует отметить, что модификация алгоритма Jamspell обоснованна, поскольку стандартная его реализация приводит к снижению качества).

Наконец, мы применяем группировку токенов по функциональному признаку (как было предложено в \ref{hierarchy_distinct}). Этот приём ведёт к значительному улучшению качества на всех трёх проверочных выборках. Заметим, что механизм относительных весов модальностей повысил интерпретируемость численных значений и позволил успешно перенести веса модальностей, подобранные на первой коллекции, на вторую коллекцию.

\begin{table}[!h] 
    \centering 
\begin{tabular}{p{3.7cm}|c|c|c}
    \hline
    & 1-big            & 1-small            & 2-small            \\ \hline
    hARTM с модальностями         & 0.657          & 0.686          & 0.663          \\
    + веса модальностей & \textbf{0.667} & \textbf{0.715} & \textbf{0.672} \\ \hline
\end{tabular}
    \caption{Увеличение точности классификации в результате функциональной группировки признаков}
    \label{results}
\end{table}

Ниже приведены примеры найденных моделью тем. В таблице \ref{topic_subtopic} представлены все подтемы темы ``Тарифный план''; каждая тема описана своим характерным вопросом. В таблице \ref{subtopic_documents} приведены верхние документы, относящиесся к подтеме ``Отключение антивируса и родительского контроля''.

\begin{table}[!h]
\begin{tabular}{p{7cm}}
  \hline
  \textbf{Тарифный план} \\
  \hline
  \textsl{Как сменить тарифный план?} \\
  \textsl{Когда произошло изменение тарифного плана?} \\
  \textsl{Как часто можно менять тарифный план?} \\
  \textsl{Когда вступят в силу изменения тарифного плана?} \\
  \textsl{Почему у меня не получается изменить тарифный план?} \\
  \textsl{Почему тарифный план изменился без моего ведома?} \\
  \textsl{Почему нет доступных тарифных планов для перехода?} \\
  \hline
\end{tabular}
\caption{Подтемы темы ``Тарифный план''}
\label{topic_subtopic}
\end{table}

\begin{table}[!h]
\begin{tabular}{p{7cm}}
  \hline
  \textbf{Телефон и домашний интернет} \\
  \hline
  \textsl{Подскажите пожалуйста как можно приостановить (\&quot; заморозить \&quot;) услуги Домашний телефон Домашний интернет на время отпуска?} \\
  \textsl{как можно отключить телефон домашний и интернет полностью выключить все?
я не хочу блокировать я хочу полностью отелючить домашний телефон и интернет} \\
  \textsl{Как временно отключить интернет и телефон на время отпуска} \\
  \textsl{Как можно отключить услугу домашний телефон с сохранением интернета? домашним телефоном не пользуемся} \\
  \textsl{Здравствуйте заблокируйте услуги Домашний интернет и домашний телефон с 25.10.18 по 27.11.18 спасибо.} \\
  \textsl{Добрый день !!!! Подскажите хочу временно приостановить обслуживание домашнего телефона но интернет хочу оставить !!! Возможно такое ??  Ну а если вообще отказаться от телефона интернет останется при этом ??} \\
  \textsl{у меня подключен домашний телефон и домашний интернет если я отключу домашний телефон будет ли рабоать домашний интернет?} \\
  \hline
\end{tabular}

\caption{Верхние документы темы первого уровня <<Телефон и домашний интернет>>}
\label{topic_documents}
\end{table}

\begin{table}[!h]
\begin{tabular}{p{7cm}}
  \hline
  \textbf{Отключить родительский контроль и антивирус} \\
  \hline
  \textsl{я бы хотел отключить услуги родительский контроль и антивирус} \\
  \textsl{хочу отключить услугу родительский контроль и пнтивирус
18100XXXXXXXX Иванов Иван Иванович} \\
  \textsl{Здравствуйте девушка а можно отключить услугу родительский контроль.} \\
  \textsl{Добрый день. Я бы хотела отключить интернет.} \\
  \textsl{Здравствуйте я хотела бы отключить услуги Родительский контроль и Антивирус. Как это сделать?} \\
  \textsl{Здравствуйте я бы хотела отключить услугу интернет Касперский плюс} \\
  \textsl{Здравствуйте как отключить услугу Родительский контроль и Антивирус .} \\
  \hline
\end{tabular}
\caption{Верхние документы темы второго уровня <<Отключить родительский контроль и антивирус>>, родителем которой является тема <<Домашний телефон и интернет>>}
\label{subtopic_documents}
\end{table}

\section{Conclusion}

В этой главе описана возможная формализации задачи о кластеризации неразмеченной выборки пользовательских обращений в колл-центр.

Вооружившись осознанием того, что любой интент состоит из двух частей --- объекта, имеющего отношение к пользовательскому запросу, и действия, которое пользователь хочет предпринять, --- мы выбрали двухуровневую мультимодальную тематическую модель в качестве инструмента для решения этой задачи. Была разработана пользовательская метрика качества, которая учитывает степень схожести диалогов.

Качество кластеризации было существенно улучшено при помощи $n$-грам, выделения именованных сущностей, исправления опечаток и разделения токенов по функциональному признаку. В результате была построена иерархическая мультимодальная регуляризованная тематическая модель, превосходящая все базовые модели.

