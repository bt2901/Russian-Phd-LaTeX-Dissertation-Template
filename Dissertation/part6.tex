\chapter{Применение регуляризации для кластеризации интентов}



В этой главе предлагается иерархическая мультимодальная регуляризованная тематическая модель, играющая роль первого приближения к структуре коллекции. Главное достижение, описанное в этой главе --- построение двухуровневой иерархической тематической модели, использующей различные признаки на первом и втором уровнях. Насколько нам известно, ранее в литературе подобный подход не изучался.  Вводится специфичная для задачи мера качества, измеряющую качество иерархических связей между темами и обнаруженными интентами.

Иерархическая структура увеличивает интерпретируемость предлагаемой кластеризации. Точнее говоря, от тем первого уровня требуется, чтобы они описывали субъект диалога, а темы второго уровня должны описывать действие, в котором заинтересован пользователь. Информация о частях речи слов используется для того, чтобы достичь эту цель.


\section{Подходы к кластеризации текста}
\subsection{Подходы, основанные на векторных представлениях} \label{embeddings}

\par Простейший способ построения модели кластеризации на коллекции текстовых документов состоит из двух этапов. Сначала каждый документ каким-то образом отображается в вектор действительных чисел. Затем к полученным векторам применяется один из стандартных алгоритмов кластеризации.

\par Существует много способов построить по документу его векторное представление. Простейший из них --- просто кодировка каждого документа вектором длины $W$ (полученный вектор будет равен сумме прямо закодированных (one-hot encoding) векторов для всех составляющих документ слов). Несложная, но действенная его модификация --- замена счётчиков встречемостей на tf-idf.

Логистическая регрессия, работающая с tf-idf представлениями показывает достаточно хорошее качество в задачах классификации. Этот алгоритм --- достойная точка отсчёта даже в контексте глубинного обучения \cite{park2019adc}. При этом, зачастую tf-idf представление оказывается непременимым для задачи кластеризации в силу проклятья размерности. Методы снижения размерности могут помочь решить эту проблему; к их числу относятся PCA и Uniform Manifold Approximation and Projection (UMAP, \cite{mcinnes2018umap}).

\par Другой распространённый подход основывается на векторных представлениях слов \cite{embeddings_in_tm}. Сначала каждому слову ставится в соответствие его векторное представление; представление документа получается через усреднение представлений отдельных слов.

Наиболее распроненённые представления слов относятся к семейству word2vec \cite{word2vec}: CBOW, Skip-gram и их модификации (\cite{mikolov2013efficient}). 
Подобные модели необходимо обучать на большой коллекции документов, такой как Википедия.
% в последнее время набирают популярность BERT?

При использовании векторных представлений слов для кластеризации часто применяется снижение размерности \cite{park2019adc} (как и в предыдущем подходе).

Векторное представление документа может быть получено несколькими способами, использующими различные схемы усреднения представлений слов. Помимо арифметического усреднения, в котором вклад каждого слова одинаков, используется усреднение с idf-весами, благодаря которым редкие слова оказывают большее влияение, чем частые.

Использование этих и подобных техник вкупе с тонкой настройкой их параметров позволяет повысить качество кластеризации.


\section{Multilevel clustering} \label{multilevel}     

\par Темы тематической модели, которую мы стремимся построить, должны соответствовать пользовательским интентам. В этой работе используется следующее операциональное определение интента:
два диалога (представленные репликами пользователя) считаются \textit{имеющими одинаковый интент}, если оба пользователя были бы удовлетворены практически одинаковой реакцией оператора колл-центра. Это определение, несмотря на свои недостатки, позволяет подчеркнуть ряд важных практических проблем:

\begin{itemize}
\item Простого подхода, основанного на гипотезе ``мешка слов'', недостаточно. Сравните: ``Я хочу заблокировать свою учётную запись, что мне делать?`` and ``Моя учётная запись заблокирована, что мне делать?``.

\item В каких-то случаях замена одного слова изменяет интент реплики, а в каких-то нет. В контексте имеющейся коллекции ``я хочу записаться на приём к кардиологу`` и ``я хочу записаться на приём к неврологу`` считаются одинаковыми интентами, поскольку требующиесся от пользователя/оператора действия практически не отличаются. Однако, ``уплата госпошлины за паспорт`` и ``уплата госпошлины за автомобиль`` существенно различаются.

\end{itemize}

\todo{no PTDW smoothing here, actually}

В данной работе применяется несколько приёмов, частично решающих проблему ``мешка слов'': используется дополнительная модальность нграм и специальный регуляризатор сглаживания $p_{tdw}$  \cite{ptdw}.
The $p_{tdw}$ smoothing regularizer respects the sequential nature of text, making the distributions $p(t|d, w)$ more stable for $w$ belonging to a same local segment. In a way, $p(t|d, w)$ distribution could be interpreted as the analogue for context embeddings in topic modeling world. $p(t|d, w)$ distribution isn't used directly for topic representation, but it is used on the E-step of EM-algorithm for $\phi_{wt}$ and $\theta_{td}$ recalculation.

Для того, чтобы получить больше контроля над выделением итентов, предлагается двухуровневая иерархическая тематическая модель. Первый уровень предназначен для грубого определения похожести, в то время как второй уровень учитывает менее очевидные, но важные, признаки. 

Иерарическая АРТМ модель представлят собой набор различных АРТМ моделей для каждого уровня, связанных друг с другом. Первый уровень иерарии может быть любой тематической моделью. Второй уровень строится с использованием специального регуляризатора из работы \cite{chirkova2016additive}, который стремится сделать модель такой, чтобы темы первого уровня выражались через взвешенную сумму тем второго уровня. 

Для обеспечения того, чтобы каждая родительсая тема была связана лишь с небольшим числом дочерних тем, можно применить различные методы. Обычно используется регуляризатор, разреживающий междууровневые связи \cite{chirkova2016additive} ; также в литературе была предложена процеура прямого удаления ``плохих'' рёбер (в соответствии с мерой качества bedSim \cite{belyy}).

\subsection{Distinct hierarchy levels} \label{hierarchy_ distinct}

Для того, чтобы построенные моделью темы второго уровня существенно отличались от тем первого уровня, мы использовали различные алгоритмы кластеризации: грубо говоря, мы основываем модель второго уровня на другом множестве признаков.

В контексте поставленной задачи было предпринято разделение признаков по их функциональному назначению. Из всех токенов (слов и нграм) были выделены две группы на основании их частеречного состава: ``тематическая'' и ``функциональная''. ``Функциональная'' группа состоит из одиночных глаголов и нграм, содержащих хотя бы один глагол. ``Тематическая'' группа состоит из одилночных сущетвительных, одилночных прилагательных и нграмм,включающих в себя хотя бы одно существительное и не имеющих в своём составе глаголов.

Таким образом, предлагаемая тематическая модель имеет пять модальностей: \texttt{@lemmatized} (просто слова), \texttt{@verb\_lemmatized} (слова-глаголы), \texttt{@noun\_lemmatized} (слова-существительные и слова-прилагательные), \texttt{@theme\_ngramms} (нграмы с существительными и без глаголов), \texttt{@verb\_ngramms} (нграмы с глаголами).

% Inspired by multi--level \cite{Tang2018SubgoalDF} and multi--syntactic \cite{Gupta2018SemanticPF} phrases annotation, among with hierarchical partition, our approach is essential for client goal and subgoals extraction.

\par Первый уровень иерархии предназначен для определения субъекта диалога (то есть сущностей, о которых идёт речь). По этим соображениям, вес тематической группы токенов должен быть существенно выше, чем вес функциональной группы. Цель второго уровня иерархии --- нахождение интента клиента, касающегосся определённых сущностей (то есть действий, которые пытается предпринять клиент). На этом уровне функциональные токены должны иметь большее влияние, нежели токены тематической группы. Токены модальностей, напрямую не относящихся к данным двум группам, используются на всех уровнях; они выступают в роли связи между двумя разными слоями иерархии.

\section{Предобработка} \label{nlp_methods}

Для двух описанных далее коллекций использовалась одна и та же процедура предобработки. Она состоит из большого числа частей (поэтому важно, чтобы каждая из них была сравнительно быстрой), описанных ниже.

\textbf{Первичная обработка данных}: очистка от ненужной информации. В её состав входит:
\begin{itemize}
    \item  \textbf{Фильтрация документов}. Удаление длинных документов, часто встречающихся в качестве сообщений (например, автоматические сообщения — ``Создано новое обращение \#123''). 
    \item \textbf{Выделение/удаление простых сущностей}. Выделение URL-адресов, html-тегов, адресов электронной почты. Каждая сущность заменяется на свой тег, например все гиперссылки заменяются на <URL>. Для каждой из сущностей задан набор регулярных выражений.
\item \textbf{Выделение/удаление географических локаций}. Выделение городов и регионов. Выделение происходит с помощью поиска по некоторому списку географических локаций.
\item \textbf{Посимвольная нормализация}. Удаление повторяющихся знаков пунктуации. Замена нескольких повторяющихся знаков пунктуации на один знак. Замена ё на е. Удаление повторяющихся пробелов.
\item \textbf{Исправление раскладки}. Если в тексте содержится несколько слов не из словаря, которые при смене раскладки будут оказываться в словаре, раскладка всего текста будет изменена.
\item \textbf{Исправление опечаток}. Опечатки исправляются двухэтапно. Для каждого слова в предложении генерируются кандидаты на исправление, затем из всех кандидатов выбирается лучший с помощью языковой модели. Теоретически алгоритм способен исправлять ошибки: пропущено две буквы, добавлено две лишних буквы, два раза две буквы поменяны местами, склеивание двух соседних слов в одно. Далее процесс испралвения опечаток будет рассмотрен более подробно.
\end{itemize}

Разделение данных на слова:
Нормализация данных:

\begin{itemize}

\item \textbf{Токенизация}. Токенизация документов (разделение на отдельные токены) с помощью библиотеки spacy. Сначала документ разделяется на токены по любым символам, которые не являются буквой или цифрой. Затем, согласно специальным шаблонам, некоторые группы токенов соединяются вместе.
\item \textbf{Выделение/удаление имён/фамилий}. Выделение имён и фамилий. Производится с помощью библиотеки pymorphy2.
\item \textbf{Лемматизация}. Лемматизация документов (приведение всех слов в начальную форму) с помощью библиотеки pymorphy2. 
\item \textbf{Учёт частицы не}. Склеивание не с следующим слово, если слово является глаголом (таким образом, тематическая модель может отличить ``получается'' от ``не получается'').
\item \textbf{Выделение словосочетаний/коллокаций/информативных нграм}. Выделение коллокаций (устойчивых словосочетаний) с помощью алгоритма TopMine. Далее этот этап будет рассмотрен более подробно.
\item \textbf{Удаление стоп-слов}. Удаление стоп-слов по заранее заданному списку слов и списку регулярных выражений.
\item \textbf{Разделение данных на группы}. Разделение слов и словосочетаний на две группы: “тематичные/предметные” и “относящиеся к действиям”
\end{itemize}

\subsection{Объединение и фильтрация реплик} 

Дополнительно происходит фильтрация некоторых реплик, определённых как “мусорные”. Обычно это дословный copy-paste текста какого-то нормативного документа или содержимого веб-страницы (в том числе и предыдущего диалога). 

Обучение кластеризации на таких объектах снижает итоговое качество модели. Кроме того, становится неудобно изучать выдачу топ-документов тем. В системе используется три критерия “мусорности” реплики (реплика удаляется, если справедлив хотя бы одно из них).

\todo{перефразировать три абзаца ниже, антиплагиат}

Многострочность: реплика достаточно большая (состоит больше чем из 6 строк или 10 слов), и число строк хотя бы в полтора раза больше, чем количество знаков препинания. Это правило выделяет copy-paste содержимого веб-страницы (тогда перенос строки означает не конец абзаца, а конец структурного элемента страницы; соответственно, отсутствует завершающий предложение знак препинания).

Пропорция одинаковых слов: реплика состоит больше чем из 10 слов, и её длина в словах более чем в 2.5 раза превосходит количество уникальных слов. Это правило выделяет реплики, содержащие многократное повторение одной и той же фразы с небольшими изменениями.

Скорость убывания частоты слов: сравниваются частоты трёх самых распространённых слов внутри реплики (не учитывая при этом предлоги и другие распространённые слова). Это правило выделяет реплики, содержащие многократное повторение одной и той же фразы или непохожие на естественную речь (частотность которой обычно подчиняются закону Ципфа или похожим степенным распределениям). Например, текст отчёта об ошибке (error traceback). Реплики, состоящие из одной строки, обрабатываются не так, как мноострочные реплики.

После удаления мусорных реплик происходит объединение всех сообщений диалога в один документ. Реплики оператора не учитываются в этом объединении (they are not informative in our datasets; for example, there are many cases where operator fails to reply at all). Таким образом, один документ коллекции --- это конкатенация всех реплик пользователя из одного диалога.


\subsection{N-grams extracting}

\par Распространённая техника, улучшающая интерпретируемость тематических моделей --- использование информативных нграм (или коллокаций) в качестве дополнительной модальности. Для того, чтобы извлечь информативные нграмы, был использован алгоритм TopMine \cite{topmine}, основанный на статистике совстречаемостей слов.

% тут можно расширить
% посмотреть диплом артёма

Для нашей задачи имело смысл внести ряд правок в алгоритм TopMine. 

Во-первых, исходный алгоритм вычисляет статистики совстречаемости для упорядоченных кортежей слов $(w_1, w_2, \dots, w_k)$. В частности, алгоритм различает последовательности $(w_1, w_2)$ и $(w_2, w_1)$. Эта особенность делает алгоритм TopMine плохо приспособленными для обработки текстов на языках с нестрогим порядком слов (таких как русский). По этой причине логика подсчёта статистик совстречаемостей алгоритма была модифицирована таким образом, чтобы в ней использовались мультимножества слов вместо последовательностей слов.

Во-вторых, TopMine не выделяет пересекающиеся коллокации. Это приводит к тому, что похожие предложения (``записать ребёнка в детский сад'' и ``записать ребёнка в детский садик'') могут вообще не содержать общих коллокаций. Примером такого служат предложение «получение паспорта РФ» (выделится коллокация \texttt{получение\_паспорт\_РФ}) и предложение «паспорт РФ был утерян» (выделится коллокация \texttt{паспорт\_РФ}). Это следует из процесса поиска коллокаций алгоритмом: на каждом шаге обработки соседние коллокации-кандидаты сливаются, если их объединение удовлетворяет критерию информативности. Для того, чтобы устранить вышеописанную проблему, достаточно измеить процесс итеративного слияния фраз так, чтобы при успешном слиянии коллокаций они не удалялись из множества кандидатов. Эта модификация увеличивает расход памяти алгоритма, однако делает процесс поиска не ``жадным''.

\subsection{Named entity recognition}

\par В текстах диалогов содержится большое число упоминаний имён участников, названий компаний/продуктов и названий улиц/городов. Аккуратный учёт такого рода сущностей даст прирост качества моделей, а также сделает списки верхних слов более интерпретируемыми (к информативным словам не будут примешиваться имена операторов-собеседников).
Поэтому в рамках предобработки коллекции все обнаруженные именованные сущности заменялись на тэги $\langle \mathrm{PERSON} \rangle$.

Рассматривалось несколько подходов к выделению именованных сущностей: с помощью правил/словарей и с помощью использования RNN+CRF модели. 
Нейронная сеть из работы \cite{burtsev}, предобученная на датасете PERSONS-1000 (\cite{persona}), давала лучшее качество выделения именнованных сущностей. Однако, соображениями вычислительной эффективности был продиктован выбор модуля \texttt{pymorphy2} (основанного на правилах) для окончательной версии конвеера предобработки.

\subsection{Коррекция ошибок}

\par Ошибки и опечатки --- частое явление в дилоговых корпусах. Для того, чтобы сделать текстовую коллекцию более подходящей для тематического моделирования, необходимо использовать алгоритм коррекции ошибок. В данной работе используется Jamspell\footnote{\href{https://github.com/bakwc/JamSpell}{Jamspell github}} по причине высокой скорости его работы.

\par Был проведён ряд модификаций для того, чтобы адаптировать модель Jamspell к нашему случаю. Во-первых, языковая модель, согласно которой выбирается лучший кандидат для исправления, была дообучена на кластеризуемой коллекции. Эта модификация нужна для учёта специфики коллекции; в противном случае специфичные для коллекции слова (такие как ``гибдд'' или ``адсл'') считались бы незнакомыми и в них исправлялись бы несуществующие опечатки.

Во-вторых, было расширено множество кандидатов для исправления. Согласно статистике поиска Яндекса\footnote{Сложности русского языка: \texttt{https://yandex.ru/company/researches/2016/ya\_spelling}}, одной из наиболее распространённых категорий ошибок является слитно-раздельное написание слов; в текстах коллекции также часто встречаются ``слипшиесся'' слова (``очередьк врачу''). Поэтому все возможные разбиения слова на два также рассматривались как кандидаты на исправление.

\section{Построение модели}

Для кластеризации полученных объектов строится тематическая модель. 

Модель основана на подходе аддитивной регуляризации. Кластер в этой модели называется темой. Каждая тема представляется распределением по токенам (словам, словосочетаниям и другим сущностям, на которых строится модель) и распределением по документам. Распределения строятся в процессе обучения модели. Распределение темы по документам можно трактовать как мягкую кластеризацию — для каждой темы известны документы, которые к ней относятся.  

Сначала строится первый уровень тематической модели, затем строится второй, так чтобы любая тема первого уровня являлась линейной комбинацией тем второго уровня. Для каждого из уровней используется свой набор коэффициентов важности категорий признаков (модальностей). 

Первый уровень соответствует разделению документов на “темы”, поэтому для построения уровня больший вес имеют “тематичные” признаки: слова-существительный и слова-прилагательные, словосочетания, не содержащие глаголов. 

Второй уровень соответствует разделению документов на “интенты”, поэтому для построения второго уровня больший вес по сравнению с первым уровнем имеют признаки: слова глаголы, словосочетания, содержащие глаголы.

На обоих уровнях для построения кластеризации с небольшим весом используются словосочетания из реплик операторов. Для того, чтобы модель была устойчива к разным коллекциям, используются не абсолютные веса для задания значимости категории признаков,  а относительные (вес — число от 0 до 1, равный вес у разных категорий означает одинаковую значимость этих категорий для модели).

На каждом уровне модели используется некоторое количество “фоновых” тем. Эти темы являются техническими, их цель — собирать внутри себя слова фоновой лексики, не влияющие на кластеризацию. К этим темам нельзя получить доступ через систему запросов. 

На каждом уровне модели используется свой набор регуляризаторов с своими коэффициентами регуляризации. Все коэффициенты за исключением коэффициента регуляризатора декоррелирования (отвечающего за повышение различности разных тем друг от друга) настраиваются автоматически. 

Выход иерархической модели — распределение тем первого и второго уровня по словам и документам, а также матрица зависимостей тем второго уровня от тем первого уровня. Выход плоской тематической модели — распределение тем по словам и документам. Для каждого кластера известны репрезентативные документы и токены. Для каждого документа можно получить наиболее вероятный кластер.

\subsection{Обработка результатов моделирования}
Выход иерархической модели не обязан по построению иметь древовидную структуру (хотя и стремится к ней). Чтобы выход иерархии был древовидным, используются дополнительные преобразования итоговой структуры: для каждой темы второго уровня выбирается единственная тема первого уровня. 

Каждую полученную тему можно представить наборами репрезентативных документов и репрезентативных токенов. Над набором документов проводится дополнительная фильтрация: удаляются документы в которых почти не встречается самых репрезентативных нграмм, удаляются тексты с непропорционально большим количеством заглавных букв.


\section{Experiments}

В экспериментах были использованы две коллекции диалогов из русских колл-центров (примерно по 90 тысяч диалогов в каждой, средняя длина диалога --- шесть реплик). Первый корпус состоит из диалогов клиентов с представителями различными государственных организаций. Второй корпус представляет собой логи технической поддержки провайдера. Оба датасета непубличны.

\subsection{Оценка качества}
Существует ряд подходов для оценки качества тематических моделей, в частности их интерпретируемости. Традиционная процедура включает в себя анализ списка верхних слов каждой из тем людьми. Однако, данный подход имеет фундаментальные недостатки, о которых было рассказано ранее \ref{sec:intracoh}. В рамках этой задачи был использован другой подход.

Для каждого датасета были сгенерированы пары $(d_1, d_2)$ (где $d_i$ --- диалог), которые впоследствии были размечены тремя экспертами. Для измерения качества модели, полученные таким образом метки сравнивались с метками, полученными из модели.

Приведённый ниже список резюмирует описанный подход к оценке качества и регламент разметки, предложенный экспертам:

\begin{itemize}
    \item \texttt{0}: между $d_1$ и $d_2$ нет ничего общего. В терминах тематической модели, такие диалоги должны относиться к разным темам первого уровня.
    \item \texttt{1}: Оба диалога связаны с одним и тем же объектом, но между ними имеются существенные различия. Такие диалоги должны попадать в одну и ту же тему первого уровня, но относиться к двум разным темам второго уровня.
    \item \texttt{2}: Оба диалога $d_1$ и $d_2$ имеют один и тот же интент. Такие диалоги должны относиться к одинаковым темам первого и второго уровня.
    \item \texttt{?}: Невозможно определить интент по крайней мере в однм из рассматриваемых диалогов.
\end{itemize}

Оценка качества моделей производится через измерения точности по этим размеченным парам. Таким образом, отбор моделей происходит по принципам, согласующимисся с соображениями, изложенными в \ref{multilevel}. 
Разметка разделена на три набора пар: ``1-small`` и ``2-small`` относятся к первой коллекции и состоят из $\sim\!12K$ и $\sim1.5K$ объектов соответственно, а набор ``2-small'' относится ко второй коллекции и состоит из $\sim\!1.5K$ пар. Гиперпараметры модели подбираются исходя из точности, измеренной на  наборе ``1-big``. Два оставшихся набора (``1-small`` и ``2-small``) используются для контроля переобучения. Стоит заметить, что хорошее качество, продемонстрированное на датасете 2-small свидетельствует о том, что стратегия построения модели обобщается на коллекции, отличные от изначальной.



\subsection{Бэзлайны}
Первая базовая модель, использованная для сравнения, 

Следующая процедура была использована в качестве первой базовой модели. Сначала тексты были преобразованы в вектора действительных чисел при помощи предобученных представлений либо значений tf-idf (как было описано в \ref{embeddings}). Затем данный набор векторов был прокластеризован при помощи алгоритма K-средних (K-Means). Наконец, каждый полученный кластер разбивается на несколько более мелких (каждый кластер при этом считается отдельной коллекцией). В результате получаются кластера как первого так и второго уровня.

В роли второй базовой модели выступали иерархические тематические модели: иерархическая тематическая модель без дополнительной регуляризации и иерархическая тематическая модель со сглаживанием распределений  $\Phi$ и $\Theta$ на обоих уровнях.

Количество тем/кластеров на обоих уровнях для базовых моделей настраивались для получения наилучшего качества. Для подхода, основанного на K-Means, дополнительно настраивалась размерность представлений. Как демонстрирует таблица  \ref{baselines}, на двух из трёх размеченных наборах пар регуляризованная тематическая модель превосходит основанный на K-Means подход.

\begin{table}[!h]
    \centering
\begin{tabular}{p{2.7cm}|c|c|c}
    \hline
    & 1-big           & 1-small            & 2-small            \\ \hline
    hKmeans (tf-idf)      & 0.568          & 0.593          & \textbf{0.649} \\
    hKmeans (emb.) & 0.615          & 0.638          & 0.641          \\
    hPLSA         & 0.603          & 0.675          & 0.633          \\
    hARTM         & \textbf{0.636} & \textbf{0.683} & 0.631          \\  \hline 
\end{tabular}
    \caption{Baselines accuracy}
    \label{baselines}
\end{table}

\subsection{Качество предложенной модели}

\todo{Обсудить с КВ, насколько можно менять эксперименты}

We use several NLP-based techniques described in \ref{nlp_methods} to improve main model quality.
We start with the hPLSA model.
For each problem we test a few approaches and choose the best one. 
We add all main features one by one, e.g. we choose the best method for extracting n-grams and use it on the next step.
We conduct all the experiments in the following order:
\begin{enumerate}
    \item including additional n-gram modality, choosing between the based and modified n-grams extracting methods, tuning modality weights and topics number;
    \item adding $p_{tdw}$ smoothing at the first model level for all tokens, tuning regularizer coefficient and topics number;
    \item replacing person related named entities, choosing between the dictionary-based and rnn-based methods;
    \item typo correction, choosing between the base and modified algorithm
\end{enumerate}

\begin{table}[!h]
    \centering
\begin{tabular}{p{2.7cm}|c|c|c}
    \hline
    & 1-big            & 1-small            & 2-small            \\ \hline
    hPLSA         & 0.603          & 0.675          & 0.633          \\
\hline \hline
    + n-grams base & 0.612 & 0.634 & 0.633 \\
    + n-grams mod. & \textbf{0.635} & \textbf{0.674} & \textbf{0.655} \\
\hline \hline
    + ptdw smooth. & \textbf{0.64} & \textbf{0.678} & \textbf{0.66} \\
\hline \hline
    + NER dict. & 0.634         & 0.661         & 0.635 \\
    + NER NN & \textbf{0.64} & \textbf{0.68} & \textbf{0.662} \\
\hline \hline
    + Jamspell  & {0.635} & {0.674} &  {0.655} \\  
    + mod. Jamspell & \textbf{0.657} & \textbf{0.686} & \textbf{0.663} \\  \hline 
\end{tabular}
    \caption{NLP techniques quality improvement}
    \label{nlp_techniques}
\end{table}

As the table \ref{nlp_techniques} demonstrates our n-grams extraction method outperforms traditional TopMine algorithm in this task. Replacing persons by a tag does not lead to a great improvement of the quality. Our analysis of hPLSA cluster top-tokens shows that only $3\%$ of the top-tokens are related to persons. After the NER preprocessing the proportion of named entities in top tokens reduces to $0.3\%$. And at the same time spellchecking improves the performance on all three pair sets. It should be noted that standard Jamspell algorithm leads to a quality decrease. 

Finally, we apply feature grouping schemes proposed in \ref{hierarchy_ distinct}. The results (table \ref{results}) turned out to be reassuring. There is a noticeable performance boost for all of the pair sets. 

\begin{table}[!h]
    \centering
\begin{tabular}{p{2.7cm}|c|c|c}
    \hline
    & 1-big            & 1-small            & 2-small            \\ \hline
    featured hARTM         & 0.657          & 0.686          & 0.663          \\
    + groups & \textbf{0.667} & \textbf{0.715} & \textbf{0.672} \\ \hline
\end{tabular}
    \caption{Grouping feature quality improvement}
    \label{results}
\end{table}


Further, we represent some examples of the model performance. All example texts from examples were translated from Russian to English. In the table \ref{topic_subtopic} all subtopics of the topic ``Tariff plan'' are presented. Each subtopic described by the characteristic question. 

In the table \ref{subtopic_documents} we demonstrate top documents corresponding to the ``How do I switch from credit to advance payment?'' subtopic.

\begin{table}[!h]
\begin{tabular}{p{7cm}}
  \hline
  \textbf{Tariff plan} \\
  \hline
  \textsl{How to change the tariff plan?} \\
  \textsl{When did the tariff change happen?} \\
  \textsl{How often can I change my tariff plan?} \\
  \textsl{When will the changes take effect when the tariff is changed?} \\
  \textsl{Why can't I change the tariff?} \\
  \textsl{Why was the tariff plan changed without my knowledge?} \\
  \textsl{Why there are no available tariff plans for the transition?} \\
  \hline
\end{tabular}
\caption{Subtopics of topic ``Tariff plan''}
\label{topic_subtopic}
\end{table}

\begin{table}[!h]
\begin{tabular}{p{7cm}}
  \hline
  \textbf{How do I switch from credit to advance payment?} \\
  \hline
  \textsl{How do I switch from credit to advance payment?} \\
  \textsl{Hi. Tell me can we change the credit system of payment to advance? Well thanks!} \\
  \textsl{I need to change my payment from credit to advance.} \\
  \textsl{How to disable credit payment system?} \\
  \textsl{Hello. Change the payment system from credit to advance!} \\
  \textsl{Good morning. How to change the payment system from credit to normal?} \\
  \textsl{Disable the credit payment system.} \\
  \hline
\end{tabular}
\caption{Top documents of subtopic ``How do I switch from credit to advance payment?''}
\label{subtopic_documents}
\end{table}

\section{Conclusion}
In this paper, we report a success in formalizing the clustering process suitable for unsupervised inference of user intents. 

The realization that any intent consists of two crucial parts: the entity relevant to the user's request and the action user wishes to perform helped us  to choose a two-level hierarchical model as our main tool. This leads us to design a custom quality metric which takes into account several degrees of the dialogues relevancy.

Our next step was to devise a PoS-based feature separation and to leverage n-grams, named entities and spellchecking. This allowed us to construct a hierarchical multimodal regularized topic model which outperforms all baseline models.

