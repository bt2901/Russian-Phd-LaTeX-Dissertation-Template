\pdfbookmark{Общая характеристика работы}{characteristic}             % Закладка pdf
\section*{Общая характеристика работы}

\newcommand{\actuality}{\pdfbookmark[1]{Актуальность}{actuality}\underline{\textbf{\actualityTXT}}}
\newcommand{\progress}{\pdfbookmark[1]{Разработанность темы}{progress}\underline{\textbf{\progressTXT}}}
\newcommand{\aim}{\pdfbookmark[1]{Цели}{aim}\underline{{\textbf\aimTXT}}}
\newcommand{\tasks}{\pdfbookmark[1]{Задачи}{tasks}\underline{\textbf{\tasksTXT}}}
\newcommand{\aimtasks}{\pdfbookmark[1]{Цели и задачи}{aimtasks}\aimtasksTXT}
\newcommand{\novelty}{\pdfbookmark[1]{Научная новизна}{novelty}\underline{\textbf{\noveltyTXT}}}
\newcommand{\influence}{\pdfbookmark[1]{Практическая значимость}{influence}\underline{\textbf{\influenceTXT}}}
\newcommand{\methods}{\pdfbookmark[1]{Методология и методы исследования}{methods}\underline{\textbf{\methodsTXT}}}
\newcommand{\defpositions}{\pdfbookmark[1]{Положения, выносимые на защиту}{defpositions}\underline{\textbf{\defpositionsTXT}}}
\newcommand{\reliability}{\pdfbookmark[1]{Достоверность}{reliability}\underline{\textbf{\reliabilityTXT}}}
\newcommand{\probation}{\pdfbookmark[1]{Апробация}{probation}\underline{\textbf{\probationTXT}}}
\newcommand{\contribution}{\pdfbookmark[1]{Личный вклад}{contribution}\underline{\textbf{\contributionTXT}}}
\newcommand{\publications}{\pdfbookmark[1]{Публикации}{publications}\underline{\textbf{\publicationsTXT}}}

\input{common/characteristic} % Характеристика работы по структуре во введении и в автореферате не отличается (ГОСТ Р 7.0.11, пункты 5.3.1 и 9.2.1), потому её загружаем из одного и того же внешнего файла, предварительно задав форму выделения некоторым параметрам

%Диссертационная работа была выполнена при поддержке грантов \dots

\underline{\textbf{Объем и структура работы.}} Диссертация состоит из введения, двух обзорных глав, четырёх глав с результатами проведенного исследования, заключения и библиографии. Полный объём диссертации составляет 143 страницы, включая 17 рисунков и 12 таблиц. Список литературы содержит 154 наименования.

\pdfbookmark{Содержание работы}{description}                          % Закладка pdf
\section*{Содержание работы}
Во \underline{\textbf{введении}} отражается актуальность
исследований, проводимых в~рамках данной диссертационной работы,
приводится обзор научной литературы по~изучаемой проблеме,
формулируется цель, ставятся задачи работы, определяется научная новизна, практическая и теоретическая значимости представляемой работы. Приводится список публикаций автора по теме диссертации и формулируются положения, вносимые на защиту.

\underline{\textbf{Первая глава}} посвящена постановке задачи тематического моделирования. Рассмотрены основные подходы к тематическому моделированию и подробно описан математический аппарат подхода ARTM.

Задача тематического моделирования заключается в нахождении матриц $\Phi$ (задающей распределение <<вероятность слова в теме>>) и $\Theta$ (задающей распределение <<вероятность темы в документе>>), которые хорошо описывают заданную коллекцию документов. Подход ARTM позволяет находить $\Phi$ и $\Theta$, учитывающие какие-либо дополнительные требования помимо соответствия коллекции.

В рамках ARTM тематические модели строятся при помощи \textit{EM-алгоритма}, в котором текущие значения матриц $\Phi$ и $\Theta$ итеративно обновляются по определённым формулам. Введение каждого дополнительного регуляризатора приводит к появлению в этих формулах новых слагаемых специального вида и не требует каких-либо сложных дополнительных выкладок.

Область применения, уникальная для тематических моделей --- описание коллекции, дающее общее представление о тематической кластерной структуре больших объёмов данных. Зачастую исследователь ищет ответы на вопросы о структуре и природе коллекции, а не пытается оптимизировать заданную меру качества.

Приложения тематического моделирования в различных областях наталкиваются на проблемы плохой интерпретируемости тем, дублирующих, мусорных и вводящих в заблуждение тем, неустойчивости результатов моделирования. 

Большую часть этих проблем можно разрешить при помощи настройки гиперпараметров, но большинство прикладных работ либо не занимаются настройкой гиперпараметров, либо производят её слабо специфицированным образом.

Сложность подбора гиперпараметров у многокритериальных моделей вызывает необходимость как измерения, так и улучшения качества тематических моделей одновременно по множеству критериев.

\underline{\textbf{Во второй главе}} рассматриваются критерии качества тематических моделей, используемые в литературе. Проведена категоризация распространённых подходов к измерению качества. Особое внимание уделяется мерам качества, связанными с анализом верхних (наиболее частотных, вероятных, <<топовых>>, top-10) токенов тем.

\underline{\textbf{Третья глава}} посвящена анализу общих недостатков этих мер качества. Большинство используемых в литературе подходов к оценке интерпретируемости укладываются в следующую схему:

\begin{enumerate}
    \item Для каждой темы выбирается какой-то небольшой набор характеризующих её токенов (как правило, это 10 верхних токенов).
    \item{Этот набор анализируется одним из двух способов:
    \begin{itemize}
        \item Проведя визуальный осмотр этих токенов, эксперт каким-либо образом оценивает качество темы
        \item Для каждого элемента этого набора собирается ряд статистик совстречаемостей, на основе которого вычисляются различные численные показатели
    \end{itemize}
    }
\end{enumerate}

В пункте (1) вышеописанной схемы тема целиком огрубляется до конечного числа слов. Каким бы образом не проводился бы дальнейший анализ, обоснование качества тематической модели
при помощи короткого списка верхних токенов представляется проблематичным.

Рисунок \ref{fig:ch3_doc_compound_auto} демонстрирует то, что верхние токены покрывают исчезающе малую часть коллекции, и ситуация ещё более усложняется наличием дополнительного требования совместной встречаемости токенов в окне. Численные расчёты подтверждают, что когерентность отдельно взятой темы, в большинстве случаев, учитывает менее тысячной доли всего корпуса текста.

\begin{figure}
    %\begin{tabular}{p{7.5cm}p{3.5cm}}
        \includegraphics[width=0.75\textwidth]{doc11358_topic0.png} %&
        % \includegraphics[width=0.25\textwidth]{legend.eps} \\
    %\end{tabular}
    \captionsetup{justification=raggedright,singlelinecheck=false,format=hang}
    \caption{Демонстрация доли текста, покрытой верхними словами, на примере одного документа. Словопозиции обозначены серо-синим цветом, словопозиции верхних слов показаны красным цветом, зелёным цветом показаны словопозиции, имеющие ненулевой вклад в расчёт когерентности (т.е. попадающие в скользящее окно вместе с другим верхним словом).}
\label{fig:ch3_doc_compound_auto}
\end{figure}

Также в этой главе предлагаются несколько мер качества, основанных на идее \textit{внутритекстовой когерентности}.
Традиционные меры когерентности сначала выделяют какое-то множество слов по их $\phi_{wt}$ и затем анализируют, каким образом эти слова встречаются в тексте (\emph{от темы к тексту}). В предлагаемом же методе сначала выделяются все соседние слова текста, распределение $\phi_{wt}$ которых затем анализируется (\emph{от текста к теме}).

Предполагается, что этот метод будет отражать коллекцию в большей степени, нежели традиционные меры когерентности. Эксперимент на полусинтетической коллекции показывает, что предлагаемый подход действительно отличается большей чувствительностью.

\underline{\textbf{В четвёртой главе}} рассматриваются способы увеличения интерпретируемости тематических моделей. Предложен метод подбора коэффициентов сглаживания, коэффициентов разреживания и весов дополнительных модальностей при помощи математического аппарата \textit{относительных коэффициентов}.

Эта техника облегчает построение тематических моделей, все темы которых различны и не <<испорчены>> большим числом неинформативных токенов (слов общей лексики). Также она позволяет перенести имеющуюсся стратегию обучения тематической модели на
другие текстовые коллекции схожей структуры.

Показано, что любому абсолютному коэффициенту сглаживания/разреживания $\Phi$ и $\Theta$ соответствует какой-то относительный коэффициент (и наоборот). Приводятся следующие формулы преобразований из $\lambda$ в $\tau$:

\begin{equation}
\tau = \frac{\lambda \sum_{d\in D} n_d }{(1-\lambda) |D| \cdot |T|} \label{sp_theta_rel2abs}
\end{equation}

\begin{equation}
\tau = \frac{n}{|T|\cdot|W|} \frac{\lambda}{(1-\lambda)}  \label{sp_phi_rel2abs}
\end{equation}

Регуляризатор сглаживания $\Theta$, воздействующий на темы $T$ и документы $D$, коэффициент $\tau$ которого вычислен по формуле \ref{sp_theta_rel2abs}, можно проинтерпретировать как нахождение <<компромисса>>: результирующая $\Theta_{td}$ будет на $\lambda$ состоять из априорного распределения $\frac{1}{|T|}$ и на $(1-\lambda)$ из оценки максимума правдоподобия $\frac{n_{td}}{n_d}$.

Аналогично интерпретируется и формула \ref{sp_phi_rel2abs} для регуляризатора сглаживания $\Phi$, воздействующего на темы $T$ и токены $W$. Здесь задаётся взвешенная комбинация из априорного распределения $\frac{1}{|W|}$ и оценки максимума правдоподобия $\frac{n_{wt}}{n_t}$.

Это означает, что любую существующую модель можно переформулировать в терминах относительного сглаживания и разреживания без каких-либо потерь. Практическая значимость этого результата заключается в вытекающей из него возможности переносить существующую схему сглаживания/разреживания на другую коллекцию схожей структуры.

Оставшаяся часть главы посвящена изучению роли матриц $\Phi$ и $\Theta$ в тематическом моделировании. Вопрос рассматривается с двух позиций: с точки зрения математической постановки задачи и с точки зрения построения и использования тематических моделей на практике. В терминах вероятностного смысла задачи обе матрицы являются равноправными.

При этом существует ряд причин, по которым естественно считать матрицу $\Theta$ чем-то второстепенным. Особое значение для рассматриваемой работы имеет аргумент об интерпретируемости: описанный в предыдущей главе процесс оценки тем на интерпретируемость обычно состоит из выбора небольшого набора верхних слов для каждой темы и представления этого набора эксперту-человеку \cite{roder2015exploring}. В этом процессе используется только матрица $\Phi$. Таким образом, в контексте интерпретации найденных тем экспертами качество матрицы $\Phi$ оказывается более важным, чем качество матрицы $\Theta$.

В работе \cite{thetaless} доказывается, что введение функциональной зависимости $\Theta = f(\Phi)$ требует модификации EM-алгоритма. Эту модификацию можно истолковать, как добавление псевдорегуляризатора быстрой векторизации.

Оставшаясся часть четвёртой главы посвящена изучению свойств этого псевдоргеуляризатора. Поведение модели с предложенным псевдоргеуляризатором исследуется на реальной текстовой коллекции. Результаты показывают, что этот псевдорегуляризатор действительно повышает ряд критериев качества, связанных с интерпретируемостью и успешно комбинируется с другими регуляризаторами.

\underline{\textbf{Пятая глава}} посвящена библиотеке TopicNet. TopicNet --- открытая надстройка над библиотекой BigARTM, предоставляющая более удобные
возможности для подбора гиперпараметров, для работы с пользовательскими
регуляризаторами и для визуализации тематических моделей. Описанная библиотека доступна онлайн на GitHub.

Главная мотивация TopicNet --- создать инструмент, удобный как для новичков, так и для продвинутых пользователей. Большое внимание уделяется удобству работу и наличию <<рецептов>>, показывающих хорошее качество без трудоёмкой настройки. Численный эксперимент показывает, что TopicNet с настройками <<по умолчанию>> превосходит аналогичную библиотеку GenSim с настройками <<по умолчанию>> по критериям различности, когерентности и информативности тем.

Библиотека TopicNet состоит из двух больших модулей: \texttt{viewers} и \texttt{cooking machine}.  

Модуль \texttt{Viewers} содержит различные инструменты визуализации. Дизайн придерживается философии Unix: каждый вьювер имеет ограниченную область ответственности и способен возвращать результат операции в JSON-подобном виде.

Это даёт возможность комбинировать содержащиеся в модуле вьюверы, не теряя при этом удобные для конечного пользователя методы, возвращающие \texttt{pandas.DataFrame}, строку сформированного HTML, или отображающие результат сразу в ячейке вывода Jupyter Notebook. 

Модуль \texttt{Cooking Machine} содержит различные инструменты для моделирования, расположенные в иерархии основных классов. Эти классы отвечают за построение и обучение модели заданной структуры, за отбор моделей согласно заданным пользователем ограничениям, а также за сохранение, загрузку и журналирование происходящего процесса.

Процесс построения тематической модели представим в виде дерева. Каждый узел дерева содержит в себе тематическую модель, а ориентированные рёбра хранят информацию об отношениях <<предок-потомок>> вида <<модель $Y$ была получена из модели $X$ при помощи преобразования $T_{XY}$>>. Не все деревья эксперимента являются допустимыми. Мы накладываем ограничения на допустимые преобразования: требуем, чтобы все рёбра одного уровня описывали преобразования из одного и того же семейства, различающиеся лишь набором параметров. 

Одним из примеров таких преобразований является <<Применить к модели регуляризатор с произвольными параметрами>>. 

Класс \texttt{Experiment} отвечает за хранение, журналирование и актуализацию этой структуры.  

Все преобразования связаны с экземпляром класса \texttt{Cube}. Каждый \texttt{Cube} играет роль чертежа, задающего все преобразования на текущем уровне эксперимента. Таким образом, процесс обучения можно представить как цепочку кубов, последовательно соединённых друг с другом.  

Куб выполняет две важные функции. Первая --- это \textit{спецификация}: во время инициализации куб преобразует заданные пользователем параметры в многомерное пространство поиска. Вторая функция --- \textit{применение}: получив точку в пространстве поиска и тематическую модель, куб изменяет какое-либо количество параметров и/или гиперпараметров модели. Таким образом, он играет роль инкубатора для моделей, что отражено в названии класса. На Рис.  \ref{Training-scheme} приведена схема обучения, состоящая из двух кубов, применённых к одной модели.  

Классы \texttt{Experiment} и \texttt{Cube} позволяют сделать логгирование и сложные стратегии обучения более сжатыми и доступными. Модуль \texttt{config\_parser} делает ещё один шаг в сторону облегчения конфигурируемости: стратегию обучения можно задать при помощи текстового конфигурационного файла в формате YAML.  

\begin{figure}[ht]
    \centering
    \captionsetup{justification=raggedright,singlelinecheck=false,format=hang}

    \includegraphics[width=0.7\textwidth]{training_scheme_example.eps}
    %\begin{flushleft}

    \caption{
        Пример двухэтапной схемы эксперимента.
        На первом этапе применяется регуляризатор с коэффициентом $\tau$, принимающим значения из некоторого множества $\{\tau_1, \tau_2, \tau_3\}$.
        Лучшими моделями после первого этапа являются \emph{Model 1} и \emph{Model 2}, поэтому \emph{Model 3} больше не участвует в процессе обучения.\\
        Второй этап связан с другим регуляризатором с коэффициентом $\xi$, принимающим значения из множества $\{\xi_1, \xi_2\}$.
        В результате этого этапа у каждой из ранее отобранных моделей  появляется два потомка.
    }
    %\end{flushleft}
\label{Training-scheme}
\end{figure} 


\begin{figure*}[ht]
% \footnotesize
\captionsetup{justification=raggedright,singlelinecheck=false,format=hang}
\texttt{TopicKernel@word.average\_contrast > 0.95 * MAXIMUM( \\
\hphantom{\ \ \ \ \ \ \ \ }TopicKernel@word.average\_contrast) \\
\hphantom{\ \ } and PerplexityScore@all < 1.1 * MINIMUM( \\
\hphantom{\ \ \ \ \ \ \ \ }PerplexityScore@all) \\
\hphantom{\ \ } and SparsityPhiScore@word -> max\\
\hphantom{\ \ } COLLECT 3} \\
\caption{Пример строки, задающей критерий отбора моделей. Здесь в качестве критериев отбора участвуют перплексия, контраст лексического ядра модальности \texttt{@word} и разреженность матрицы $\Phi$. Результатом будут три модели, контраст которых не более чем на 5\% отличается от наилучшего достигнутого контраста, имеют допустимую перплексию и как можно более разреженны. }
\label{DSL-example}
\end{figure*} 

В реальных экспериментах не у каждой модели есть потомки; большинство моделей отбрасывается в соответствии с каким-то критерием. Самый естественный, но в то же время самый трудозатратный способ --- это ручное изучение список верхних токенов и верхних документов, на основании которого пользователь принимает решение, какие из моделей являются <<удовлетворительными>>. Другой подход заключается в сравнении численных показателей; обычно используется перплексия и когерентность. Библиотека \mbox{BigARTM} добавляет к их числу ещё какое-либо число дополнительных метрик: например, разреженность, чистота и контраст \cite{voron15mlj}.

Библиотека TopicNet также поддерживает пользовательские критерии качества. Значительная часть мер, описанных во второй главе, реализована на платформе TopicNet.  

Для того чтобы облегчить груз ручной инспекции, мы реализовали простой domain-specific язык для отбора моделей (пример приведён на Рис \ref{DSL-example}). Результатом этого становится более простой и прозрачный процесс отбора моделей, способный учитывать множество критериев одновременно.  

\underline{\textbf{В шестой главе}} рассматривается задача создания таксономии коллекции диалогов контактного центра без наличия разметки. Предлагается регуляризованная тематическая модель, играющая роль первого приближения к структуре коллекции.

Был использован усложнённый вариант распространённой техники, улучшающей интерпретируемость --- использования информативных нграм (или коллокаций) в качестве дополнительной модальности. Для того чтобы извлечь информативные нграмы, был использован алгоритм TopMine \cite{topmine}, основанный на статистике совстречаемостей слов.

Для нашей задачи имело смысл внести ряд правок в алгоритм TopMine. Во-первых, логика подсчёта статистик совстречаемостей алгоритма была модифицирована таким образом, чтобы в ней использовались мультимножества слов вместо последовательностей слов. 

Во-вторых, TopMine не выделяет пересекающиеся коллокации. Это приводит к тому, что похожие предложения (<<записать ребёнка в детский сад>> и <<записать ребёнка в детский садик>>) могут вовсе не содержать общих коллокаций. Примером служат предложение <<получение паспорта РФ>> (выделится коллокация \texttt{получение\_паспорт\_РФ}) и предложение <<паспорт РФ был утерян>> (выделится коллокация \texttt{паспорт\_РФ}). Это следует из процесса поиска коллокаций алгоритмом: на каждом шаге обработки соседние коллокации-кандидаты сливаются, если их объединение удовлетворяет критерию информативности. Для того чтобы устранить вышеописанную проблему, достаточно изменить процесс итеративного слияния фраз так, чтобы при успешном слиянии коллокаций они не удалялись из множества кандидатов. Данная модификация увеличивает расход памяти алгоритма, однако делает процесс поиска не <<жадным>>.

% Во-первых, исходный алгоритм вычисляет статистики совстречаемости для упорядоченных кортежей слов $(w_1, w_2, \dots, w_k)$. В частности, алгоритм различает последовательности $(w_1, w_2)$ и $(w_2, w_1)$. Эта особенность делает алгоритм TopMine плохо приспособленными для обработки текстов на языках с нестрогим порядком слов (таких как русский). По этой причине логика подсчёта статистик совстречаемостей алгоритма была модифицирована таким образом, чтобы в ней использовались мультимножества слов вместо последовательностей слов.

Предлагаемая модель является двухуровневой, то есть состоит из двух <<обычных>> тематических моделей. Модель первого уровня и модель второго ориентируются на различные признаки. Это связано с тем, что первый уровень иерархии предназначен для определения предмета диалога, а цель второго уровня иерархии --- нахождение действий, которые пытается предпринять пользователь. % Существительные имеют большое влияние на темы первого уровня, а для тем второго уровня более важную роль играют глаголы.

В контексте поставленной задачи было предпринято разделение признаков по их функциональному назначению. Из всех токенов (слов и нграм) были выделены две группы на основании их частеречного состава: <<тематическая>> и <<функциональная>>. <<Функциональная>> группа состоит из одиночных глаголов и нграм, содержащих хотя бы один глагол. <<Тематическая>> группа состоит из одиночных существительных, одиночных прилагательных и нграмм, включающих в себя хотя бы одно существительное и не имеющих в своём составе глаголов.

Таким образом, предлагаемая тематическая модель использует пять модальностей: \texttt{@lemmatized} (просто слова), \texttt{@verb\_lemmatized} (слова-глаголы), \texttt{@noun\_lemmatized} (слова-существительные и слова-прилагательные), \texttt{@theme\_ngramms} (нграмы с существительными и без глаголов), \texttt{@verb\_ngramms} (нграмы с глаголами).



Одна и та же стратегия обучения успешно применяется к двум разным коллекциям диалогов. Первая коллекция состоит из диалогов клиентов с представителями различными государственных организаций, а вторая представляет собой логи технической поддержки провайдера. Механизм относительных коэффициентов позволил успешно перенести веса модальностей, подобранные на первой коллекции, на вторую коллекцию.


\FloatBarrier
\pdfbookmark{Заключение}{conclusion}                                  % Закладка pdf
В \underline{\textbf{заключении}} приведены основные результаты работы, которые заключаются в следующем:
\input{common/concl}

\pdfbookmark{Литература}{bibliography}                                % Закладка pdf

% При использовании пакета \verb!biblatex! список публикаций автора по теме диссертации формируется в разделе <<\publications>>\ файла \verb!common/characteristic.tex!  при помощи команды \verb!\nocite!

\ifdefmacro{\microtypesetup}{\microtypesetup{protrusion=false}}{} % не рекомендуется применять пакет микротипографики к автоматически генерируемому списку литературы
\urlstyle{rm}                               % ссылки URL обычным шрифтом
\ifnumequal{\value{bibliosel}}{0}{% Встроенная реализация с загрузкой файла через движок bibtex8
  \renewcommand{\bibname}{\large \bibtitleauthor}
  \nocite{*}
  \insertbiblioauthor           % Подключаем Bib-базы
  %\insertbiblioexternal   % !!! bibtex не умеет работать с несколькими библиографиями !!!
}{% Реализация пакетом biblatex через движок biber
  % Цитирования.
  %  * Порядок перечисления определяет порядок в библиографии (только внутри подраздела, если `\insertbiblioauthorgrouped`).
  %  * Если не соблюдать порядок "как для \printbibliography", нумерация в `\insertbiblioauthor` будет кривой.
  %  * Если цитировать каждый источник отдельной командой --- найти некоторые ошибки будет проще.
  %
  \nocite{intracoh, popov_hier, bulatov2020topicnet, thetaless}

  %% authorprogram
  \nocite{prog_cook}%
  \nocite{prog_stkc}%
  \nocite{prog_view}%
  %

  \ifnumgreater{\value{usefootcite}}{0}{
    \begin{refcontext}[labelprefix={}]
      \ifnum \value{bibgrouped}>0
        \insertbiblioauthorgrouped    % Вывод всех работ автора, сгруппированных по источникам
      \else
        \insertbiblioauthor      % Вывод всех работ автора
      \fi
    \end{refcontext}
  }{
  \ifnum \totvalue{citeexternal}>0
    \begin{refcontext}[labelprefix=A]
      \ifnum \value{bibgrouped}>0
        \insertbiblioauthorgrouped    % Вывод всех работ автора, сгруппированных по источникам
      \else
        \insertbiblioauthor      % Вывод всех работ автора
      \fi
    \end{refcontext}
  \else
    \ifnum \value{bibgrouped}>0
      \insertbiblioauthorgrouped    % Вывод всех работ автора, сгруппированных по источникам
    \else
      \insertbiblioauthor      % Вывод всех работ автора
    \fi
  \fi
  %  \insertbiblioauthorimportant  % Вывод наиболее значимых работ автора (определяется в файле characteristic во второй section)
  \begin{refcontext}[labelprefix={}]
      \insertbiblioexternal            % Вывод списка литературы, на которую ссылались в тексте автореферата
  \end{refcontext}
  
  % Невидимый библиографический список для подсчёта количества внешних публикаций
  % Используется, чтобы убрать приставку "А" у работ автора, если в автореферате нет
  % цитирований внешних источников.
  % \printbibliography[heading=nobibheading, section=1, keyword=biblioexternal]%
  }
}
\ifdefmacro{\microtypesetup}{\microtypesetup{protrusion=true}}{}
\urlstyle{tt}                               % возвращаем установки шрифта ссылок URL
